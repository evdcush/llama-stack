{
  "('meta-llama/Llama-3.1-8B-Instruct', [SystemMessage(role='system', content='You are a helpful assistant Always respond with tool calls no matter what. '), UserMessage(role='user', content='Get the boiling point of polyjuice with a tool call.', context=None), CompletionMessage(role='assistant', content='', stop_reason=<StopReason.end_of_turn: 'end_of_turn'>, tool_calls=[ToolCall(call_id='<UUID>', tool_name='get_boiling_point', arguments={'liquid_name': 'polyjuice', 'celcius': 'true'})]), ToolResponseMessage(role='tool', call_id='<UUID>', tool_name='get_boiling_point', content='-100'), CompletionMessage(role='assistant', content='', stop_reason=<StopReason.end_of_turn: 'end_of_turn'>, tool_calls=[ToolCall(call_id='<UUID>', tool_name='get_boiling_point', arguments={'liquid_name': 'polyjuice', 'celcius': 'false'})]), ToolResponseMessage(role='tool', call_id='<UUID>', tool_name='get_boiling_point', content='-100')])_[('response_format', None), ('sampling_params', SamplingParams(strategy=TopPSamplingStrategy(type='top_p', temperature=0.0001, top_p=0.9), max_tokens=0, repetition_penalty=1.0)), ('stream', True), ('tool_config', ToolConfig(tool_choice=<ToolChoice.auto: 'auto'>, tool_prompt_format=None, system_message_behavior=<SystemMessageBehavior.append: 'append'>)), ('tool_prompt_format', None), ('tools', [ToolDefinition(tool_name='get_boiling_point', description='Returns the boiling point of a liquid in Celcius or Fahrenheit', parameters={'liquid_name': ToolParamDefinition(param_type='str', description='The name of the liquid', required=True, default=None), 'celcius': ToolParamDefinition(param_type='bool', description='Whether to return the boiling point in Celcius', required=False, default=True)})])]": {
    "chunks": [
      {
        "event": {
          "delta": {
            "text": "",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "start"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "The",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " boiling point of polyjuice is -100 degrees Fahrenheit.",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "complete"
          },
          "logprobs": null,
          "stop_reason": {
            "__enum__": "StopReason",
            "value": "end_of_turn"
          }
        },
        "metrics": null
      }
    ],
    "type": "generator"
  },
  "('meta-llama/Llama-3.1-8B-Instruct', [SystemMessage(role='system', content='You are a helpful assistant Always respond with tool calls no matter what. '), UserMessage(role='user', content='Get the boiling point of polyjuice with a tool call.', context=None), CompletionMessage(role='assistant', content='', stop_reason=<StopReason.end_of_turn: 'end_of_turn'>, tool_calls=[ToolCall(call_id='<UUID>', tool_name='get_boiling_point', arguments={'liquid_name': 'polyjuice', 'celcius': 'true'})]), ToolResponseMessage(role='tool', call_id='<UUID>', tool_name='get_boiling_point', content='-100'), CompletionMessage(role='assistant', content='', stop_reason=<StopReason.end_of_turn: 'end_of_turn'>, tool_calls=[ToolCall(call_id='<UUID>', tool_name='get_boiling_point', arguments={'liquid_name': 'polyjuice', 'celcius': 'false'})]), ToolResponseMessage(role='tool', call_id='<UUID>', tool_name='get_boiling_point', content='-100')])_[('response_format', None), ('sampling_params', SamplingParams(strategy=TopPSamplingStrategy(type='top_p', temperature=0.0001, top_p=0.9), max_tokens=0, repetition_penalty=1.0)), ('stream', True), ('tool_config', ToolConfig(tool_choice=<ToolChoice.auto: 'auto'>, tool_prompt_format=None, system_message_behavior=<SystemMessageBehavior.append: 'append'>)), ('tool_prompt_format', None), ('tools', [ToolDefinition(tool_name='get_boiling_point', description='Returns the boiling point of a liquid in Celcius or Fahrenheit', parameters={'liquid_name': ToolParamDefinition(param_type='string', description='The name of the liquid', required=True, default=None), 'celcius': ToolParamDefinition(param_type='bool', description='Whether to return the boiling point in Celcius', required=False, default=True)})])]": {
    "chunks": [
      {
        "event": {
          "delta": {
            "text": "",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "start"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "The",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " boiling point of polyjuice is -100",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " degrees Fahrenheit.",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "complete"
          },
          "logprobs": null,
          "stop_reason": {
            "__enum__": "StopReason",
            "value": "end_of_turn"
          }
        },
        "metrics": null
      }
    ],
    "type": "generator"
  },
  "('meta-llama/Llama-3.1-8B-Instruct', [SystemMessage(role='system', content='You are a helpful assistant Always respond with tool calls no matter what. '), UserMessage(role='user', content='Get the boiling point of polyjuice with a tool call.', context=None), CompletionMessage(role='assistant', content='', stop_reason=<StopReason.end_of_turn: 'end_of_turn'>, tool_calls=[ToolCall(call_id='<UUID>', tool_name='get_boiling_point', arguments={'liquid_name': 'polyjuice', 'celcius': 'true'})]), ToolResponseMessage(role='tool', call_id='<UUID>', tool_name='get_boiling_point', content='-100')])_[('response_format', None), ('sampling_params', SamplingParams(strategy=TopPSamplingStrategy(type='top_p', temperature=0.0001, top_p=0.9), max_tokens=0, repetition_penalty=1.0)), ('stream', True), ('tool_config', ToolConfig(tool_choice=<ToolChoice.auto: 'auto'>, tool_prompt_format=None, system_message_behavior=<SystemMessageBehavior.append: 'append'>)), ('tool_prompt_format', None), ('tools', [ToolDefinition(tool_name='get_boiling_point', description='Returns the boiling point of a liquid in Celcius or Fahrenheit', parameters={'liquid_name': ToolParamDefinition(param_type='str', description='The name of the liquid', required=True, default=None), 'celcius': ToolParamDefinition(param_type='bool', description='Whether to return the boiling point in Celcius', required=False, default=True)})])]": {
    "chunks": [
      {
        "event": {
          "delta": {
            "text": "",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "start"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "{\"",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "type\": \"function\", \"name\": \"get_boiling_point",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "\", \"parameters\": {\"liquid_name\":",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " \"polyjuice\", \"cel",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "cius\": \"false\"}}",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "succeeded"
            },
            "tool_call": {
              "arguments": {
                "celcius": "false",
                "liquid_name": "polyjuice"
              },
              "call_id": "dc0f86d3-2b7a-45b0-8e58-8f49c9942190",
              "tool_name": "get_boiling_point"
            },
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": {
            "__enum__": "StopReason",
            "value": "end_of_turn"
          }
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "complete"
          },
          "logprobs": null,
          "stop_reason": {
            "__enum__": "StopReason",
            "value": "end_of_turn"
          }
        },
        "metrics": null
      }
    ],
    "type": "generator"
  },
  "('meta-llama/Llama-3.1-8B-Instruct', [SystemMessage(role='system', content='You are a helpful assistant Always respond with tool calls no matter what. '), UserMessage(role='user', content='Get the boiling point of polyjuice with a tool call.', context=None), CompletionMessage(role='assistant', content='', stop_reason=<StopReason.end_of_turn: 'end_of_turn'>, tool_calls=[ToolCall(call_id='<UUID>', tool_name='get_boiling_point', arguments={'liquid_name': 'polyjuice', 'celcius': 'true'})]), ToolResponseMessage(role='tool', call_id='<UUID>', tool_name='get_boiling_point', content='-100')])_[('response_format', None), ('sampling_params', SamplingParams(strategy=TopPSamplingStrategy(type='top_p', temperature=0.0001, top_p=0.9), max_tokens=0, repetition_penalty=1.0)), ('stream', True), ('tool_config', ToolConfig(tool_choice=<ToolChoice.auto: 'auto'>, tool_prompt_format=None, system_message_behavior=<SystemMessageBehavior.append: 'append'>)), ('tool_prompt_format', None), ('tools', [ToolDefinition(tool_name='get_boiling_point', description='Returns the boiling point of a liquid in Celcius or Fahrenheit', parameters={'liquid_name': ToolParamDefinition(param_type='string', description='The name of the liquid', required=True, default=None), 'celcius': ToolParamDefinition(param_type='bool', description='Whether to return the boiling point in Celcius', required=False, default=True)})])]": {
    "chunks": [
      {
        "event": {
          "delta": {
            "text": "",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "start"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "{\"",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "type\": \"function\", \"name\": \"get_boiling_point\", \"",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "parameters\": {\"liquid_name\": \"polyjuice\",",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " \"celcius\": \"false\"}}",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "succeeded"
            },
            "tool_call": {
              "arguments": {
                "celcius": "false",
                "liquid_name": "polyjuice"
              },
              "call_id": "ddfa8f71-9491-481b-9b86-dceadc6e3173",
              "tool_name": "get_boiling_point"
            },
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": {
            "__enum__": "StopReason",
            "value": "end_of_turn"
          }
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "complete"
          },
          "logprobs": null,
          "stop_reason": {
            "__enum__": "StopReason",
            "value": "end_of_turn"
          }
        },
        "metrics": null
      }
    ],
    "type": "generator"
  },
  "('meta-llama/Llama-3.1-8B-Instruct', [SystemMessage(role='system', content='You are a helpful assistant Always respond with tool calls no matter what. '), UserMessage(role='user', content='Get the boiling point of polyjuice with a tool call.', context=None)])_[('response_format', None), ('sampling_params', SamplingParams(strategy=TopPSamplingStrategy(type='top_p', temperature=0.0001, top_p=0.9), max_tokens=0, repetition_penalty=1.0)), ('stream', True), ('tool_config', ToolConfig(tool_choice=<ToolChoice.auto: 'auto'>, tool_prompt_format=None, system_message_behavior=<SystemMessageBehavior.append: 'append'>)), ('tool_prompt_format', None), ('tools', [ToolDefinition(tool_name='get_boiling_point', description='Returns the boiling point of a liquid in Celcius or Fahrenheit', parameters={'liquid_name': ToolParamDefinition(param_type='str', description='The name of the liquid', required=True, default=None), 'celcius': ToolParamDefinition(param_type='bool', description='Whether to return the boiling point in Celcius', required=False, default=True)})])]": {
    "chunks": [
      {
        "event": {
          "delta": {
            "text": "",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "start"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "started"
            },
            "tool_call": "",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "in_progress"
            },
            "tool_call": "{\"type\": \"function\", \"name\": \"get_bo",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "in_progress"
            },
            "tool_call": "iling_point\", \"parameters\": {\"liquid_name\": \"poly",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "in_progress"
            },
            "tool_call": "juice\", \"celcius\": \"true\"}}",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "succeeded"
            },
            "tool_call": {
              "arguments": {
                "celcius": "true",
                "liquid_name": "polyjuice"
              },
              "call_id": "510ca34b-5ba9-4d5f-9ff3-c56de756fc95",
              "tool_name": "get_boiling_point"
            },
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": {
            "__enum__": "StopReason",
            "value": "end_of_turn"
          }
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "complete"
          },
          "logprobs": null,
          "stop_reason": {
            "__enum__": "StopReason",
            "value": "end_of_turn"
          }
        },
        "metrics": null
      }
    ],
    "type": "generator"
  },
  "('meta-llama/Llama-3.1-8B-Instruct', [SystemMessage(role='system', content='You are a helpful assistant Always respond with tool calls no matter what. '), UserMessage(role='user', content='Get the boiling point of polyjuice with a tool call.', context=None)])_[('response_format', None), ('sampling_params', SamplingParams(strategy=TopPSamplingStrategy(type='top_p', temperature=0.0001, top_p=0.9), max_tokens=0, repetition_penalty=1.0)), ('stream', True), ('tool_config', ToolConfig(tool_choice=<ToolChoice.auto: 'auto'>, tool_prompt_format=None, system_message_behavior=<SystemMessageBehavior.append: 'append'>)), ('tool_prompt_format', None), ('tools', [ToolDefinition(tool_name='get_boiling_point', description='Returns the boiling point of a liquid in Celcius or Fahrenheit', parameters={'liquid_name': ToolParamDefinition(param_type='string', description='The name of the liquid', required=True, default=None), 'celcius': ToolParamDefinition(param_type='bool', description='Whether to return the boiling point in Celcius', required=False, default=True)})])]": {
    "chunks": [
      {
        "event": {
          "delta": {
            "text": "",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "start"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "started"
            },
            "tool_call": "",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "in_progress"
            },
            "tool_call": "{\"type\": \"function\", \"name",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "in_progress"
            },
            "tool_call": "\": \"get_boiling_point\", \"parameters\": {\"liquid",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "in_progress"
            },
            "tool_call": "_name\": \"polyjuice\", \"cel",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "in_progress"
            },
            "tool_call": "cius\": \"true\"}}",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "succeeded"
            },
            "tool_call": {
              "arguments": {
                "celcius": "true",
                "liquid_name": "polyjuice"
              },
              "call_id": "b6df5567-b0af-4c0f-bc5a-d66a557cac2e",
              "tool_name": "get_boiling_point"
            },
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": {
            "__enum__": "StopReason",
            "value": "end_of_turn"
          }
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "complete"
          },
          "logprobs": null,
          "stop_reason": {
            "__enum__": "StopReason",
            "value": "end_of_turn"
          }
        },
        "metrics": null
      }
    ],
    "type": "generator"
  },
  "('meta-llama/Llama-3.1-8B-Instruct', [SystemMessage(role='system', content='You are a helpful assistant'), UserMessage(role='user', content='Call get_boiling_point and answer What is the boiling point of polyjuice?', context=None), CompletionMessage(role='assistant', content='', stop_reason=<StopReason.end_of_turn: 'end_of_turn'>, tool_calls=[ToolCall(call_id='<UUID>', tool_name='get_boiling_point', arguments={'liquid_name': 'polyjuice', 'celcius': 'true'})]), ToolResponseMessage(role='tool', call_id='<UUID>', tool_name='get_boiling_point', content='-100')])_[('response_format', None), ('sampling_params', SamplingParams(strategy=TopPSamplingStrategy(type='top_p', temperature=0.0001, top_p=0.9), max_tokens=0, repetition_penalty=1.0)), ('stream', True), ('tool_config', ToolConfig(tool_choice=<ToolChoice.auto: 'auto'>, tool_prompt_format=None, system_message_behavior=<SystemMessageBehavior.append: 'append'>)), ('tool_prompt_format', None), ('tools', [ToolDefinition(tool_name='get_boiling_point', description='Returns the boiling point of a liquid in Celcius or Fahrenheit', parameters={'liquid_name': ToolParamDefinition(param_type='str', description='The name of the liquid', required=True, default=None), 'celcius': ToolParamDefinition(param_type='bool', description='Whether to return the boiling point in Celcius', required=False, default=True)})])]": {
    "chunks": [
      {
        "event": {
          "delta": {
            "text": "",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "start"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "The",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " boiling point of polyjuice is -100\u00b0C.",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "complete"
          },
          "logprobs": null,
          "stop_reason": {
            "__enum__": "StopReason",
            "value": "end_of_turn"
          }
        },
        "metrics": null
      }
    ],
    "type": "generator"
  },
  "('meta-llama/Llama-3.1-8B-Instruct', [SystemMessage(role='system', content='You are a helpful assistant'), UserMessage(role='user', content='Call get_boiling_point and answer What is the boiling point of polyjuice?', context=None), CompletionMessage(role='assistant', content='', stop_reason=<StopReason.end_of_turn: 'end_of_turn'>, tool_calls=[ToolCall(call_id='<UUID>', tool_name='get_boiling_point', arguments={'liquid_name': 'polyjuice', 'celcius': 'true'})]), ToolResponseMessage(role='tool', call_id='<UUID>', tool_name='get_boiling_point', content='-100')])_[('response_format', None), ('sampling_params', SamplingParams(strategy=TopPSamplingStrategy(type='top_p', temperature=0.0001, top_p=0.9), max_tokens=0, repetition_penalty=1.0)), ('stream', True), ('tool_config', ToolConfig(tool_choice=<ToolChoice.auto: 'auto'>, tool_prompt_format=None, system_message_behavior=<SystemMessageBehavior.append: 'append'>)), ('tool_prompt_format', None), ('tools', [ToolDefinition(tool_name='get_boiling_point', description='Returns the boiling point of a liquid in Celcius or Fahrenheit', parameters={'liquid_name': ToolParamDefinition(param_type='string', description='The name of the liquid', required=True, default=None), 'celcius': ToolParamDefinition(param_type='bool', description='Whether to return the boiling point in Celcius', required=False, default=True)})])]": {
    "chunks": [
      {
        "event": {
          "delta": {
            "text": "",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "start"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "The",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " boiling point of polyjuice is -100\u00b0C.",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "complete"
          },
          "logprobs": null,
          "stop_reason": {
            "__enum__": "StopReason",
            "value": "end_of_turn"
          }
        },
        "metrics": null
      }
    ],
    "type": "generator"
  },
  "('meta-llama/Llama-3.1-8B-Instruct', [SystemMessage(role='system', content='You are a helpful assistant'), UserMessage(role='user', content='Call get_boiling_point and answer What is the boiling point of polyjuice?', context=None)])_[('response_format', None), ('sampling_params', SamplingParams(strategy=TopPSamplingStrategy(type='top_p', temperature=0.0001, top_p=0.9), max_tokens=0, repetition_penalty=1.0)), ('stream', True), ('tool_config', ToolConfig(tool_choice=<ToolChoice.auto: 'auto'>, tool_prompt_format=None, system_message_behavior=<SystemMessageBehavior.append: 'append'>)), ('tool_prompt_format', None), ('tools', [ToolDefinition(tool_name='get_boiling_point', description='Returns the boiling point of a liquid in Celcius or Fahrenheit', parameters={'liquid_name': ToolParamDefinition(param_type='str', description='The name of the liquid', required=True, default=None), 'celcius': ToolParamDefinition(param_type='bool', description='Whether to return the boiling point in Celcius', required=False, default=True)})])]": {
    "chunks": [
      {
        "event": {
          "delta": {
            "text": "",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "start"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "started"
            },
            "tool_call": "",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "in_progress"
            },
            "tool_call": "{\"type\": \"function\", \"name\": \"get_boiling_point",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "in_progress"
            },
            "tool_call": "\", \"parameters\": {\"liquid_name\": \"",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "in_progress"
            },
            "tool_call": "polyjuice\", \"celcius\": \"true\"}}",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "succeeded"
            },
            "tool_call": {
              "arguments": {
                "celcius": "true",
                "liquid_name": "polyjuice"
              },
              "call_id": "ac699f8a-43ca-4f0b-abd4-0597722b42ee",
              "tool_name": "get_boiling_point"
            },
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": {
            "__enum__": "StopReason",
            "value": "end_of_turn"
          }
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "complete"
          },
          "logprobs": null,
          "stop_reason": {
            "__enum__": "StopReason",
            "value": "end_of_turn"
          }
        },
        "metrics": null
      }
    ],
    "type": "generator"
  },
  "('meta-llama/Llama-3.1-8B-Instruct', [SystemMessage(role='system', content='You are a helpful assistant'), UserMessage(role='user', content='Call get_boiling_point and answer What is the boiling point of polyjuice?', context=None)])_[('response_format', None), ('sampling_params', SamplingParams(strategy=TopPSamplingStrategy(type='top_p', temperature=0.0001, top_p=0.9), max_tokens=0, repetition_penalty=1.0)), ('stream', True), ('tool_config', ToolConfig(tool_choice=<ToolChoice.auto: 'auto'>, tool_prompt_format=None, system_message_behavior=<SystemMessageBehavior.append: 'append'>)), ('tool_prompt_format', None), ('tools', [ToolDefinition(tool_name='get_boiling_point', description='Returns the boiling point of a liquid in Celcius or Fahrenheit', parameters={'liquid_name': ToolParamDefinition(param_type='string', description='The name of the liquid', required=True, default=None), 'celcius': ToolParamDefinition(param_type='bool', description='Whether to return the boiling point in Celcius', required=False, default=True)})])]": {
    "chunks": [
      {
        "event": {
          "delta": {
            "text": "",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "start"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "started"
            },
            "tool_call": "",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "in_progress"
            },
            "tool_call": "{\"type\": \"function\", \"name\": \"get_boiling_point\",",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "in_progress"
            },
            "tool_call": " \"parameters\": {\"liquid_name\": \"polyjuice\", \"celci",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "in_progress"
            },
            "tool_call": "us\": \"true\"}}",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "succeeded"
            },
            "tool_call": {
              "arguments": {
                "celcius": "true",
                "liquid_name": "polyjuice"
              },
              "call_id": "cea5396d-ebc7-4b85-8b8c-09eaa89cf6b5",
              "tool_name": "get_boiling_point"
            },
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": {
            "__enum__": "StopReason",
            "value": "end_of_turn"
          }
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "complete"
          },
          "logprobs": null,
          "stop_reason": {
            "__enum__": "StopReason",
            "value": "end_of_turn"
          }
        },
        "metrics": null
      }
    ],
    "type": "generator"
  },
  "('meta-llama/Llama-3.1-8B-Instruct', [SystemMessage(role='system', content='You are a helpful assistant'), UserMessage(role='user', content='Give me a sentence that contains the word: hello', context=None)])_[('response_format', None), ('sampling_params', SamplingParams(strategy=TopPSamplingStrategy(type='top_p', temperature=0.0001, top_p=0.9), max_tokens=0, repetition_penalty=1.0)), ('stream', True), ('tool_config', ToolConfig(tool_choice=<ToolChoice.auto: 'auto'>, tool_prompt_format=None, system_message_behavior=<SystemMessageBehavior.append: 'append'>)), ('tool_prompt_format', None), ('tools', [])]": {
    "chunks": [
      {
        "event": {
          "delta": {
            "text": "",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "start"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "The",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " customer smiled and said \"hello\" to the friendly store clerk.",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "complete"
          },
          "logprobs": null,
          "stop_reason": {
            "__enum__": "StopReason",
            "value": "end_of_turn"
          }
        },
        "metrics": null
      }
    ],
    "type": "generator"
  },
  "('meta-llama/Llama-3.1-8B-Instruct', [SystemMessage(role='system', content='You are a helpful assistant'), UserMessage(role='user', content='Here is a csv file, can you describe it?', context=None), ToolResponseMessage(role='tool', call_id='', tool_name=<BuiltinTool.code_interpreter: 'code_interpreter'>, content=[TextContentItem(type='text', text='# User provided a file accessible to you at \"<TEMP_FILE>\"\\nYou can use code_interpreter to load and inspect it.')]), CompletionMessage(role='assistant', content='', stop_reason=<StopReason.end_of_turn: 'end_of_turn'>, tool_calls=[ToolCall(call_id='<UUID>', tool_name=<BuiltinTool.code_interpreter: 'code_interpreter'>, arguments={'code': 'import pandas as pd\\ndf = pd.read_csv(\"<TEMP_FILE>\")\\nprint(df.head())'})]), ToolResponseMessage(role='tool', call_id='<UUID>', tool_name=<BuiltinTool.code_interpreter: 'code_interpreter'>, content=\"completed\\n[stderr]\\nTraceback (most recent call last):\\n  line 5, in <module>\\n    from bwrap.core import main\\nModuleNotFoundError: No module named 'bwrap.core'\\n[/stderr]\"), CompletionMessage(role='assistant', content='', stop_reason=<StopReason.end_of_turn: 'end_of_turn'>, tool_calls=[ToolCall(call_id='<UUID>', tool_name=<BuiltinTool.code_interpreter: 'code_interpreter'>, arguments={'code': 'import pandas as pd\\ndf = pd.read_csv(\"<TEMP_FILE>\")\\nprint(df.head())\\nprint(df.info())\\nprint(df.describe())'})]), ToolResponseMessage(role='tool', call_id='<UUID>', tool_name=<BuiltinTool.code_interpreter: 'code_interpreter'>, content=\"completed\\n[stderr]\\nTraceback (most recent call last):\\n  line 5, in <module>\\n    from bwrap.core import main\\nModuleNotFoundError: No module named 'bwrap.core'\\n[/stderr]\")])_[('response_format', None), ('sampling_params', SamplingParams(strategy=TopPSamplingStrategy(type='top_p', temperature=0.0001, top_p=0.9), max_tokens=0, repetition_penalty=1.0)), ('stream', True), ('tool_config', ToolConfig(tool_choice=<ToolChoice.auto: 'auto'>, tool_prompt_format=None, system_message_behavior=<SystemMessageBehavior.append: 'append'>)), ('tool_prompt_format', None), ('tools', [ToolDefinition(tool_name=<BuiltinTool.code_interpreter: 'code_interpreter'>, description='Execute code', parameters={'code': ToolParamDefinition(param_type='string', description='The code to execute', required=True, default=None)}), ToolDefinition(tool_name='knowledge_search', description='Search for information in a database.', parameters={'query': ToolParamDefinition(param_type='string', description='The query to search for. Can be a natural language sentence or keywords.', required=True, default=None)})])]": {
    "chunks": [
      {
        "event": {
          "delta": {
            "text": "",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "start"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "The",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " error message indicates that the `bwrap.core` module",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " is not found. This is likely",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " because the `bwrap` package is not installed. To fix this",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": ", you can install the `bwrap` package using pip:\n\n```\n",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "pip install bwrap\n```\n\nHowever, if you don't have",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " permission to install packages, you can use the `knowledge_search` function to",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " get information about the CSV file instead:\n\n```\n{\n",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "    \"type\": \"function\",\n    \"name\": \"",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "knowledge_search\",\n    \"parameters\": {\n       ",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " \"query\": \"describe a csv file\"\n    }\n}\n``",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "`\n\nThis will return a description of the CSV file.",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "complete"
          },
          "logprobs": null,
          "stop_reason": {
            "__enum__": "StopReason",
            "value": "end_of_turn"
          }
        },
        "metrics": null
      }
    ],
    "type": "generator"
  },
  "('meta-llama/Llama-3.1-8B-Instruct', [SystemMessage(role='system', content='You are a helpful assistant'), UserMessage(role='user', content='Here is a csv file, can you describe it?', context=None), ToolResponseMessage(role='tool', call_id='', tool_name=<BuiltinTool.code_interpreter: 'code_interpreter'>, content=[TextContentItem(type='text', text='# User provided a file accessible to you at \"<TEMP_FILE>\"\\nYou can use code_interpreter to load and inspect it.')]), CompletionMessage(role='assistant', content='', stop_reason=<StopReason.end_of_turn: 'end_of_turn'>, tool_calls=[ToolCall(call_id='<UUID>', tool_name=<BuiltinTool.code_interpreter: 'code_interpreter'>, arguments={'code': 'import pandas as pd\\ndf = pd.read_csv(\"<TEMP_FILE>\")\\nprint(df.head())'})]), ToolResponseMessage(role='tool', call_id='<UUID>', tool_name=<BuiltinTool.code_interpreter: 'code_interpreter'>, content=\"completed\\n[stderr]\\nTraceback (most recent call last):\\n  line 5, in <module>\\n    from bwrap.core import main\\nModuleNotFoundError: No module named 'bwrap.core'\\n[/stderr]\")])_[('response_format', None), ('sampling_params', SamplingParams(strategy=TopPSamplingStrategy(type='top_p', temperature=0.0001, top_p=0.9), max_tokens=0, repetition_penalty=1.0)), ('stream', True), ('tool_config', ToolConfig(tool_choice=<ToolChoice.auto: 'auto'>, tool_prompt_format=None, system_message_behavior=<SystemMessageBehavior.append: 'append'>)), ('tool_prompt_format', None), ('tools', [ToolDefinition(tool_name='knowledge_search', description='Search for information in a database.', parameters={'query': ToolParamDefinition(param_type='string', description='The query to search for. Can be a natural language sentence or keywords.', required=True, default=None)}), ToolDefinition(tool_name=<BuiltinTool.code_interpreter: 'code_interpreter'>, description='Execute code', parameters={'code': ToolParamDefinition(param_type='string', description='The code to execute', required=True, default=None)})])]": {
    "chunks": [
      {
        "event": {
          "delta": {
            "text": "",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "start"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "The",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " error message indicates that the `",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "bwrap.core` module is not found.",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " This is because the `bwrap` library is not",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " installed in your environment. To fix this",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": ", you can install the `bwrap` library using pip",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": ":\n\n```\npip install bwrap\n```\n\nHowever,",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " since you are using a function `knowledge_search` which is not defined",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " in the given functions, I will assume",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " that you want to use the `",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "knowledge_search` function to search for information in a database",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": ".\n\nIf that's the case, you can use the `",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "knowledge_search` function to search for information in a database related to",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " the csv file. Here's an example",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": ":\n\n```\nknowledge_search(query=\"information about the csv file",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "\")\n```\n\nThis will search for information in",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " the database related to the csv file. The",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " result will depend on the information available in the database.",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "complete"
          },
          "logprobs": null,
          "stop_reason": {
            "__enum__": "StopReason",
            "value": "end_of_turn"
          }
        },
        "metrics": null
      }
    ],
    "type": "generator"
  },
  "('meta-llama/Llama-3.1-8B-Instruct', [SystemMessage(role='system', content='You are a helpful assistant'), UserMessage(role='user', content='Here is a csv file, can you describe it?', context=None), ToolResponseMessage(role='tool', call_id='', tool_name=<BuiltinTool.code_interpreter: 'code_interpreter'>, content=[TextContentItem(type='text', text='# User provided a file accessible to you at \"<TEMP_FILE>\"\\nYou can use code_interpreter to load and inspect it.')]), CompletionMessage(role='assistant', content='', stop_reason=<StopReason.end_of_turn: 'end_of_turn'>, tool_calls=[ToolCall(call_id='<UUID>', tool_name=<BuiltinTool.code_interpreter: 'code_interpreter'>, arguments={'code': 'import pandas as pd\\ndf = pd.read_csv(\"<TEMP_FILE>\")\\nprint(df.head())'})]), ToolResponseMessage(role='tool', call_id='<UUID>', tool_name=<BuiltinTool.code_interpreter: 'code_interpreter'>, content=\"completed\\n[stderr]\\nTraceback (most recent call last):\\n  line 5, in <module>\\n    from bwrap.core import main\\nModuleNotFoundError: No module named 'bwrap.core'\\n[/stderr]\")])_[('response_format', None), ('sampling_params', SamplingParams(strategy=TopPSamplingStrategy(type='top_p', temperature=0.0001, top_p=0.9), max_tokens=0, repetition_penalty=1.0)), ('stream', True), ('tool_config', ToolConfig(tool_choice=<ToolChoice.auto: 'auto'>, tool_prompt_format=None, system_message_behavior=<SystemMessageBehavior.append: 'append'>)), ('tool_prompt_format', None), ('tools', [ToolDefinition(tool_name=<BuiltinTool.code_interpreter: 'code_interpreter'>, description='Execute code', parameters={'code': ToolParamDefinition(param_type='string', description='The code to execute', required=True, default=None)}), ToolDefinition(tool_name='knowledge_search', description='Search for information in a database.', parameters={'query': ToolParamDefinition(param_type='string', description='The query to search for. Can be a natural language sentence or keywords.', required=True, default=None)})])]": {
    "chunks": [
      {
        "event": {
          "delta": {
            "text": "",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "start"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "started"
            },
            "tool_call": "",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "in_progress"
            },
            "tool_call": "import pandas as pd\ndf = pd.read",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "in_progress"
            },
            "tool_call": "_csv(\"/var/folders/cz/v",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "in_progress"
            },
            "tool_call": "yh7y1d11xg881",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "in_progress"
            },
            "tool_call": "lsxsshnc5c0000gn/T/tmpn9tl",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "in_progress"
            },
            "tool_call": "gts1/qYsQ3ZJLinflation.csv",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "in_progress"
            },
            "tool_call": "\")\nprint(df.head())\nprint(df.info())\nprint(df.describe())",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "succeeded"
            },
            "tool_call": {
              "arguments": {
                "code": "import pandas as pd\ndf = pd.read_csv(\"/var/folders/cz/vyh7y1d11xg881lsxsshnc5c0000gn/T/tmpn9tlgts1/qYsQ3ZJLinflation.csv\")\nprint(df.head())\nprint(df.info())\nprint(df.describe())"
              },
              "call_id": "6c3c4895-55a7-4083-b5d1-6ee42bcbe5fa",
              "tool_name": {
                "__enum__": "BuiltinTool",
                "value": "code_interpreter"
              }
            },
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": {
            "__enum__": "StopReason",
            "value": "end_of_turn"
          }
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "complete"
          },
          "logprobs": null,
          "stop_reason": {
            "__enum__": "StopReason",
            "value": "end_of_turn"
          }
        },
        "metrics": null
      }
    ],
    "type": "generator"
  },
  "('meta-llama/Llama-3.1-8B-Instruct', [SystemMessage(role='system', content='You are a helpful assistant'), UserMessage(role='user', content='Here is a csv file, can you describe it?', context=None), ToolResponseMessage(role='tool', call_id='', tool_name=<BuiltinTool.code_interpreter: 'code_interpreter'>, content=[TextContentItem(type='text', text='# User provided a file accessible to you at \"<TEMP_FILE>\"\\nYou can use code_interpreter to load and inspect it.')])])_[('response_format', None), ('sampling_params', SamplingParams(strategy=TopPSamplingStrategy(type='top_p', temperature=0.0001, top_p=0.9), max_tokens=0, repetition_penalty=1.0)), ('stream', True), ('tool_config', ToolConfig(tool_choice=<ToolChoice.auto: 'auto'>, tool_prompt_format=None, system_message_behavior=<SystemMessageBehavior.append: 'append'>)), ('tool_prompt_format', None), ('tools', [ToolDefinition(tool_name='knowledge_search', description='Search for information in a database.', parameters={'query': ToolParamDefinition(param_type='string', description='The query to search for. Can be a natural language sentence or keywords.', required=True, default=None)}), ToolDefinition(tool_name=<BuiltinTool.code_interpreter: 'code_interpreter'>, description='Execute code', parameters={'code': ToolParamDefinition(param_type='string', description='The code to execute', required=True, default=None)})])]": {
    "chunks": [
      {
        "event": {
          "delta": {
            "text": "",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "start"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "started"
            },
            "tool_call": "",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "in_progress"
            },
            "tool_call": "import pandas as pd\ndf = pd.read_csv(\"/var/folders",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "in_progress"
            },
            "tool_call": "/cz/vyh7y1d11",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "in_progress"
            },
            "tool_call": "xg881lsxsshnc5c0000gn/T/tmpcp",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "in_progress"
            },
            "tool_call": "be4t2d/J39TZZPxinflation.csv\")\nprint",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "in_progress"
            },
            "tool_call": "(df.head())",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "succeeded"
            },
            "tool_call": {
              "arguments": {
                "code": "import pandas as pd\ndf = pd.read_csv(\"/var/folders/cz/vyh7y1d11xg881lsxsshnc5c0000gn/T/tmpcpbe4t2d/J39TZZPxinflation.csv\")\nprint(df.head())"
              },
              "call_id": "8a73d4c4-2b3d-435b-9f96-f20b04c5d1f0",
              "tool_name": {
                "__enum__": "BuiltinTool",
                "value": "code_interpreter"
              }
            },
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": {
            "__enum__": "StopReason",
            "value": "end_of_turn"
          }
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "complete"
          },
          "logprobs": null,
          "stop_reason": {
            "__enum__": "StopReason",
            "value": "end_of_turn"
          }
        },
        "metrics": null
      }
    ],
    "type": "generator"
  },
  "('meta-llama/Llama-3.1-8B-Instruct', [SystemMessage(role='system', content='You are a helpful assistant'), UserMessage(role='user', content='Here is a csv file, can you describe it?', context=None), ToolResponseMessage(role='tool', call_id='', tool_name=<BuiltinTool.code_interpreter: 'code_interpreter'>, content=[TextContentItem(type='text', text='# User provided a file accessible to you at \"<TEMP_FILE>\"\\nYou can use code_interpreter to load and inspect it.')])])_[('response_format', None), ('sampling_params', SamplingParams(strategy=TopPSamplingStrategy(type='top_p', temperature=0.0001, top_p=0.9), max_tokens=0, repetition_penalty=1.0)), ('stream', True), ('tool_config', ToolConfig(tool_choice=<ToolChoice.auto: 'auto'>, tool_prompt_format=None, system_message_behavior=<SystemMessageBehavior.append: 'append'>)), ('tool_prompt_format', None), ('tools', [ToolDefinition(tool_name=<BuiltinTool.code_interpreter: 'code_interpreter'>, description='Execute code', parameters={'code': ToolParamDefinition(param_type='string', description='The code to execute', required=True, default=None)}), ToolDefinition(tool_name='knowledge_search', description='Search for information in a database.', parameters={'query': ToolParamDefinition(param_type='string', description='The query to search for. Can be a natural language sentence or keywords.', required=True, default=None)})])]": {
    "chunks": [
      {
        "event": {
          "delta": {
            "text": "",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "start"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "started"
            },
            "tool_call": "",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "in_progress"
            },
            "tool_call": "import pandas as pd\ndf = pd.read_csv(\"/",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "in_progress"
            },
            "tool_call": "var/folders/cz/vyh7",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "in_progress"
            },
            "tool_call": "y1d11xg881lsxsshnc5c0000",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "in_progress"
            },
            "tool_call": "gn/T/tmpn9tlgts1",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "in_progress"
            },
            "tool_call": "/qYsQ3ZJLin",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "in_progress"
            },
            "tool_call": "flation.csv\")\nprint(df.head())",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "succeeded"
            },
            "tool_call": {
              "arguments": {
                "code": "import pandas as pd\ndf = pd.read_csv(\"/var/folders/cz/vyh7y1d11xg881lsxsshnc5c0000gn/T/tmpn9tlgts1/qYsQ3ZJLinflation.csv\")\nprint(df.head())"
              },
              "call_id": "e6c48b40-6504-4043-b3fa-644bd7fafd0f",
              "tool_name": {
                "__enum__": "BuiltinTool",
                "value": "code_interpreter"
              }
            },
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": {
            "__enum__": "StopReason",
            "value": "end_of_turn"
          }
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "complete"
          },
          "logprobs": null,
          "stop_reason": {
            "__enum__": "StopReason",
            "value": "end_of_turn"
          }
        },
        "metrics": null
      }
    ],
    "type": "generator"
  },
  "('meta-llama/Llama-3.1-8B-Instruct', [SystemMessage(role='system', content='You are a helpful assistant'), UserMessage(role='user', content='Here is a csv, can you describe it?', context=None), CompletionMessage(role='assistant', content='', stop_reason=<StopReason.end_of_turn: 'end_of_turn'>, tool_calls=[ToolCall(call_id='<UUID>', tool_name=<BuiltinTool.code_interpreter: 'code_interpreter'>, arguments={'code': 'import pandas as pd\\n# Load data\\ndf = pd.read_csv(\"<TEMP_FILE>\")\\n# Rows\\nprint(\"Number of rows and columns in the data:\", df.shape)\\n# Columns\\nprint(\"Columns of the data are:\", len(df.columns))\\n# Column names\\nprint(\"Columns of the data are:\", df.columns)\\n# Column dtypes\\nprint(\"Datatype of the columns are:\", df.dtypes)'})]), ToolResponseMessage(role='tool', call_id='<UUID>', tool_name=<BuiltinTool.code_interpreter: 'code_interpreter'>, content=\"completed\\n[stderr]\\nTraceback (most recent call last):\\n  line 5, in <module>\\n    from bwrap.core import main\\nModuleNotFoundError: No module named 'bwrap.core'\\n[/stderr]\"), CompletionMessage(role='assistant', content='It seems that the file \"<TEMP_FILE>\" does not exist. \\n\\nTo describe the csv file, you need to provide the actual file path or the file itself. If you are running this code in a notebook, you can use the `upload` button to upload the file. If you are running this code in a script, you need to provide the file path.\\n\\nHere is an example of how you can describe the csv file if you have it in the same directory as your script:\\n\\n```python\\nimport pandas as pd\\n\\n# Load data\\ndf = pd.read_csv(\\'inflation.csv\\')\\n\\n# Print summary of the data\\nprint(df.head())  # Print the first few rows of the data\\nprint(df.info())  # Print information about the data\\nprint(df.describe())  # Print summary statistics about the data\\n```\\n\\nThis will print the first few rows of the data, information about the data, and summary statistics about the data.', stop_reason=<StopReason.end_of_turn: 'end_of_turn'>, tool_calls=[]), UserMessage(role='user', content='Plot average yearly inflation as a time series', context=None), CompletionMessage(role='assistant', content='', stop_reason=<StopReason.end_of_turn: 'end_of_turn'>, tool_calls=[ToolCall(call_id='<UUID>', tool_name=<BuiltinTool.code_interpreter: 'code_interpreter'>, arguments={'code': \"import pandas as pd\\nimport matplotlib.pyplot as plt\\n\\n# Load data\\ndf = pd.read_csv('inflation.csv')\\n\\n# Convert date column to datetime\\ndf['date'] = pd.to_datetime(df['date'])\\n\\n# Group by year and calculate average inflation\\naverage_inflation = df.groupby(df['date'].dt.year)['inflation'].mean()\\n\\n# Plot time series\\nplt.figure(figsize=(10,6))\\nplt.plot(average_inflation.index, average_inflation.values, marker='o')\\nplt.title('Average Yearly Inflation')\\nplt.xlabel('Year')\\nplt.ylabel('Average Inflation')\\nplt.grid(True)\\nplt.show()\"})]), ToolResponseMessage(role='tool', call_id='<UUID>', tool_name=<BuiltinTool.code_interpreter: 'code_interpreter'>, content=\"completed\\n[stderr]\\nTraceback (most recent call last):\\n  line 5, in <module>\\n    from bwrap.core import main\\nModuleNotFoundError: No module named 'bwrap.core'\\n[/stderr]\")])_[('response_format', None), ('sampling_params', SamplingParams(strategy=TopPSamplingStrategy(type='top_p', temperature=0.0001, top_p=0.9), max_tokens=0, repetition_penalty=1.0)), ('stream', True), ('tool_config', ToolConfig(tool_choice=<ToolChoice.auto: 'auto'>, tool_prompt_format=None, system_message_behavior=<SystemMessageBehavior.append: 'append'>)), ('tool_prompt_format', None), ('tools', [ToolDefinition(tool_name=<BuiltinTool.code_interpreter: 'code_interpreter'>, description='Execute code', parameters={'code': ToolParamDefinition(param_type='string', description='The code to execute', required=True, default=None)})])]": {
    "chunks": [
      {
        "event": {
          "delta": {
            "text": "",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "start"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "This",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " code will create a time series plot of the average yearly inflation.",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " The x-axis represents the year",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " and the y-axis represents the average inflation",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": ". The plot will show the trend of average yearly inflation over the",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " years.\n\nPlease note that you need to replace 'inflation.csv",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "' with the actual path to your csv file. Also, this",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " code assumes that the csv file has a column named 'date'",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " and another column named 'inflation'. If",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " your csv file has different column names, you",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " need to adjust the code accordingly.",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "complete"
          },
          "logprobs": null,
          "stop_reason": {
            "__enum__": "StopReason",
            "value": "end_of_turn"
          }
        },
        "metrics": null
      }
    ],
    "type": "generator"
  },
  "('meta-llama/Llama-3.1-8B-Instruct', [SystemMessage(role='system', content='You are a helpful assistant'), UserMessage(role='user', content='Here is a csv, can you describe it?', context=None), CompletionMessage(role='assistant', content='', stop_reason=<StopReason.end_of_turn: 'end_of_turn'>, tool_calls=[ToolCall(call_id='<UUID>', tool_name=<BuiltinTool.code_interpreter: 'code_interpreter'>, arguments={'code': 'import pandas as pd\\n# Load data\\ndf = pd.read_csv(\"<TEMP_FILE>\")\\n# Rows\\nprint(\"Number of rows and columns in the data:\", df.shape)\\n# Columns\\nprint(\"Columns of the data are:\", len(df.columns))\\n# Column names\\nprint(\"Columns of the data are:\", df.columns)\\n# Column dtypes\\nprint(\"Datatype of the columns are:\", df.dtypes)'})]), ToolResponseMessage(role='tool', call_id='<UUID>', tool_name=<BuiltinTool.code_interpreter: 'code_interpreter'>, content=\"completed\\n[stderr]\\nTraceback (most recent call last):\\n  line 5, in <module>\\n    from bwrap.core import main\\nModuleNotFoundError: No module named 'bwrap.core'\\n[/stderr]\"), CompletionMessage(role='assistant', content='It seems that the file \"<TEMP_FILE>\" does not exist. \\n\\nTo describe the csv file, you need to provide the actual file path or the file itself. If you are running this code in a notebook, you can use the `upload` button to upload the file. If you are running this code in a script, you need to provide the file path.\\n\\nHere is an example of how you can describe the csv file if you have it in the same directory as your script:\\n\\n```python\\nimport pandas as pd\\n\\n# Load data\\ndf = pd.read_csv(\\'inflation.csv\\')\\n\\n# Print summary of the data\\nprint(df.head())  # Print the first few rows of the data\\nprint(df.info())  # Print information about the data\\nprint(df.describe())  # Print summary statistics about the data\\n```\\n\\nThis will print the first few rows of the data, information about the data, and summary statistics about the data.', stop_reason=<StopReason.end_of_turn: 'end_of_turn'>, tool_calls=[]), UserMessage(role='user', content='Plot average yearly inflation as a time series', context=None)])_[('response_format', None), ('sampling_params', SamplingParams(strategy=TopPSamplingStrategy(type='top_p', temperature=0.0001, top_p=0.9), max_tokens=0, repetition_penalty=1.0)), ('stream', True), ('tool_config', ToolConfig(tool_choice=<ToolChoice.auto: 'auto'>, tool_prompt_format=None, system_message_behavior=<SystemMessageBehavior.append: 'append'>)), ('tool_prompt_format', None), ('tools', [ToolDefinition(tool_name=<BuiltinTool.code_interpreter: 'code_interpreter'>, description='Execute code', parameters={'code': ToolParamDefinition(param_type='string', description='The code to execute', required=True, default=None)})])]": {
    "chunks": [
      {
        "event": {
          "delta": {
            "text": "",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "start"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "started"
            },
            "tool_call": "",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "in_progress"
            },
            "tool_call": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "in_progress"
            },
            "tool_call": " data\ndf = pd.read_csv('inflation.csv')\n\n#",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "in_progress"
            },
            "tool_call": " Convert date column to datetime\ndf['date'] = pd.to",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "in_progress"
            },
            "tool_call": "_datetime(df['date'])\n\n# Group by year and calculate average",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "in_progress"
            },
            "tool_call": " inflation\naverage_inflation = df.groupby(df['date'].",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "in_progress"
            },
            "tool_call": "dt.year)['inflation'].mean()\n\n# Plot",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "in_progress"
            },
            "tool_call": " time series\nplt.figure(figsize=(10,6))\nplt",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "in_progress"
            },
            "tool_call": ".plot(average_inflation.index, average_inflation.values, marker='",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "in_progress"
            },
            "tool_call": "o')\nplt.title('Average Yearly Inflation')\nplt.xlabel",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "in_progress"
            },
            "tool_call": "('Year')\nplt.ylabel('",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "in_progress"
            },
            "tool_call": "Average Inflation')\nplt.grid(True)\n",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "in_progress"
            },
            "tool_call": "plt.show()",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "succeeded"
            },
            "tool_call": {
              "arguments": {
                "code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load data\ndf = pd.read_csv('inflation.csv')\n\n# Convert date column to datetime\ndf['date'] = pd.to_datetime(df['date'])\n\n# Group by year and calculate average inflation\naverage_inflation = df.groupby(df['date'].dt.year)['inflation'].mean()\n\n# Plot time series\nplt.figure(figsize=(10,6))\nplt.plot(average_inflation.index, average_inflation.values, marker='o')\nplt.title('Average Yearly Inflation')\nplt.xlabel('Year')\nplt.ylabel('Average Inflation')\nplt.grid(True)\nplt.show()"
              },
              "call_id": "81d7a873-376b-438e-916d-d5454e6ed09e",
              "tool_name": {
                "__enum__": "BuiltinTool",
                "value": "code_interpreter"
              }
            },
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": {
            "__enum__": "StopReason",
            "value": "end_of_turn"
          }
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "complete"
          },
          "logprobs": null,
          "stop_reason": {
            "__enum__": "StopReason",
            "value": "end_of_turn"
          }
        },
        "metrics": null
      }
    ],
    "type": "generator"
  },
  "('meta-llama/Llama-3.1-8B-Instruct', [SystemMessage(role='system', content='You are a helpful assistant'), UserMessage(role='user', content='Here is a csv, can you describe it?', context=None), CompletionMessage(role='assistant', content='', stop_reason=<StopReason.end_of_turn: 'end_of_turn'>, tool_calls=[ToolCall(call_id='<UUID>', tool_name=<BuiltinTool.code_interpreter: 'code_interpreter'>, arguments={'code': 'import pandas as pd\\n# Load data\\ndf = pd.read_csv(\"<TEMP_FILE>\")\\n# Rows\\nprint(\"Number of rows and columns in the data:\", df.shape)\\n# Columns\\nprint(\"Columns of the data are:\", len(df.columns))\\n# Column names\\nprint(\"Columns of the data are:\", df.columns)\\n# Column dtypes\\nprint(\"Datatype of the columns are:\", df.dtypes)'})]), ToolResponseMessage(role='tool', call_id='<UUID>', tool_name=<BuiltinTool.code_interpreter: 'code_interpreter'>, content=\"completed\\n[stderr]\\nTraceback (most recent call last):\\n  line 5, in <module>\\n    from bwrap.core import main\\nModuleNotFoundError: No module named 'bwrap.core'\\n[/stderr]\"), CompletionMessage(role='assistant', content='It seems that the file \"<TEMP_FILE>\" does not exist. \\n\\nTo describe the csv file, you need to provide the actual file path or the file itself. If you are using a remote server, you can use the `requests` library to download the file and then load it into a pandas dataframe. \\n\\nHere is an example of how you can do it:\\n\\n```\\nimport pandas as pd\\nimport requests\\n\\n# Download the csv file\\nurl = \"https://example.com/your_file.csv\"\\nresponse = requests.get(url)\\n\\n# Load the csv file into a pandas dataframe\\ndf = pd.read_csv(response.content)\\n\\n# Print the description of the dataframe\\nprint(df.describe())\\n```\\n\\nPlease replace the `url` variable with the actual URL of your csv file. \\n\\nIf you are using a local file, you can simply use the `pd.read_csv()` function with the file path:\\n\\n```\\nimport pandas as pd\\n\\n# Load the csv file into a pandas dataframe\\ndf = pd.read_csv(\\'your_file.csv\\')\\n\\n# Print the description of the dataframe\\nprint(df.describe())\\n```\\n\\nPlease replace `\\'your_file.csv\\'` with the actual path to your csv file.', stop_reason=<StopReason.end_of_turn: 'end_of_turn'>, tool_calls=[]), UserMessage(role='user', content='Plot average yearly inflation as a time series', context=None), CompletionMessage(role='assistant', content='', stop_reason=<StopReason.end_of_turn: 'end_of_turn'>, tool_calls=[ToolCall(call_id='<UUID>', tool_name=<BuiltinTool.code_interpreter: 'code_interpreter'>, arguments={'code': 'import pandas as pd\\nimport matplotlib.pyplot as plt\\n\\n# Load data\\ndf = pd.read_csv(\"<TEMP_FILE>\")\\n\\n# Convert \\'Year\\' column to datetime\\ndf[\\'Year\\'] = pd.to_datetime(df[\\'Year\\'])\\n\\n# Group by \\'Year\\' and calculate average inflation\\naverage_inflation = df.groupby(\\'Year\\')[\\'Inflation\\'].mean().reset_index()\\n\\n# Plot average yearly inflation as a time series\\nplt.figure(figsize=(10,6))\\nplt.plot(average_inflation[\\'Year\\'], average_inflation[\\'Inflation\\'], marker=\\'o\\')\\nplt.title(\\'Average Yearly Inflation\\')\\nplt.xlabel(\\'Year\\')\\nplt.ylabel(\\'Inflation Rate\\')\\nplt.grid(True)\\nplt.show()'})]), ToolResponseMessage(role='tool', call_id='<UUID>', tool_name=<BuiltinTool.code_interpreter: 'code_interpreter'>, content=\"completed\\n[stderr]\\nTraceback (most recent call last):\\n  line 5, in <module>\\n    from bwrap.core import main\\nModuleNotFoundError: No module named 'bwrap.core'\\n[/stderr]\")])_[('response_format', None), ('sampling_params', SamplingParams(strategy=TopPSamplingStrategy(type='top_p', temperature=0.0001, top_p=0.9), max_tokens=0, repetition_penalty=1.0)), ('stream', True), ('tool_config', ToolConfig(tool_choice=<ToolChoice.auto: 'auto'>, tool_prompt_format=None, system_message_behavior=<SystemMessageBehavior.append: 'append'>)), ('tool_prompt_format', None), ('tools', [ToolDefinition(tool_name=<BuiltinTool.code_interpreter: 'code_interpreter'>, description='Execute code', parameters={'code': ToolParamDefinition(param_type='string', description='The code to execute', required=True, default=None)})])]": {
    "chunks": [
      {
        "event": {
          "delta": {
            "text": "",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "start"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "It",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " seems that the file \"/var/folders/cz/v",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "yh7y1d11xg",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "881lsxsshnc5c0000gn/T/tmpcpbe",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "4t2d/GlI6X8jKinflation",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": ".csv\" does not exist. \n\nTo plot the average yearly inflation",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " as a time series, you need to provide the actual file path",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " or the file itself. If you are using a remote server,",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " you can use the `requests` library to download the file and",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " then load it into a pandas dataframe. \n\nHere is",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " an example of how you can do it:\n\n```\nimport pandas",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " as pd\nimport matplotlib.pyplot as plt\nimport requests\n\n# Download",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " the csv file\nurl = \"https://example.com/your_file",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": ".csv\"\nresponse = requests.get(url)\n\n#",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " Load the csv file into a pandas dataframe\ndf = pd.read",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "_csv(response.content)\n\n# Convert 'Year' column to datetime\ndf['Year",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "'] = pd.to_datetime(df['Year'])\n\n#",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " Group by 'Year' and calculate average inflation",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "\naverage_inflation = df.groupby('Year')['Inflation'].",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "mean().reset_index()\n\n# Plot average yearly",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " inflation as a time series\n",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "plt.figure(figsize=(10,6))\nplt",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": ".plot(average_inflation['Year'], average_inflation['In",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "flation'], marker='o')\nplt.title('Average Yearly In",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "flation')\nplt.xlabel('Year')\nplt.ylabel('Inflation Rate",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "')\nplt.grid(True)\nplt.show()\n```\n\nPlease replace the",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " `url` variable with the actual URL of your csv file.",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " \n\nIf you are using a local file, you can simply use",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " the `pd.read_csv()` function with the file path:\n\n```\n",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " the csv file into a pandas dataframe\ndf = pd.read_csv",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "('your_file.csv')\n\n# Convert 'Year' column to",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " datetime",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "\n",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "df['Year'] = pd.to_datetime(df['Year'])\n\n# Group by 'Year' and calculate average inflation\naverage_inflation = df.groupby('Year')['Inflation'].mean().reset_index()\n\n# Plot average yearly inflation as a time series\nplt.figure(figsize=(10,6))\nplt.plot(average_inflation['Year'], average_inflation['Inflation'], marker='o')\nplt.title('Average Yearly Inflation')\nplt.xlabel('Year')\n",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "plt.ylabel('Inflation Rate')\nplt.grid(True)\nplt.show",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "()\n```\n\nPlease replace `'your_file.csv'` with the",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " actual path to your csv file.",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "complete"
          },
          "logprobs": null,
          "stop_reason": {
            "__enum__": "StopReason",
            "value": "end_of_turn"
          }
        },
        "metrics": null
      }
    ],
    "type": "generator"
  },
  "('meta-llama/Llama-3.1-8B-Instruct', [SystemMessage(role='system', content='You are a helpful assistant'), UserMessage(role='user', content='Here is a csv, can you describe it?', context=None), CompletionMessage(role='assistant', content='', stop_reason=<StopReason.end_of_turn: 'end_of_turn'>, tool_calls=[ToolCall(call_id='<UUID>', tool_name=<BuiltinTool.code_interpreter: 'code_interpreter'>, arguments={'code': 'import pandas as pd\\n# Load data\\ndf = pd.read_csv(\"<TEMP_FILE>\")\\n# Rows\\nprint(\"Number of rows and columns in the data:\", df.shape)\\n# Columns\\nprint(\"Columns of the data are:\", len(df.columns))\\n# Column names\\nprint(\"Columns of the data are:\", df.columns)\\n# Column dtypes\\nprint(\"Datatype of the columns are:\", df.dtypes)'})]), ToolResponseMessage(role='tool', call_id='<UUID>', tool_name=<BuiltinTool.code_interpreter: 'code_interpreter'>, content=\"completed\\n[stderr]\\nTraceback (most recent call last):\\n  line 5, in <module>\\n    from bwrap.core import main\\nModuleNotFoundError: No module named 'bwrap.core'\\n[/stderr]\"), CompletionMessage(role='assistant', content='It seems that the file \"<TEMP_FILE>\" does not exist. \\n\\nTo describe the csv file, you need to provide the actual file path or the file itself. If you are using a remote server, you can use the `requests` library to download the file and then load it into a pandas dataframe. \\n\\nHere is an example of how you can do it:\\n\\n```\\nimport pandas as pd\\nimport requests\\n\\n# Download the csv file\\nurl = \"https://example.com/your_file.csv\"\\nresponse = requests.get(url)\\n\\n# Load the csv file into a pandas dataframe\\ndf = pd.read_csv(response.content)\\n\\n# Print the description of the dataframe\\nprint(df.describe())\\n```\\n\\nPlease replace the `url` variable with the actual URL of your csv file. \\n\\nIf you are using a local file, you can simply use the `pd.read_csv()` function with the file path:\\n\\n```\\nimport pandas as pd\\n\\n# Load the csv file into a pandas dataframe\\ndf = pd.read_csv(\\'your_file.csv\\')\\n\\n# Print the description of the dataframe\\nprint(df.describe())\\n```\\n\\nPlease replace `\\'your_file.csv\\'` with the actual path to your csv file.', stop_reason=<StopReason.end_of_turn: 'end_of_turn'>, tool_calls=[]), UserMessage(role='user', content='Plot average yearly inflation as a time series', context=None)])_[('response_format', None), ('sampling_params', SamplingParams(strategy=TopPSamplingStrategy(type='top_p', temperature=0.0001, top_p=0.9), max_tokens=0, repetition_penalty=1.0)), ('stream', True), ('tool_config', ToolConfig(tool_choice=<ToolChoice.auto: 'auto'>, tool_prompt_format=None, system_message_behavior=<SystemMessageBehavior.append: 'append'>)), ('tool_prompt_format', None), ('tools', [ToolDefinition(tool_name=<BuiltinTool.code_interpreter: 'code_interpreter'>, description='Execute code', parameters={'code': ToolParamDefinition(param_type='string', description='The code to execute', required=True, default=None)})])]": {
    "chunks": [
      {
        "event": {
          "delta": {
            "text": "",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "start"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "started"
            },
            "tool_call": "",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "in_progress"
            },
            "tool_call": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n#",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "in_progress"
            },
            "tool_call": " Load data\ndf = pd.read_csv(\"/var/folders/c",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "in_progress"
            },
            "tool_call": "z/vyh7y1d11xg881lsx",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "in_progress"
            },
            "tool_call": "sshnc5c0000gn/T/tmp",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "in_progress"
            },
            "tool_call": "cpbe4t2d/GlI6X8j",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "in_progress"
            },
            "tool_call": "Kinflation.csv\")\n\n# Convert 'Year' column to datetime\n",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "in_progress"
            },
            "tool_call": "df['Year'] = pd.to_datetime(df",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "in_progress"
            },
            "tool_call": "['Year'])\n\n# Group by 'Year' and calculate average inflation",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "in_progress"
            },
            "tool_call": "\naverage_inflation = df.groupby('Year')['Inflation",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "in_progress"
            },
            "tool_call": "'].mean().reset_index()\n\n# Plot average yearly inflation as a",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "in_progress"
            },
            "tool_call": " time series\nplt.figure(figsize=(10,6))\nplt",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "in_progress"
            },
            "tool_call": ".plot(average_inflation['Year'], average",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "in_progress"
            },
            "tool_call": "_inflation['Inflation'], marker='o')\nplt.title('",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "in_progress"
            },
            "tool_call": "Average Yearly Inflation')\nplt.xlabel('Year')\nplt.ylabel",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "in_progress"
            },
            "tool_call": "('Inflation Rate')\nplt.grid(True)\nplt.show()",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "succeeded"
            },
            "tool_call": {
              "arguments": {
                "code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load data\ndf = pd.read_csv(\"/var/folders/cz/vyh7y1d11xg881lsxsshnc5c0000gn/T/tmpcpbe4t2d/GlI6X8jKinflation.csv\")\n\n# Convert 'Year' column to datetime\ndf['Year'] = pd.to_datetime(df['Year'])\n\n# Group by 'Year' and calculate average inflation\naverage_inflation = df.groupby('Year')['Inflation'].mean().reset_index()\n\n# Plot average yearly inflation as a time series\nplt.figure(figsize=(10,6))\nplt.plot(average_inflation['Year'], average_inflation['Inflation'], marker='o')\nplt.title('Average Yearly Inflation')\nplt.xlabel('Year')\nplt.ylabel('Inflation Rate')\nplt.grid(True)\nplt.show()"
              },
              "call_id": "e91eb9fe-6f94-4f8b-b02a-a67eb2672895",
              "tool_name": {
                "__enum__": "BuiltinTool",
                "value": "code_interpreter"
              }
            },
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": {
            "__enum__": "StopReason",
            "value": "end_of_turn"
          }
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "complete"
          },
          "logprobs": null,
          "stop_reason": {
            "__enum__": "StopReason",
            "value": "end_of_turn"
          }
        },
        "metrics": null
      }
    ],
    "type": "generator"
  },
  "('meta-llama/Llama-3.1-8B-Instruct', [SystemMessage(role='system', content='You are a helpful assistant'), UserMessage(role='user', content='Here is a csv, can you describe it?', context=None), CompletionMessage(role='assistant', content='', stop_reason=<StopReason.end_of_turn: 'end_of_turn'>, tool_calls=[ToolCall(call_id='<UUID>', tool_name=<BuiltinTool.code_interpreter: 'code_interpreter'>, arguments={'code': 'import pandas as pd\\n# Load data\\ndf = pd.read_csv(\"<TEMP_FILE>\")\\n# Rows\\nprint(\"Number of rows and columns in the data:\", df.shape)\\n# Columns\\nprint(\"Columns of the data are:\", len(df.columns))\\n# Column names\\nprint(\"Columns of the data are:\", df.columns)\\n# Column dtypes\\nprint(\"Datatype of the columns are:\", df.dtypes)'})]), ToolResponseMessage(role='tool', call_id='<UUID>', tool_name=<BuiltinTool.code_interpreter: 'code_interpreter'>, content=\"completed\\n[stderr]\\nTraceback (most recent call last):\\n  line 5, in <module>\\n    from bwrap.core import main\\nModuleNotFoundError: No module named 'bwrap.core'\\n[/stderr]\"), CompletionMessage(role='assistant', content='It seems that the file \"<TEMP_FILE>\" is not accessible to me. Can you please provide the contents of the csv file or a link to it, so I can help you describe it?', stop_reason=<StopReason.end_of_turn: 'end_of_turn'>, tool_calls=[]), UserMessage(role='user', content='Plot average yearly inflation as a time series', context=None), CompletionMessage(role='assistant', content='', stop_reason=<StopReason.end_of_turn: 'end_of_turn'>, tool_calls=[ToolCall(call_id='<UUID>', tool_name=<BuiltinTool.code_interpreter: 'code_interpreter'>, arguments={'code': 'import pandas as pd\\nimport matplotlib.pyplot as plt\\n\\n# Load data\\ndf = pd.read_csv(\"<TEMP_FILE>\")\\n\\n# Convert \\'Year\\' column to datetime\\ndf[\\'Year\\'] = pd.to_datetime(df[\\'Year\\'])\\n\\n# Group by \\'Year\\' and calculate average inflation\\navg_inflation = df.groupby(\\'Year\\')[\\'Inflation\\'].mean().reset_index()\\n\\n# Plot time series\\nplt.figure(figsize=(10,6))\\nplt.plot(avg_inflation[\\'Year\\'], avg_inflation[\\'Inflation\\'], marker=\\'o\\')\\nplt.title(\\'Average Yearly Inflation\\')\\nplt.xlabel(\\'Year\\')\\nplt.ylabel(\\'Inflation Rate\\')\\nplt.grid(True)\\nplt.show()'})]), ToolResponseMessage(role='tool', call_id='<UUID>', tool_name=<BuiltinTool.code_interpreter: 'code_interpreter'>, content=\"completed\\n[stderr]\\nTraceback (most recent call last):\\n  line 5, in <module>\\n    from bwrap.core import main\\nModuleNotFoundError: No module named 'bwrap.core'\\n[/stderr]\")])_[('response_format', None), ('sampling_params', SamplingParams(strategy=TopPSamplingStrategy(type='top_p', temperature=0.0001, top_p=0.9), max_tokens=0, repetition_penalty=1.0)), ('stream', True), ('tool_config', ToolConfig(tool_choice=<ToolChoice.auto: 'auto'>, tool_prompt_format=None, system_message_behavior=<SystemMessageBehavior.append: 'append'>)), ('tool_prompt_format', None), ('tools', [ToolDefinition(tool_name=<BuiltinTool.code_interpreter: 'code_interpreter'>, description='Execute code', parameters={'code': ToolParamDefinition(param_type='string', description='The code to execute', required=True, default=None)})])]": {
    "chunks": [
      {
        "event": {
          "delta": {
            "text": "",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "start"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "It",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " seems that the file \"/var/folders/cz/vyh7",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "y1d11xg881lsxsshnc5c0000gn",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "/T/tmp8ch1hcym/QBy4nTyhinflation",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": ".csv\" is not accessible to me.",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " Can you please provide the contents of the csv file or a link to",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " it, so I can help you plot the average yearly inflation as a time",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " series?",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "complete"
          },
          "logprobs": null,
          "stop_reason": {
            "__enum__": "StopReason",
            "value": "end_of_turn"
          }
        },
        "metrics": null
      }
    ],
    "type": "generator"
  },
  "('meta-llama/Llama-3.1-8B-Instruct', [SystemMessage(role='system', content='You are a helpful assistant'), UserMessage(role='user', content='Here is a csv, can you describe it?', context=None), CompletionMessage(role='assistant', content='', stop_reason=<StopReason.end_of_turn: 'end_of_turn'>, tool_calls=[ToolCall(call_id='<UUID>', tool_name=<BuiltinTool.code_interpreter: 'code_interpreter'>, arguments={'code': 'import pandas as pd\\n# Load data\\ndf = pd.read_csv(\"<TEMP_FILE>\")\\n# Rows\\nprint(\"Number of rows and columns in the data:\", df.shape)\\n# Columns\\nprint(\"Columns of the data are:\", len(df.columns))\\n# Column names\\nprint(\"Columns of the data are:\", df.columns)\\n# Column dtypes\\nprint(\"Datatype of the columns are:\", df.dtypes)'})]), ToolResponseMessage(role='tool', call_id='<UUID>', tool_name=<BuiltinTool.code_interpreter: 'code_interpreter'>, content=\"completed\\n[stderr]\\nTraceback (most recent call last):\\n  line 5, in <module>\\n    from bwrap.core import main\\nModuleNotFoundError: No module named 'bwrap.core'\\n[/stderr]\"), CompletionMessage(role='assistant', content='It seems that the file \"<TEMP_FILE>\" is not accessible to me. Can you please provide the contents of the csv file or a link to it, so I can help you describe it?', stop_reason=<StopReason.end_of_turn: 'end_of_turn'>, tool_calls=[]), UserMessage(role='user', content='Plot average yearly inflation as a time series', context=None)])_[('response_format', None), ('sampling_params', SamplingParams(strategy=TopPSamplingStrategy(type='top_p', temperature=0.0001, top_p=0.9), max_tokens=0, repetition_penalty=1.0)), ('stream', True), ('tool_config', ToolConfig(tool_choice=<ToolChoice.auto: 'auto'>, tool_prompt_format=None, system_message_behavior=<SystemMessageBehavior.append: 'append'>)), ('tool_prompt_format', None), ('tools', [ToolDefinition(tool_name=<BuiltinTool.code_interpreter: 'code_interpreter'>, description='Execute code', parameters={'code': ToolParamDefinition(param_type='string', description='The code to execute', required=True, default=None)})])]": {
    "chunks": [
      {
        "event": {
          "delta": {
            "text": "",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "start"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "started"
            },
            "tool_call": "",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "in_progress"
            },
            "tool_call": "import pandas as pd\nimport matplotlib.pyplot as plt",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "in_progress"
            },
            "tool_call": "\n\n# Load data\ndf = pd.read_csv(\"/var/f",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "in_progress"
            },
            "tool_call": "olders/cz/vyh7y1d11xg881",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "in_progress"
            },
            "tool_call": "lsxsshnc5c0000gn/T/tmp",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "in_progress"
            },
            "tool_call": "8ch1hcym/QBy4nTyhinflation",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "in_progress"
            },
            "tool_call": ".csv\")\n\n# Convert 'Year' column",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "in_progress"
            },
            "tool_call": " to datetime\ndf['Year']",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "in_progress"
            },
            "tool_call": " = pd.to_datetime(df['Year'])\n\n",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "in_progress"
            },
            "tool_call": "# Group by 'Year' and calculate average",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "in_progress"
            },
            "tool_call": " inflation\navg_inflation = df.groupby('Year')['Inflation'].mean",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "in_progress"
            },
            "tool_call": "().reset_index()\n\n# Plot time series\nplt.figure(figsize=(10,",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "in_progress"
            },
            "tool_call": "6))\nplt.plot(avg_inflation['Year'], avg_inflation['In",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "in_progress"
            },
            "tool_call": "flation'], marker='o')\nplt.title('",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "in_progress"
            },
            "tool_call": "Average Yearly Inflation')\nplt.xlabel",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "in_progress"
            },
            "tool_call": "('Year')\nplt.ylabel('Inflation Rate')\nplt.grid",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "in_progress"
            },
            "tool_call": "(True)\nplt.show()",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "succeeded"
            },
            "tool_call": {
              "arguments": {
                "code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load data\ndf = pd.read_csv(\"/var/folders/cz/vyh7y1d11xg881lsxsshnc5c0000gn/T/tmp8ch1hcym/QBy4nTyhinflation.csv\")\n\n# Convert 'Year' column to datetime\ndf['Year'] = pd.to_datetime(df['Year'])\n\n# Group by 'Year' and calculate average inflation\navg_inflation = df.groupby('Year')['Inflation'].mean().reset_index()\n\n# Plot time series\nplt.figure(figsize=(10,6))\nplt.plot(avg_inflation['Year'], avg_inflation['Inflation'], marker='o')\nplt.title('Average Yearly Inflation')\nplt.xlabel('Year')\nplt.ylabel('Inflation Rate')\nplt.grid(True)\nplt.show()"
              },
              "call_id": "69f7163b-7f58-472e-b02c-a02216fcbdc7",
              "tool_name": {
                "__enum__": "BuiltinTool",
                "value": "code_interpreter"
              }
            },
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": {
            "__enum__": "StopReason",
            "value": "end_of_turn"
          }
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "complete"
          },
          "logprobs": null,
          "stop_reason": {
            "__enum__": "StopReason",
            "value": "end_of_turn"
          }
        },
        "metrics": null
      }
    ],
    "type": "generator"
  },
  "('meta-llama/Llama-3.1-8B-Instruct', [SystemMessage(role='system', content='You are a helpful assistant'), UserMessage(role='user', content='Here is a csv, can you describe it?', context=None), ToolResponseMessage(role='tool', call_id='', tool_name=<BuiltinTool.code_interpreter: 'code_interpreter'>, content=[TextContentItem(type='text', text='# User provided a file accessible to you at \"<TEMP_FILE>\"\\nYou can use code_interpreter to load and inspect it.')]), CompletionMessage(role='assistant', content='', stop_reason=<StopReason.end_of_turn: 'end_of_turn'>, tool_calls=[ToolCall(call_id='<UUID>', tool_name=<BuiltinTool.code_interpreter: 'code_interpreter'>, arguments={'code': 'import pandas as pd\\n# Load data\\ndf = pd.read_csv(\"<TEMP_FILE>\")\\n# Rows\\nprint(\"Number of rows and columns in the data:\", df.shape)\\n# Columns\\nprint(\"Columns of the data are:\", len(df.columns))\\n# Column names\\nprint(\"Columns of the data are:\", df.columns)\\n# Column dtypes\\nprint(\"Datatype of the columns are:\", df.dtypes)'})]), ToolResponseMessage(role='tool', call_id='<UUID>', tool_name=<BuiltinTool.code_interpreter: 'code_interpreter'>, content=\"completed\\n[stderr]\\nTraceback (most recent call last):\\n  line 5, in <module>\\n    from bwrap.core import main\\nModuleNotFoundError: No module named 'bwrap.core'\\n[/stderr]\")])_[('response_format', None), ('sampling_params', SamplingParams(strategy=TopPSamplingStrategy(type='top_p', temperature=0.0001, top_p=0.9), max_tokens=0, repetition_penalty=1.0)), ('stream', True), ('tool_config', ToolConfig(tool_choice=<ToolChoice.auto: 'auto'>, tool_prompt_format=None, system_message_behavior=<SystemMessageBehavior.append: 'append'>)), ('tool_prompt_format', None), ('tools', [ToolDefinition(tool_name=<BuiltinTool.code_interpreter: 'code_interpreter'>, description='Execute code', parameters={'code': ToolParamDefinition(param_type='string', description='The code to execute', required=True, default=None)})])]": {
    "chunks": [
      {
        "event": {
          "delta": {
            "text": "",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "start"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "It",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " seems that the file \"/var/folders/cz/vyh7y1",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "d11xg881lsxsshnc5",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "c0000gn/T/tmpcpbe4t2d/Gl",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "I6X8jKinflation.csv\" does not exist. \n\nTo",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " describe the csv file, you need to provide the actual file path or the",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " file itself. If you are using a remote server, you can use the",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " `requests` library to download the file and then load it into a pandas",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " dataframe. \n\nHere is an example of how you can do",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " it:\n\n```\nimport pandas as pd\nimport requests\n\n# Download",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " the csv file\nurl = \"https://example.com/your_file.csv",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "\"\nresponse = requests.get(url)\n\n# Load the",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " csv file into a pandas dataframe\ndf",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " = pd.read_csv(response.content)\n\n#",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " Print the description of the dataframe\nprint(df.describe())\n```\n\n",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "Please replace the `url` variable with the",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " actual URL of your csv file. \n\nIf you are using a",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " local file, you can simply use the `pd.read_csv",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "()` function with the file path:\n\n``",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "`\nimport pandas as pd\n\n# Load",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " the csv file into a pandas dataframe\ndf = pd.read_csv",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "('your_file.csv')\n\n# Print the description of the dataframe\n",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "print(df.describe())\n```\n\nPlease replace `'your_file.csv'",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "` with the actual path to your csv file.",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "complete"
          },
          "logprobs": null,
          "stop_reason": {
            "__enum__": "StopReason",
            "value": "end_of_turn"
          }
        },
        "metrics": null
      }
    ],
    "type": "generator"
  },
  "('meta-llama/Llama-3.1-8B-Instruct', [SystemMessage(role='system', content='You are a helpful assistant'), UserMessage(role='user', content='Here is a csv, can you describe it?', context=None), ToolResponseMessage(role='tool', call_id='', tool_name=<BuiltinTool.code_interpreter: 'code_interpreter'>, content=[TextContentItem(type='text', text='# User provided a file accessible to you at \"<TEMP_FILE>\"\\nYou can use code_interpreter to load and inspect it.')])])_[('response_format', None), ('sampling_params', SamplingParams(strategy=TopPSamplingStrategy(type='top_p', temperature=0.0001, top_p=0.9), max_tokens=0, repetition_penalty=1.0)), ('stream', True), ('tool_config', ToolConfig(tool_choice=<ToolChoice.auto: 'auto'>, tool_prompt_format=None, system_message_behavior=<SystemMessageBehavior.append: 'append'>)), ('tool_prompt_format', None), ('tools', [ToolDefinition(tool_name=<BuiltinTool.code_interpreter: 'code_interpreter'>, description='Execute code', parameters={'code': ToolParamDefinition(param_type='string', description='The code to execute', required=True, default=None)})])]": {
    "chunks": [
      {
        "event": {
          "delta": {
            "text": "",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "start"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "started"
            },
            "tool_call": "",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "in_progress"
            },
            "tool_call": "import pandas as pd\n# Load data\ndf = pd",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "in_progress"
            },
            "tool_call": ".read_csv(\"/var/folders/cz/vyh7y",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "in_progress"
            },
            "tool_call": "1d11xg881lsxssh",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "in_progress"
            },
            "tool_call": "nc5c0000gn/T/tmp",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "in_progress"
            },
            "tool_call": "cpbe4t2d/GlI6X8j",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "in_progress"
            },
            "tool_call": "Kinflation.csv\")\n# Rows\nprint(\"Number of rows and columns in",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "in_progress"
            },
            "tool_call": " the data:\", df.shape)\n# Columns\nprint(\"Columns of the data",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "in_progress"
            },
            "tool_call": " are:\", len(df.columns))\n# Column names\nprint(\"Columns of the",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "in_progress"
            },
            "tool_call": " data are:\", df.columns)\n# Column dtypes\nprint(\"Datatype",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "in_progress"
            },
            "tool_call": " of the columns are:\", df.d",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "in_progress"
            },
            "tool_call": "types)",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "succeeded"
            },
            "tool_call": {
              "arguments": {
                "code": "import pandas as pd\n# Load data\ndf = pd.read_csv(\"/var/folders/cz/vyh7y1d11xg881lsxsshnc5c0000gn/T/tmpcpbe4t2d/GlI6X8jKinflation.csv\")\n# Rows\nprint(\"Number of rows and columns in the data:\", df.shape)\n# Columns\nprint(\"Columns of the data are:\", len(df.columns))\n# Column names\nprint(\"Columns of the data are:\", df.columns)\n# Column dtypes\nprint(\"Datatype of the columns are:\", df.dtypes)"
              },
              "call_id": "18a4d460-7f3a-4b19-af67-ea3d105a274d",
              "tool_name": {
                "__enum__": "BuiltinTool",
                "value": "code_interpreter"
              }
            },
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": {
            "__enum__": "StopReason",
            "value": "end_of_turn"
          }
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "complete"
          },
          "logprobs": null,
          "stop_reason": {
            "__enum__": "StopReason",
            "value": "end_of_turn"
          }
        },
        "metrics": null
      }
    ],
    "type": "generator"
  },
  "('meta-llama/Llama-3.1-8B-Instruct', [SystemMessage(role='system', content='You are a helpful assistant'), UserMessage(role='user', content='I am attaching some documentation for Torchtune. Help me answer questions I will ask next.', context=None), CompletionMessage(role='assistant', content='', stop_reason=<StopReason.end_of_turn: 'end_of_turn'>, tool_calls=[ToolCall(call_id='<UUID>', tool_name='knowledge_search', arguments={'query': 'Torchtune documentation'})]), ToolResponseMessage(role='tool', call_id='<UUID>', tool_name='knowledge_search', content=[TextContentItem(type='text', text='knowledge_search tool found 5 chunks:\\nBEGIN of knowledge_search tool results.\\n'), TextContentItem(type='text', text='Result 1:\\nDocument_id:e70e0\\nContent:  conversational data, :func:`~torchtune.datasets.chat_dataset` seems to be a good fit. For any\\ncustom local dataset we always need to specify ``source``, ``data_files``, and ``split`` for any dataset\\nbuilder in torchtune. For :func:`~torchtune.datasets.chat_dataset`, we additionally need to specify\\n``conversation_column`` and ``conversation_style``. Our data follows the ``\"sharegpt\"`` format, so\\nwe can specify that here. Altogether, our :func:`~torchtune.datasets.chat_dataset` call should\\nlook like so:\\n\\n.. code-block:: python\\n\\n    from torchtune.datasets import chat_dataset\\n    from torchtune.models.llama3 import llama3_tokenizer\\n\\n    tokenizer = llama3_tokenizer(\"/tmp/Meta-Llama-3-8B-Instruct/original/tokenizer.model\")\\n    ds = chat_dataset(\\n        tokenizer=tokenizer,\\n        source=\"json\",\\n        data_files=\"data/my_data.json\",\\n        split=\"train\",\\n        conversation_column=\"dialogue\",\\n        conversation_style=\"sharegpt\",\\n    )\\n\\n.. code-block:: yaml\\n\\n    # In config\\n    tokenizer:\\n      _component_: torchtune.models.llama3.llama3_tokenizer\\n      path: /tmp/Meta-Llama-3-8B-Instruct/original/tokenizer.model\\n\\n    dataset:\\n      _component_: torchtune.datasets.chat_dataset\\n      source: json\\n      data_files: data/my_data.json\\n      split: train\\n      conversation_column: dialogue\\n      conversation_style: sharegpt\\n\\n.. note::\\n    You can pass in any keyword argument for `load_dataset <https://huggingface.co/docs/datasets/v2.20.0/en/package_reference/loading_methods#datasets.load_dataset>`_ into all our\\n    Dataset classes and they will honor them. This is useful for common parameters\\n    such as specifying the data split with :code:`split` or configuration with\\n    :code:`name`\\n\\nIf you needed to add a prompt template, you would simply pass it into the tokenizer.\\nSince we\\'re fine-tuning Llama3, the tokenizer will handle all formatting for\\nus and prompt templates are optional. Other models such as Mistral\\'s :class:`~torchtune.models.mistral._tokenizer.MistralTokenizer`,\\nuse a chat template by default (:class:`~torchtune.models.mistral.MistralChatTemplate`) to format\\nall messages according to their `recommendations <https://\\n'), TextContentItem(type='text', text=\"Result 2:\\nDocument_id:f8ecb\\nContent: .. _lora_finetune_label:\\n\\n============================\\nFine-Tuning Llama2 with LoRA\\n============================\\n\\nThis guide will teach you about `LoRA <https://arxiv.org/abs/2106.09685>`_, a parameter-efficient finetuning technique,\\nand show you how you can use torchtune to finetune a Llama2 model with LoRA.\\nIf you already know what LoRA is and want to get straight to running\\nyour own LoRA finetune in torchtune, you can jump to :ref:`LoRA finetuning recipe in torchtune<lora_recipe_label>`.\\n\\n.. grid:: 2\\n\\n    .. grid-item-card:: :octicon:`mortar-board;1em;` What you will learn\\n\\n      * What LoRA is and how it saves memory during finetuning\\n      * An overview of LoRA components in torchtune\\n      * How to run a LoRA finetune using torchtune\\n      * How to experiment with different LoRA configurations\\n\\n    .. grid-item-card:: :octicon:`list-unordered;1em;` Prerequisites\\n\\n      * Be familiar with :ref:`torchtune<overview_label>`\\n      * Make sure to :ref:`install torchtune<install_label>`\\n      * Make sure you have downloaded the :ref:`Llama2-7B model weights<download_llama_label>`\\n\\nWhat is LoRA?\\n-------------\\n\\n`LoRA <https://arxiv.org/abs/2106.09685>`_ is an adapter-based method for\\nparameter-efficient finetuning that adds trainable low-rank decomposition matrices to different layers of a neural network,\\nthen freezes the network's remaining parameters. LoRA is most commonly applied to\\ntransformer models, in which case it is common to add the low-rank matrices\\nto some of the linear projections in each transformer layer's self-attention.\\n\\n.. note::\\n\\n    If you're unfamiliar, check out these references for the `definition of rank <https://en.wikipedia.org/wiki/Rank_(linear_algebra)>`_\\n    and discussion of `low-rank approximations <https://en.wikipedia.org/wiki/Low-rank_approximation>`_.\\n\\nBy finetuning with LoRA (as opposed to finetuning all model parameters),\\nyou can expect to see memory savings due to a substantial reduction in the\\nnumber of parameters with gradients. When using an optimizer with momentum,\\nlike `AdamW <https://py\\n\"), TextContentItem(type='text', text='Result 3:\\nDocument_id:e187e\\nContent: ` module, which we swap\\n   out for :class:`~torchtune.modules.peft.LoRALinear` when ``use_dora=True``.\\n\\n.. _glossary_distrib:\\n\\n\\n.. TODO\\n\\n.. Distributed\\n.. -----------\\n\\n.. .. _glossary_fsdp:\\n\\n.. Fully Sharded Data Parallel (FSDP)\\n.. ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\\n.. All our ``_distributed`` recipes use `FSDP <https://pytorch.org/docs/stable/fsdp.html>`.\\n.. .. _glossary_fsdp2:\\n\\n'), TextContentItem(type='text', text='Result 4:\\nDocument_id:f8ecb\\nContent: 06% of all params are trainable.\\n\\n.. note::\\n    If you are directly using the LoRA recipe (as detailed :ref:`here<lora_recipe_label>`), you need only pass the\\n    relevant checkpoint path. Loading model weights and setting trainable parameters will be taken care\\n    of in the recipe.\\n\\n\\n.. _lora_recipe_label:\\n\\nLoRA finetuning recipe in torchtune\\n-----------------------------------\\n\\nFinally, we can put it all together and finetune a model using torchtune\\'s `LoRA recipe <https://github.com/pytorch/torchtune/blob/48626d19d2108f92c749411fbd5f0ff140023a25/recipes/lora_finetune.py>`_.\\nMake sure that you have first downloaded the Llama2 weights and tokenizer by following :ref:`these instructions<download_llama_label>`.\\nYou can then run the following command to perform a LoRA finetune of Llama2-7B with two GPUs (each having VRAM of at least 16GB):\\n\\n.. code-block:: bash\\n\\n    tune run --nnodes 1 --nproc_per_node 2 lora_finetune_distributed --config llama2/7B_lora\\n\\n.. note::\\n    Make sure to point to the location of your Llama2 weights and tokenizer. This can be done\\n    either by adding :code:`checkpointer.checkpoint_files=[my_model_checkpoint_path] tokenizer_checkpoint=my_tokenizer_checkpoint_path`\\n    or by directly modifying the :code:`7B_lora.yaml` file. See our \"\":ref:`config_tutorial_label`\" recipe\\n    for more details on how you can easily clone and modify torchtune configs.\\n\\n.. note::\\n    You can modify the value of :code:`nproc_per_node` depending on (a) the number of GPUs you have available,\\n    and (b) the memory constraints of your hardware.\\n\\nThe preceding command will run a LoRA finetune with torchtune\\'s factory settings, but we may want to experiment a bit.\\nLet\\'s take a closer look at some of the :code:`lora_finetune_distributed` config.\\n\\n.. code-block:: yaml\\n\\n  # Model Arguments\\n  model:\\n    _component_: lora_llama2_7b\\n    lora_attn_modules: [\\'q_proj\\', \\'v_proj\\']\\n    lora_rank: 8\\n    lora_alpha: 16\\n  ...\\n\\nWe see that the\\n'), TextContentItem(type='text', text='Result 5:\\nDocument_id:e187e\\nContent: etune\\n:func:`torchtune.models.llama3.llama3_8b` with DoRA, you would use :func:`torchtune.models.llama3.lora_llama3_8b` with ``use_dora=True``:\\n\\n.. code-block:: bash\\n\\n  tune run lora_finetune_single_device --config llama3/8B_lora_single_device \\\\\\n  model.use_dora=True\\n\\n.. code-block:: yaml\\n\\n  model:\\n    _component_: torchtune.models.lora_llama3_8b\\n    use_dora: True\\n\\nSince DoRA extends LoRA, the parameters for :ref:`customizing LoRA <glossary_lora>` are identical. You can also quantize the base model weights like in :ref:`glossary_qlora` by using ``quantize=True`` to reap\\neven more memory savings!\\n\\n.. code-block:: bash\\n\\n  tune run lora_finetune_single_device --config llama3/8B_lora_single_device \\\\\\n  model.apply_lora_to_mlp=True \\\\\\n  model.lora_attn_modules=[\"q_proj\",\"k_proj\",\"v_proj\"] \\\\\\n  model.lora_rank=16 \\\\\\n  model.lora_alpha=32 \\\\\\n  model.use_dora=True \\\\\\n  model.quantize_base=True\\n\\n.. code-block:: yaml\\n\\n  model:\\n    _component_: torchtune.models.lora_llama3_8b\\n    apply_lora_to_mlp: True\\n    lora_attn_modules: [\"q_proj\", \"k_proj\", \"v_proj\"]\\n    lora_rank: 16\\n    lora_alpha: 32\\n    use_dora: True\\n    quantize_base: True\\n\\n\\n.. note::\\n\\n   Under the hood, we\\'ve enabled DoRA by adding the :class:`~torchtune.modules.peft.DoRALinear` module, which we swap\\n   out for :class:`~torchtune.modules.peft.LoRALinear` when ``use_dora=True``.\\n\\n.. _glossary_distrib:\\n\\n\\n.. TODO\\n\\n.. Distributed\\n.. -----------\\n\\n.. .. _glossary_fsdp:\\n\\n.. Fully Sharded Data Parallel (FSDP)\\n.. ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\\n.. All our ``_distributed`` recipes use `FSDP <https://pytorch.org/docs/stable/fsdp.html>`.\\n.. .. _glossary_fsdp2:\\n\\n'), TextContentItem(type='text', text='END of knowledge_search tool results.\\n')]), CompletionMessage(role='assistant', content='You can use the following function call to answer the user\\'s question:\\n\\n{\"type\": \"function\", \"name\": \"knowledge_search\", \"parameters\": {\"query\": \"How to fine-tune a Llama2 model with LoRA in torchtune\"}}', stop_reason=<StopReason.end_of_turn: 'end_of_turn'>, tool_calls=[]), UserMessage(role='user', content='Tell me how to use LoRA', context=None), CompletionMessage(role='assistant', content='', stop_reason=<StopReason.end_of_turn: 'end_of_turn'>, tool_calls=[ToolCall(call_id='<UUID>', tool_name='knowledge_search', arguments={'query': 'How to use LoRA'})]), ToolResponseMessage(role='tool', call_id='<UUID>', tool_name='knowledge_search', content=[TextContentItem(type='text', text='knowledge_search tool found 5 chunks:\\nBEGIN of knowledge_search tool results.\\n'), TextContentItem(type='text', text=\"Result 1:\\nDocument_id:f8ecb\\nContent: .. _lora_finetune_label:\\n\\n============================\\nFine-Tuning Llama2 with LoRA\\n============================\\n\\nThis guide will teach you about `LoRA <https://arxiv.org/abs/2106.09685>`_, a parameter-efficient finetuning technique,\\nand show you how you can use torchtune to finetune a Llama2 model with LoRA.\\nIf you already know what LoRA is and want to get straight to running\\nyour own LoRA finetune in torchtune, you can jump to :ref:`LoRA finetuning recipe in torchtune<lora_recipe_label>`.\\n\\n.. grid:: 2\\n\\n    .. grid-item-card:: :octicon:`mortar-board;1em;` What you will learn\\n\\n      * What LoRA is and how it saves memory during finetuning\\n      * An overview of LoRA components in torchtune\\n      * How to run a LoRA finetune using torchtune\\n      * How to experiment with different LoRA configurations\\n\\n    .. grid-item-card:: :octicon:`list-unordered;1em;` Prerequisites\\n\\n      * Be familiar with :ref:`torchtune<overview_label>`\\n      * Make sure to :ref:`install torchtune<install_label>`\\n      * Make sure you have downloaded the :ref:`Llama2-7B model weights<download_llama_label>`\\n\\nWhat is LoRA?\\n-------------\\n\\n`LoRA <https://arxiv.org/abs/2106.09685>`_ is an adapter-based method for\\nparameter-efficient finetuning that adds trainable low-rank decomposition matrices to different layers of a neural network,\\nthen freezes the network's remaining parameters. LoRA is most commonly applied to\\ntransformer models, in which case it is common to add the low-rank matrices\\nto some of the linear projections in each transformer layer's self-attention.\\n\\n.. note::\\n\\n    If you're unfamiliar, check out these references for the `definition of rank <https://en.wikipedia.org/wiki/Rank_(linear_algebra)>`_\\n    and discussion of `low-rank approximations <https://en.wikipedia.org/wiki/Low-rank_approximation>`_.\\n\\nBy finetuning with LoRA (as opposed to finetuning all model parameters),\\nyou can expect to see memory savings due to a substantial reduction in the\\nnumber of parameters with gradients. When using an optimizer with momentum,\\nlike `AdamW <https://py\\n\"), TextContentItem(type='text', text='Result 2:\\nDocument_id:f8ecb\\nContent: 06% of all params are trainable.\\n\\n.. note::\\n    If you are directly using the LoRA recipe (as detailed :ref:`here<lora_recipe_label>`), you need only pass the\\n    relevant checkpoint path. Loading model weights and setting trainable parameters will be taken care\\n    of in the recipe.\\n\\n\\n.. _lora_recipe_label:\\n\\nLoRA finetuning recipe in torchtune\\n-----------------------------------\\n\\nFinally, we can put it all together and finetune a model using torchtune\\'s `LoRA recipe <https://github.com/pytorch/torchtune/blob/48626d19d2108f92c749411fbd5f0ff140023a25/recipes/lora_finetune.py>`_.\\nMake sure that you have first downloaded the Llama2 weights and tokenizer by following :ref:`these instructions<download_llama_label>`.\\nYou can then run the following command to perform a LoRA finetune of Llama2-7B with two GPUs (each having VRAM of at least 16GB):\\n\\n.. code-block:: bash\\n\\n    tune run --nnodes 1 --nproc_per_node 2 lora_finetune_distributed --config llama2/7B_lora\\n\\n.. note::\\n    Make sure to point to the location of your Llama2 weights and tokenizer. This can be done\\n    either by adding :code:`checkpointer.checkpoint_files=[my_model_checkpoint_path] tokenizer_checkpoint=my_tokenizer_checkpoint_path`\\n    or by directly modifying the :code:`7B_lora.yaml` file. See our \"\":ref:`config_tutorial_label`\" recipe\\n    for more details on how you can easily clone and modify torchtune configs.\\n\\n.. note::\\n    You can modify the value of :code:`nproc_per_node` depending on (a) the number of GPUs you have available,\\n    and (b) the memory constraints of your hardware.\\n\\nThe preceding command will run a LoRA finetune with torchtune\\'s factory settings, but we may want to experiment a bit.\\nLet\\'s take a closer look at some of the :code:`lora_finetune_distributed` config.\\n\\n.. code-block:: yaml\\n\\n  # Model Arguments\\n  model:\\n    _component_: lora_llama2_7b\\n    lora_attn_modules: [\\'q_proj\\', \\'v_proj\\']\\n    lora_rank: 8\\n    lora_alpha: 16\\n  ...\\n\\nWe see that the\\n'), TextContentItem(type='text', text='Result 3:\\nDocument_id:e187e\\nContent:  with training with LoRA quickly,\\njust specify any config with ``_lora`` in its name, e.g:\\n\\n.. code-block:: bash\\n\\n  tune run lora_finetune_single_device --config llama3/8B_lora_single_device\\n\\n\\nThere are two sets of parameters to customize LoRA to suit your needs. Firstly, the parameters which control\\nwhich linear layers LoRA should be applied to in the model:\\n\\n* ``lora_attn_modules: List[str]`` accepts a list of strings specifying which layers of the model to apply\\n  LoRA to:\\n\\n  * ``q_proj`` applies LoRA to the query projection layer.\\n  * ``k_proj`` applies LoRA to the key projection layer.\\n  * ``v_proj`` applies LoRA to the value projection layer.\\n  * ``output_proj`` applies LoRA to the attention output projection layer.\\n\\n  Whilst adding more layers to be fine-tuned may improve model accuracy,\\n  this will come at the cost of increased memory usage and reduced training speed.\\n\\n* ``apply_lora_to_mlp: Bool`` applies LoRA to the MLP in each transformer layer.\\n* ``apply_lora_to_output: Bool`` applies LoRA to the model\\'s final output projection.\\n  This is usually a projection to vocabulary space (e.g. in language models), but\\n  other modelling tasks may have different projections - classifier models will project\\n  to the number of classes, for example\\n\\n.. note::\\n\\n  Models which use tied embeddings (such as Gemma and Qwen2 1.5B and 0.5B) for the\\n  final output projection do not support ``apply_lora_to_output``.\\n\\nThese are all specified under the ``model`` flag or config entry, i.e:\\n\\n.. code-block:: bash\\n\\n  tune run lora_finetune_single_device --config llama3/8B_lora_single_device  \\\\\\n  model.apply_lora_to_mlp=True \\\\\\n  model.lora_attn_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"output_proj\"]\\n\\n.. code-block:: yaml\\n\\n  model:\\n    _component_: torchtune.models.llama3.lora_llama3_8b\\n    apply_lora_to_mlp: True\\n    model.lora_attn_modules: [\"q_proj\", \"k_proj\", \"v_proj\",\"output_proj\"]\\n\\nSecondly, parameters which control the scale of the impact of LoRA on the model:\\n\\n* ``lora_rank: int`` affects the scale of\\n'), TextContentItem(type='text', text='Result 4:\\nDocument_id:f8ecb\\nContent:  LoRA to Llama2 models\\n------------------------------\\n\\nWith torchtune, we can easily apply LoRA to Llama2 with a variety of different configurations.\\nLet\\'s take a look at how to construct Llama2 models in torchtune with and without LoRA.\\n\\n.. code-block:: python\\n\\n  from torchtune.models.llama2 import llama2_7b, lora_llama2_7b\\n\\n  # Build Llama2 without any LoRA layers\\n  base_model = llama2_7b()\\n\\n  # The default settings for lora_llama2_7b will match those for llama2_7b\\n  # We just need to define which layers we want LoRA applied to.\\n  # Within each self-attention, we can choose from [\"q_proj\", \"k_proj\", \"v_proj\", and \"output_proj\"].\\n  # We can also set apply_lora_to_mlp=True or apply_lora_to_output=True to apply LoRA to other linear\\n  # layers outside of the self-attention.\\n  lora_model = lora_llama2_7b(lora_attn_modules=[\"q_proj\", \"v_proj\"])\\n\\n.. note::\\n\\n    Calling :func:`lora_llama_2_7b <torchtune.models.llama2.lora_llama2_7b>` alone will not handle the definition of which parameters are trainable.\\n    See :ref:`below<setting_trainable_params>` for how to do this.\\n\\nLet\\'s inspect each of these models a bit more closely.\\n\\n.. code-block:: bash\\n\\n  # Print the first layer\\'s self-attention in the usual Llama2 model\\n  >>> print(base_model.layers[0].attn)\\n  MultiHeadAttention(\\n    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\\n    (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\\n    (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\\n    (output_proj): Linear(in_features=4096, out_features=4096, bias=False)\\n    (pos_embeddings): RotaryPositionalEmbeddings()\\n  )\\n\\n  # Print the same for Llama2 with LoRA weights\\n  >>> print(lora_model.layers[0].attn)\\n  MultiHeadAttention(\\n    (q_proj): LoRALinear(\\n      (dropout): Dropout(p=0.0, inplace=False)\\n     \\n'), TextContentItem(type='text', text='Result 5:\\nDocument_id:19946\\nContent: ora_finetune_label>`.\\nFor more on QLoRA in torchtune, see our :ref:`QLoRA Tutorial <qlora_finetune_label>`.\\n\\nLet\\'s take a look at how we can fine-tune Llama3-8B-Instruct with LoRA on a single device using torchtune. In this example, we will fine-tune\\nfor one epoch on a common instruct dataset for illustrative purposes. The basic command for a single-device LoRA fine-tune is\\n\\n.. code-block:: bash\\n\\n    tune run lora_finetune_single_device --config llama3/8B_lora_single_device\\n\\n.. note::\\n    To see a full list of recipes and their corresponding configs, simply run ``tune ls`` from the command line.\\n\\nWe can also add :ref:`command-line overrides <cli_override>` as needed, e.g.\\n\\n.. code-block:: bash\\n\\n    tune run lora_finetune_single_device --config llama3/8B_lora_single_device \\\\\\n        checkpointer.checkpoint_dir=<checkpoint_dir> \\\\\\n        tokenizer.path=<checkpoint_dir>/tokenizer.model \\\\\\n        checkpointer.output_dir=<checkpoint_dir>\\n\\nThis will load the Llama3-8B-Instruct checkpoint and tokenizer from ``<checkpoint_dir>`` used in the :ref:`tune download <tune_download_label>` command above,\\nthen save a final checkpoint in the same directory following the original format. For more details on the\\ncheckpoint formats supported in torchtune, see our :ref:`checkpointing deep-dive <understand_checkpointer>`.\\n\\n.. note::\\n    To see the full set of configurable parameters for this (and other) configs we can use :ref:`tune cp <tune_cp_cli_label>` to copy (and modify)\\n    the default config. :ref:`tune cp <tune_cp_cli_label>` can be used with recipe scripts too, in case you want to make more custom changes\\n    that cannot be achieved by directly modifying existing configurable parameters. For more on :ref:`tune cp <tune_cp_cli_label>` see the section on\\n    :ref:`modifying configs <tune_cp_label>` in our \":ref:`finetune_llama_label`\" tutorial.\\n\\nOnce training is complete, the model checkpoints will be saved and their locations will be logged. For\\nLoRA fine-tuning, the final checkpoint will contain the merged weights, and a copy of just the (much smaller) LoRA weights\\nwill\\n'), TextContentItem(type='text', text='END of knowledge_search tool results.\\n')])])_[('response_format', None), ('sampling_params', SamplingParams(strategy=TopPSamplingStrategy(type='top_p', temperature=0.0001, top_p=0.9), max_tokens=0, repetition_penalty=1.0)), ('stream', True), ('tool_config', ToolConfig(tool_choice=<ToolChoice.auto: 'auto'>, tool_prompt_format=None, system_message_behavior=<SystemMessageBehavior.append: 'append'>)), ('tool_prompt_format', None), ('tools', [ToolDefinition(tool_name='knowledge_search', description='Search for information in a database.', parameters={'query': ToolParamDefinition(param_type='string', description='The query to search for. Can be a natural language sentence or keywords.', required=True, default=None)})])]": {
    "chunks": [
      {
        "event": {
          "delta": {
            "text": "",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "start"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "To",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " use LoRA, you can follow these steps:\n\n",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "1.  Install the necessary packages, including torchtune",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " and the Llama2 model.\n2. ",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " Load the Llama2 model and specify which layers to apply Lo",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "RA to.\n3.  Define the Lo",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "RA parameters, such as the rank and alpha values.\n4",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": ".  Train the model using the LoRA fine-tuning",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " recipe in torchtune.\n\nHere is an example of how to",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " use LoRA with the Llama3-8B-Instruct",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " model:\n\n```python\nfrom torchtune.models.llama2 import",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " llama2_7b, lora_llama",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "2_7b\n\n# Build L",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "lama2 without any LoRA layers",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "\nbase_model = llama2_7b()\n\n# The default settings",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " for lora_llama2",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "_7b will match those for llama2_7",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "b\n# We just need to define",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " which layers we want LoRA applied to.\n# Within each",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " self-attention, we can choose from [\"q_proj\",",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " \"k_proj\", \"v_proj\",",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " and \"output_proj\"].\n# We can also set",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " apply_lora_to_mlp=True",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " or apply_lora_to_output=True to apply LoRA",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " to other linear\n# layers outside of the self-attention.\n",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "lora_model = lora_llama2_7b(l",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "ora_attn_modules=[\"q_proj\", \"v_proj\"])\n\n# Print",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " the first layer's self-attention in the",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " usual Llama2 model\nprint(base",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "_model.layers[0].attn)\n# Print the same",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " for Llama2 with LoRA weights\nprint(lora_model.layers",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "[0].attn)\n```\n\n",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "This code will load the",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " Llama2 model and apply LoRA",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " to the query and value projection layers.",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " You can modify the `lora_attn_modules` parameter",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " to apply LoRA to different layers.\n\nTo fine-tune",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " the model using the LoRA fine-t",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "uning recipe in torchtune, you can use the following command",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": ":\n\n```bash\ntune run lora_finetune_single",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "_device --config llama3/8B_lora",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "_single_device\n```\n\nThis will run the LoRA fine-t",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "uning recipe on the Llama3-8B-Instruct model with",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " the default settings. You can modify the command to add command-line overrides",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " as needed.",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "complete"
          },
          "logprobs": null,
          "stop_reason": {
            "__enum__": "StopReason",
            "value": "end_of_turn"
          }
        },
        "metrics": null
      }
    ],
    "type": "generator"
  },
  "('meta-llama/Llama-3.1-8B-Instruct', [SystemMessage(role='system', content='You are a helpful assistant'), UserMessage(role='user', content='I am attaching some documentation for Torchtune. Help me answer questions I will ask next.', context=None), CompletionMessage(role='assistant', content='', stop_reason=<StopReason.end_of_turn: 'end_of_turn'>, tool_calls=[ToolCall(call_id='<UUID>', tool_name='knowledge_search', arguments={'query': 'Torchtune documentation'})]), ToolResponseMessage(role='tool', call_id='<UUID>', tool_name='knowledge_search', content=[TextContentItem(type='text', text='knowledge_search tool found 5 chunks:\\nBEGIN of knowledge_search tool results.\\n'), TextContentItem(type='text', text='Result 1:\\nDocument_id:e70e0\\nContent:  conversational data, :func:`~torchtune.datasets.chat_dataset` seems to be a good fit. For any\\ncustom local dataset we always need to specify ``source``, ``data_files``, and ``split`` for any dataset\\nbuilder in torchtune. For :func:`~torchtune.datasets.chat_dataset`, we additionally need to specify\\n``conversation_column`` and ``conversation_style``. Our data follows the ``\"sharegpt\"`` format, so\\nwe can specify that here. Altogether, our :func:`~torchtune.datasets.chat_dataset` call should\\nlook like so:\\n\\n.. code-block:: python\\n\\n    from torchtune.datasets import chat_dataset\\n    from torchtune.models.llama3 import llama3_tokenizer\\n\\n    tokenizer = llama3_tokenizer(\"/tmp/Meta-Llama-3-8B-Instruct/original/tokenizer.model\")\\n    ds = chat_dataset(\\n        tokenizer=tokenizer,\\n        source=\"json\",\\n        data_files=\"data/my_data.json\",\\n        split=\"train\",\\n        conversation_column=\"dialogue\",\\n        conversation_style=\"sharegpt\",\\n    )\\n\\n.. code-block:: yaml\\n\\n    # In config\\n    tokenizer:\\n      _component_: torchtune.models.llama3.llama3_tokenizer\\n      path: /tmp/Meta-Llama-3-8B-Instruct/original/tokenizer.model\\n\\n    dataset:\\n      _component_: torchtune.datasets.chat_dataset\\n      source: json\\n      data_files: data/my_data.json\\n      split: train\\n      conversation_column: dialogue\\n      conversation_style: sharegpt\\n\\n.. note::\\n    You can pass in any keyword argument for `load_dataset <https://huggingface.co/docs/datasets/v2.20.0/en/package_reference/loading_methods#datasets.load_dataset>`_ into all our\\n    Dataset classes and they will honor them. This is useful for common parameters\\n    such as specifying the data split with :code:`split` or configuration with\\n    :code:`name`\\n\\nIf you needed to add a prompt template, you would simply pass it into the tokenizer.\\nSince we\\'re fine-tuning Llama3, the tokenizer will handle all formatting for\\nus and prompt templates are optional. Other models such as Mistral\\'s :class:`~torchtune.models.mistral._tokenizer.MistralTokenizer`,\\nuse a chat template by default (:class:`~torchtune.models.mistral.MistralChatTemplate`) to format\\nall messages according to their `recommendations <https://\\n'), TextContentItem(type='text', text=\"Result 2:\\nDocument_id:f8ecb\\nContent: .. _lora_finetune_label:\\n\\n============================\\nFine-Tuning Llama2 with LoRA\\n============================\\n\\nThis guide will teach you about `LoRA <https://arxiv.org/abs/2106.09685>`_, a parameter-efficient finetuning technique,\\nand show you how you can use torchtune to finetune a Llama2 model with LoRA.\\nIf you already know what LoRA is and want to get straight to running\\nyour own LoRA finetune in torchtune, you can jump to :ref:`LoRA finetuning recipe in torchtune<lora_recipe_label>`.\\n\\n.. grid:: 2\\n\\n    .. grid-item-card:: :octicon:`mortar-board;1em;` What you will learn\\n\\n      * What LoRA is and how it saves memory during finetuning\\n      * An overview of LoRA components in torchtune\\n      * How to run a LoRA finetune using torchtune\\n      * How to experiment with different LoRA configurations\\n\\n    .. grid-item-card:: :octicon:`list-unordered;1em;` Prerequisites\\n\\n      * Be familiar with :ref:`torchtune<overview_label>`\\n      * Make sure to :ref:`install torchtune<install_label>`\\n      * Make sure you have downloaded the :ref:`Llama2-7B model weights<download_llama_label>`\\n\\nWhat is LoRA?\\n-------------\\n\\n`LoRA <https://arxiv.org/abs/2106.09685>`_ is an adapter-based method for\\nparameter-efficient finetuning that adds trainable low-rank decomposition matrices to different layers of a neural network,\\nthen freezes the network's remaining parameters. LoRA is most commonly applied to\\ntransformer models, in which case it is common to add the low-rank matrices\\nto some of the linear projections in each transformer layer's self-attention.\\n\\n.. note::\\n\\n    If you're unfamiliar, check out these references for the `definition of rank <https://en.wikipedia.org/wiki/Rank_(linear_algebra)>`_\\n    and discussion of `low-rank approximations <https://en.wikipedia.org/wiki/Low-rank_approximation>`_.\\n\\nBy finetuning with LoRA (as opposed to finetuning all model parameters),\\nyou can expect to see memory savings due to a substantial reduction in the\\nnumber of parameters with gradients. When using an optimizer with momentum,\\nlike `AdamW <https://py\\n\"), TextContentItem(type='text', text='Result 3:\\nDocument_id:e187e\\nContent: ` module, which we swap\\n   out for :class:`~torchtune.modules.peft.LoRALinear` when ``use_dora=True``.\\n\\n.. _glossary_distrib:\\n\\n\\n.. TODO\\n\\n.. Distributed\\n.. -----------\\n\\n.. .. _glossary_fsdp:\\n\\n.. Fully Sharded Data Parallel (FSDP)\\n.. ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\\n.. All our ``_distributed`` recipes use `FSDP <https://pytorch.org/docs/stable/fsdp.html>`.\\n.. .. _glossary_fsdp2:\\n\\n'), TextContentItem(type='text', text='Result 4:\\nDocument_id:f8ecb\\nContent: 06% of all params are trainable.\\n\\n.. note::\\n    If you are directly using the LoRA recipe (as detailed :ref:`here<lora_recipe_label>`), you need only pass the\\n    relevant checkpoint path. Loading model weights and setting trainable parameters will be taken care\\n    of in the recipe.\\n\\n\\n.. _lora_recipe_label:\\n\\nLoRA finetuning recipe in torchtune\\n-----------------------------------\\n\\nFinally, we can put it all together and finetune a model using torchtune\\'s `LoRA recipe <https://github.com/pytorch/torchtune/blob/48626d19d2108f92c749411fbd5f0ff140023a25/recipes/lora_finetune.py>`_.\\nMake sure that you have first downloaded the Llama2 weights and tokenizer by following :ref:`these instructions<download_llama_label>`.\\nYou can then run the following command to perform a LoRA finetune of Llama2-7B with two GPUs (each having VRAM of at least 16GB):\\n\\n.. code-block:: bash\\n\\n    tune run --nnodes 1 --nproc_per_node 2 lora_finetune_distributed --config llama2/7B_lora\\n\\n.. note::\\n    Make sure to point to the location of your Llama2 weights and tokenizer. This can be done\\n    either by adding :code:`checkpointer.checkpoint_files=[my_model_checkpoint_path] tokenizer_checkpoint=my_tokenizer_checkpoint_path`\\n    or by directly modifying the :code:`7B_lora.yaml` file. See our \"\":ref:`config_tutorial_label`\" recipe\\n    for more details on how you can easily clone and modify torchtune configs.\\n\\n.. note::\\n    You can modify the value of :code:`nproc_per_node` depending on (a) the number of GPUs you have available,\\n    and (b) the memory constraints of your hardware.\\n\\nThe preceding command will run a LoRA finetune with torchtune\\'s factory settings, but we may want to experiment a bit.\\nLet\\'s take a closer look at some of the :code:`lora_finetune_distributed` config.\\n\\n.. code-block:: yaml\\n\\n  # Model Arguments\\n  model:\\n    _component_: lora_llama2_7b\\n    lora_attn_modules: [\\'q_proj\\', \\'v_proj\\']\\n    lora_rank: 8\\n    lora_alpha: 16\\n  ...\\n\\nWe see that the\\n'), TextContentItem(type='text', text='Result 5:\\nDocument_id:e187e\\nContent: etune\\n:func:`torchtune.models.llama3.llama3_8b` with DoRA, you would use :func:`torchtune.models.llama3.lora_llama3_8b` with ``use_dora=True``:\\n\\n.. code-block:: bash\\n\\n  tune run lora_finetune_single_device --config llama3/8B_lora_single_device \\\\\\n  model.use_dora=True\\n\\n.. code-block:: yaml\\n\\n  model:\\n    _component_: torchtune.models.lora_llama3_8b\\n    use_dora: True\\n\\nSince DoRA extends LoRA, the parameters for :ref:`customizing LoRA <glossary_lora>` are identical. You can also quantize the base model weights like in :ref:`glossary_qlora` by using ``quantize=True`` to reap\\neven more memory savings!\\n\\n.. code-block:: bash\\n\\n  tune run lora_finetune_single_device --config llama3/8B_lora_single_device \\\\\\n  model.apply_lora_to_mlp=True \\\\\\n  model.lora_attn_modules=[\"q_proj\",\"k_proj\",\"v_proj\"] \\\\\\n  model.lora_rank=16 \\\\\\n  model.lora_alpha=32 \\\\\\n  model.use_dora=True \\\\\\n  model.quantize_base=True\\n\\n.. code-block:: yaml\\n\\n  model:\\n    _component_: torchtune.models.lora_llama3_8b\\n    apply_lora_to_mlp: True\\n    lora_attn_modules: [\"q_proj\", \"k_proj\", \"v_proj\"]\\n    lora_rank: 16\\n    lora_alpha: 32\\n    use_dora: True\\n    quantize_base: True\\n\\n\\n.. note::\\n\\n   Under the hood, we\\'ve enabled DoRA by adding the :class:`~torchtune.modules.peft.DoRALinear` module, which we swap\\n   out for :class:`~torchtune.modules.peft.LoRALinear` when ``use_dora=True``.\\n\\n.. _glossary_distrib:\\n\\n\\n.. TODO\\n\\n.. Distributed\\n.. -----------\\n\\n.. .. _glossary_fsdp:\\n\\n.. Fully Sharded Data Parallel (FSDP)\\n.. ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\\n.. All our ``_distributed`` recipes use `FSDP <https://pytorch.org/docs/stable/fsdp.html>`.\\n.. .. _glossary_fsdp2:\\n\\n'), TextContentItem(type='text', text='END of knowledge_search tool results.\\n')]), CompletionMessage(role='assistant', content='You can use the following function call to answer the user\\'s question:\\n\\n{\"type\": \"function\", \"name\": \"knowledge_search\", \"parameters\": {\"query\": \"How to fine-tune a Llama2 model with LoRA in torchtune\"}}', stop_reason=<StopReason.end_of_turn: 'end_of_turn'>, tool_calls=[]), UserMessage(role='user', content='Tell me how to use LoRA', context=None)])_[('response_format', None), ('sampling_params', SamplingParams(strategy=TopPSamplingStrategy(type='top_p', temperature=0.0001, top_p=0.9), max_tokens=0, repetition_penalty=1.0)), ('stream', True), ('tool_config', ToolConfig(tool_choice=<ToolChoice.auto: 'auto'>, tool_prompt_format=None, system_message_behavior=<SystemMessageBehavior.append: 'append'>)), ('tool_prompt_format', None), ('tools', [ToolDefinition(tool_name='knowledge_search', description='Search for information in a database.', parameters={'query': ToolParamDefinition(param_type='string', description='The query to search for. Can be a natural language sentence or keywords.', required=True, default=None)})])]": {
    "chunks": [
      {
        "event": {
          "delta": {
            "text": "",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "start"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "{\"",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "type\": \"function\", \"name\": \"knowledge_search\", \"",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "parameters\": {\"query\": \"How to use LoRA\"}}",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "succeeded"
            },
            "tool_call": {
              "arguments": {
                "query": "How to use LoRA"
              },
              "call_id": "a411be5a-8c4a-4c25-b335-a3068e7f7c1a",
              "tool_name": "knowledge_search"
            },
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": {
            "__enum__": "StopReason",
            "value": "end_of_turn"
          }
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "complete"
          },
          "logprobs": null,
          "stop_reason": {
            "__enum__": "StopReason",
            "value": "end_of_turn"
          }
        },
        "metrics": null
      }
    ],
    "type": "generator"
  },
  "('meta-llama/Llama-3.1-8B-Instruct', [SystemMessage(role='system', content='You are a helpful assistant'), UserMessage(role='user', content='I am attaching some documentation for Torchtune. Help me answer questions I will ask next.', context=None), CompletionMessage(role='assistant', content='', stop_reason=<StopReason.end_of_turn: 'end_of_turn'>, tool_calls=[ToolCall(call_id='<UUID>', tool_name='knowledge_search', arguments={'query': 'Torchtune documentation'})]), ToolResponseMessage(role='tool', call_id='<UUID>', tool_name='knowledge_search', content=[TextContentItem(type='text', text='knowledge_search tool found 5 chunks:\\nBEGIN of knowledge_search tool results.\\n'), TextContentItem(type='text', text='Result 1:\\nDocument_id:e70e0\\nContent:  conversational data, :func:`~torchtune.datasets.chat_dataset` seems to be a good fit. For any\\ncustom local dataset we always need to specify ``source``, ``data_files``, and ``split`` for any dataset\\nbuilder in torchtune. For :func:`~torchtune.datasets.chat_dataset`, we additionally need to specify\\n``conversation_column`` and ``conversation_style``. Our data follows the ``\"sharegpt\"`` format, so\\nwe can specify that here. Altogether, our :func:`~torchtune.datasets.chat_dataset` call should\\nlook like so:\\n\\n.. code-block:: python\\n\\n    from torchtune.datasets import chat_dataset\\n    from torchtune.models.llama3 import llama3_tokenizer\\n\\n    tokenizer = llama3_tokenizer(\"/tmp/Meta-Llama-3-8B-Instruct/original/tokenizer.model\")\\n    ds = chat_dataset(\\n        tokenizer=tokenizer,\\n        source=\"json\",\\n        data_files=\"data/my_data.json\",\\n        split=\"train\",\\n        conversation_column=\"dialogue\",\\n        conversation_style=\"sharegpt\",\\n    )\\n\\n.. code-block:: yaml\\n\\n    # In config\\n    tokenizer:\\n      _component_: torchtune.models.llama3.llama3_tokenizer\\n      path: /tmp/Meta-Llama-3-8B-Instruct/original/tokenizer.model\\n\\n    dataset:\\n      _component_: torchtune.datasets.chat_dataset\\n      source: json\\n      data_files: data/my_data.json\\n      split: train\\n      conversation_column: dialogue\\n      conversation_style: sharegpt\\n\\n.. note::\\n    You can pass in any keyword argument for `load_dataset <https://huggingface.co/docs/datasets/v2.20.0/en/package_reference/loading_methods#datasets.load_dataset>`_ into all our\\n    Dataset classes and they will honor them. This is useful for common parameters\\n    such as specifying the data split with :code:`split` or configuration with\\n    :code:`name`\\n\\nIf you needed to add a prompt template, you would simply pass it into the tokenizer.\\nSince we\\'re fine-tuning Llama3, the tokenizer will handle all formatting for\\nus and prompt templates are optional. Other models such as Mistral\\'s :class:`~torchtune.models.mistral._tokenizer.MistralTokenizer`,\\nuse a chat template by default (:class:`~torchtune.models.mistral.MistralChatTemplate`) to format\\nall messages according to their `recommendations <https://\\n'), TextContentItem(type='text', text=\"Result 2:\\nDocument_id:f8ecb\\nContent: .. _lora_finetune_label:\\n\\n============================\\nFine-Tuning Llama2 with LoRA\\n============================\\n\\nThis guide will teach you about `LoRA <https://arxiv.org/abs/2106.09685>`_, a parameter-efficient finetuning technique,\\nand show you how you can use torchtune to finetune a Llama2 model with LoRA.\\nIf you already know what LoRA is and want to get straight to running\\nyour own LoRA finetune in torchtune, you can jump to :ref:`LoRA finetuning recipe in torchtune<lora_recipe_label>`.\\n\\n.. grid:: 2\\n\\n    .. grid-item-card:: :octicon:`mortar-board;1em;` What you will learn\\n\\n      * What LoRA is and how it saves memory during finetuning\\n      * An overview of LoRA components in torchtune\\n      * How to run a LoRA finetune using torchtune\\n      * How to experiment with different LoRA configurations\\n\\n    .. grid-item-card:: :octicon:`list-unordered;1em;` Prerequisites\\n\\n      * Be familiar with :ref:`torchtune<overview_label>`\\n      * Make sure to :ref:`install torchtune<install_label>`\\n      * Make sure you have downloaded the :ref:`Llama2-7B model weights<download_llama_label>`\\n\\nWhat is LoRA?\\n-------------\\n\\n`LoRA <https://arxiv.org/abs/2106.09685>`_ is an adapter-based method for\\nparameter-efficient finetuning that adds trainable low-rank decomposition matrices to different layers of a neural network,\\nthen freezes the network's remaining parameters. LoRA is most commonly applied to\\ntransformer models, in which case it is common to add the low-rank matrices\\nto some of the linear projections in each transformer layer's self-attention.\\n\\n.. note::\\n\\n    If you're unfamiliar, check out these references for the `definition of rank <https://en.wikipedia.org/wiki/Rank_(linear_algebra)>`_\\n    and discussion of `low-rank approximations <https://en.wikipedia.org/wiki/Low-rank_approximation>`_.\\n\\nBy finetuning with LoRA (as opposed to finetuning all model parameters),\\nyou can expect to see memory savings due to a substantial reduction in the\\nnumber of parameters with gradients. When using an optimizer with momentum,\\nlike `AdamW <https://py\\n\"), TextContentItem(type='text', text='Result 3:\\nDocument_id:e187e\\nContent: ` module, which we swap\\n   out for :class:`~torchtune.modules.peft.LoRALinear` when ``use_dora=True``.\\n\\n.. _glossary_distrib:\\n\\n\\n.. TODO\\n\\n.. Distributed\\n.. -----------\\n\\n.. .. _glossary_fsdp:\\n\\n.. Fully Sharded Data Parallel (FSDP)\\n.. ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\\n.. All our ``_distributed`` recipes use `FSDP <https://pytorch.org/docs/stable/fsdp.html>`.\\n.. .. _glossary_fsdp2:\\n\\n'), TextContentItem(type='text', text='Result 4:\\nDocument_id:f8ecb\\nContent: 06% of all params are trainable.\\n\\n.. note::\\n    If you are directly using the LoRA recipe (as detailed :ref:`here<lora_recipe_label>`), you need only pass the\\n    relevant checkpoint path. Loading model weights and setting trainable parameters will be taken care\\n    of in the recipe.\\n\\n\\n.. _lora_recipe_label:\\n\\nLoRA finetuning recipe in torchtune\\n-----------------------------------\\n\\nFinally, we can put it all together and finetune a model using torchtune\\'s `LoRA recipe <https://github.com/pytorch/torchtune/blob/48626d19d2108f92c749411fbd5f0ff140023a25/recipes/lora_finetune.py>`_.\\nMake sure that you have first downloaded the Llama2 weights and tokenizer by following :ref:`these instructions<download_llama_label>`.\\nYou can then run the following command to perform a LoRA finetune of Llama2-7B with two GPUs (each having VRAM of at least 16GB):\\n\\n.. code-block:: bash\\n\\n    tune run --nnodes 1 --nproc_per_node 2 lora_finetune_distributed --config llama2/7B_lora\\n\\n.. note::\\n    Make sure to point to the location of your Llama2 weights and tokenizer. This can be done\\n    either by adding :code:`checkpointer.checkpoint_files=[my_model_checkpoint_path] tokenizer_checkpoint=my_tokenizer_checkpoint_path`\\n    or by directly modifying the :code:`7B_lora.yaml` file. See our \"\":ref:`config_tutorial_label`\" recipe\\n    for more details on how you can easily clone and modify torchtune configs.\\n\\n.. note::\\n    You can modify the value of :code:`nproc_per_node` depending on (a) the number of GPUs you have available,\\n    and (b) the memory constraints of your hardware.\\n\\nThe preceding command will run a LoRA finetune with torchtune\\'s factory settings, but we may want to experiment a bit.\\nLet\\'s take a closer look at some of the :code:`lora_finetune_distributed` config.\\n\\n.. code-block:: yaml\\n\\n  # Model Arguments\\n  model:\\n    _component_: lora_llama2_7b\\n    lora_attn_modules: [\\'q_proj\\', \\'v_proj\\']\\n    lora_rank: 8\\n    lora_alpha: 16\\n  ...\\n\\nWe see that the\\n'), TextContentItem(type='text', text='Result 5:\\nDocument_id:e187e\\nContent: etune\\n:func:`torchtune.models.llama3.llama3_8b` with DoRA, you would use :func:`torchtune.models.llama3.lora_llama3_8b` with ``use_dora=True``:\\n\\n.. code-block:: bash\\n\\n  tune run lora_finetune_single_device --config llama3/8B_lora_single_device \\\\\\n  model.use_dora=True\\n\\n.. code-block:: yaml\\n\\n  model:\\n    _component_: torchtune.models.lora_llama3_8b\\n    use_dora: True\\n\\nSince DoRA extends LoRA, the parameters for :ref:`customizing LoRA <glossary_lora>` are identical. You can also quantize the base model weights like in :ref:`glossary_qlora` by using ``quantize=True`` to reap\\neven more memory savings!\\n\\n.. code-block:: bash\\n\\n  tune run lora_finetune_single_device --config llama3/8B_lora_single_device \\\\\\n  model.apply_lora_to_mlp=True \\\\\\n  model.lora_attn_modules=[\"q_proj\",\"k_proj\",\"v_proj\"] \\\\\\n  model.lora_rank=16 \\\\\\n  model.lora_alpha=32 \\\\\\n  model.use_dora=True \\\\\\n  model.quantize_base=True\\n\\n.. code-block:: yaml\\n\\n  model:\\n    _component_: torchtune.models.lora_llama3_8b\\n    apply_lora_to_mlp: True\\n    lora_attn_modules: [\"q_proj\", \"k_proj\", \"v_proj\"]\\n    lora_rank: 16\\n    lora_alpha: 32\\n    use_dora: True\\n    quantize_base: True\\n\\n\\n.. note::\\n\\n   Under the hood, we\\'ve enabled DoRA by adding the :class:`~torchtune.modules.peft.DoRALinear` module, which we swap\\n   out for :class:`~torchtune.modules.peft.LoRALinear` when ``use_dora=True``.\\n\\n.. _glossary_distrib:\\n\\n\\n.. TODO\\n\\n.. Distributed\\n.. -----------\\n\\n.. .. _glossary_fsdp:\\n\\n.. Fully Sharded Data Parallel (FSDP)\\n.. ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\\n.. All our ``_distributed`` recipes use `FSDP <https://pytorch.org/docs/stable/fsdp.html>`.\\n.. .. _glossary_fsdp2:\\n\\n'), TextContentItem(type='text', text='END of knowledge_search tool results.\\n')])])_[('response_format', None), ('sampling_params', SamplingParams(strategy=TopPSamplingStrategy(type='top_p', temperature=0.0001, top_p=0.9), max_tokens=0, repetition_penalty=1.0)), ('stream', True), ('tool_config', ToolConfig(tool_choice=<ToolChoice.auto: 'auto'>, tool_prompt_format=None, system_message_behavior=<SystemMessageBehavior.append: 'append'>)), ('tool_prompt_format', None), ('tools', [ToolDefinition(tool_name='knowledge_search', description='Search for information in a database.', parameters={'query': ToolParamDefinition(param_type='string', description='The query to search for. Can be a natural language sentence or keywords.', required=True, default=None)})])]": {
    "chunks": [
      {
        "event": {
          "delta": {
            "text": "",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "start"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "You",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " can use the following function call to answer the user's question:\n\n",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "{\"type\": \"function\", \"name\": \"knowledge_search",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "\", \"parameters\": {\"query\": \"How to fine-tune",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " a Llama2 model with LoRA in torchtune\"}}",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "complete"
          },
          "logprobs": null,
          "stop_reason": {
            "__enum__": "StopReason",
            "value": "end_of_turn"
          }
        },
        "metrics": null
      }
    ],
    "type": "generator"
  },
  "('meta-llama/Llama-3.1-8B-Instruct', [SystemMessage(role='system', content='You are a helpful assistant'), UserMessage(role='user', content='I am attaching some documentation for Torchtune. Help me answer questions I will ask next.', context=None), CompletionMessage(role='assistant', content='', stop_reason=<StopReason.end_of_turn: 'end_of_turn'>, tool_calls=[ToolCall(call_id='<UUID>', tool_name='knowledge_search', arguments={'query': 'Torchtune documentation'})]), ToolResponseMessage(role='tool', call_id='<UUID>', tool_name='knowledge_search', content=[TextContentItem(type='text', text='knowledge_search tool found 5 chunks:\\nBEGIN of knowledge_search tool results.\\n'), TextContentItem(type='text', text='Result 1:\\nDocument_id:ee8b0\\nContent:  conversational data, :func:`~torchtune.datasets.chat_dataset` seems to be a good fit. For any\\ncustom local dataset we always need to specify ``source``, ``data_files``, and ``split`` for any dataset\\nbuilder in torchtune. For :func:`~torchtune.datasets.chat_dataset`, we additionally need to specify\\n``conversation_column`` and ``conversation_style``. Our data follows the ``\"sharegpt\"`` format, so\\nwe can specify that here. Altogether, our :func:`~torchtune.datasets.chat_dataset` call should\\nlook like so:\\n\\n.. code-block:: python\\n\\n    from torchtune.datasets import chat_dataset\\n    from torchtune.models.llama3 import llama3_tokenizer\\n\\n    tokenizer = llama3_tokenizer(\"/tmp/Meta-Llama-3-8B-Instruct/original/tokenizer.model\")\\n    ds = chat_dataset(\\n        tokenizer=tokenizer,\\n        source=\"json\",\\n        data_files=\"data/my_data.json\",\\n        split=\"train\",\\n        conversation_column=\"dialogue\",\\n        conversation_style=\"sharegpt\",\\n    )\\n\\n.. code-block:: yaml\\n\\n    # In config\\n    tokenizer:\\n      _component_: torchtune.models.llama3.llama3_tokenizer\\n      path: /tmp/Meta-Llama-3-8B-Instruct/original/tokenizer.model\\n\\n    dataset:\\n      _component_: torchtune.datasets.chat_dataset\\n      source: json\\n      data_files: data/my_data.json\\n      split: train\\n      conversation_column: dialogue\\n      conversation_style: sharegpt\\n\\n.. note::\\n    You can pass in any keyword argument for `load_dataset <https://huggingface.co/docs/datasets/v2.20.0/en/package_reference/loading_methods#datasets.load_dataset>`_ into all our\\n    Dataset classes and they will honor them. This is useful for common parameters\\n    such as specifying the data split with :code:`split` or configuration with\\n    :code:`name`\\n\\nIf you needed to add a prompt template, you would simply pass it into the tokenizer.\\nSince we\\'re fine-tuning Llama3, the tokenizer will handle all formatting for\\nus and prompt templates are optional. Other models such as Mistral\\'s :class:`~torchtune.models.mistral._tokenizer.MistralTokenizer`,\\nuse a chat template by default (:class:`~torchtune.models.mistral.MistralChatTemplate`) to format\\nall messages according to their `recommendations <https://\\n'), TextContentItem(type='text', text=\"Result 2:\\nDocument_id:65813\\nContent: .. _lora_finetune_label:\\n\\n============================\\nFine-Tuning Llama2 with LoRA\\n============================\\n\\nThis guide will teach you about `LoRA <https://arxiv.org/abs/2106.09685>`_, a parameter-efficient finetuning technique,\\nand show you how you can use torchtune to finetune a Llama2 model with LoRA.\\nIf you already know what LoRA is and want to get straight to running\\nyour own LoRA finetune in torchtune, you can jump to :ref:`LoRA finetuning recipe in torchtune<lora_recipe_label>`.\\n\\n.. grid:: 2\\n\\n    .. grid-item-card:: :octicon:`mortar-board;1em;` What you will learn\\n\\n      * What LoRA is and how it saves memory during finetuning\\n      * An overview of LoRA components in torchtune\\n      * How to run a LoRA finetune using torchtune\\n      * How to experiment with different LoRA configurations\\n\\n    .. grid-item-card:: :octicon:`list-unordered;1em;` Prerequisites\\n\\n      * Be familiar with :ref:`torchtune<overview_label>`\\n      * Make sure to :ref:`install torchtune<install_label>`\\n      * Make sure you have downloaded the :ref:`Llama2-7B model weights<download_llama_label>`\\n\\nWhat is LoRA?\\n-------------\\n\\n`LoRA <https://arxiv.org/abs/2106.09685>`_ is an adapter-based method for\\nparameter-efficient finetuning that adds trainable low-rank decomposition matrices to different layers of a neural network,\\nthen freezes the network's remaining parameters. LoRA is most commonly applied to\\ntransformer models, in which case it is common to add the low-rank matrices\\nto some of the linear projections in each transformer layer's self-attention.\\n\\n.. note::\\n\\n    If you're unfamiliar, check out these references for the `definition of rank <https://en.wikipedia.org/wiki/Rank_(linear_algebra)>`_\\n    and discussion of `low-rank approximations <https://en.wikipedia.org/wiki/Low-rank_approximation>`_.\\n\\nBy finetuning with LoRA (as opposed to finetuning all model parameters),\\nyou can expect to see memory savings due to a substantial reduction in the\\nnumber of parameters with gradients. When using an optimizer with momentum,\\nlike `AdamW <https://py\\n\"), TextContentItem(type='text', text='Result 3:\\nDocument_id:73794\\nContent: ` module, which we swap\\n   out for :class:`~torchtune.modules.peft.LoRALinear` when ``use_dora=True``.\\n\\n.. _glossary_distrib:\\n\\n\\n.. TODO\\n\\n.. Distributed\\n.. -----------\\n\\n.. .. _glossary_fsdp:\\n\\n.. Fully Sharded Data Parallel (FSDP)\\n.. ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\\n.. All our ``_distributed`` recipes use `FSDP <https://pytorch.org/docs/stable/fsdp.html>`.\\n.. .. _glossary_fsdp2:\\n\\n'), TextContentItem(type='text', text='Result 4:\\nDocument_id:65813\\nContent: 06% of all params are trainable.\\n\\n.. note::\\n    If you are directly using the LoRA recipe (as detailed :ref:`here<lora_recipe_label>`), you need only pass the\\n    relevant checkpoint path. Loading model weights and setting trainable parameters will be taken care\\n    of in the recipe.\\n\\n\\n.. _lora_recipe_label:\\n\\nLoRA finetuning recipe in torchtune\\n-----------------------------------\\n\\nFinally, we can put it all together and finetune a model using torchtune\\'s `LoRA recipe <https://github.com/pytorch/torchtune/blob/48626d19d2108f92c749411fbd5f0ff140023a25/recipes/lora_finetune.py>`_.\\nMake sure that you have first downloaded the Llama2 weights and tokenizer by following :ref:`these instructions<download_llama_label>`.\\nYou can then run the following command to perform a LoRA finetune of Llama2-7B with two GPUs (each having VRAM of at least 16GB):\\n\\n.. code-block:: bash\\n\\n    tune run --nnodes 1 --nproc_per_node 2 lora_finetune_distributed --config llama2/7B_lora\\n\\n.. note::\\n    Make sure to point to the location of your Llama2 weights and tokenizer. This can be done\\n    either by adding :code:`checkpointer.checkpoint_files=[my_model_checkpoint_path] tokenizer_checkpoint=my_tokenizer_checkpoint_path`\\n    or by directly modifying the :code:`7B_lora.yaml` file. See our \"\":ref:`config_tutorial_label`\" recipe\\n    for more details on how you can easily clone and modify torchtune configs.\\n\\n.. note::\\n    You can modify the value of :code:`nproc_per_node` depending on (a) the number of GPUs you have available,\\n    and (b) the memory constraints of your hardware.\\n\\nThe preceding command will run a LoRA finetune with torchtune\\'s factory settings, but we may want to experiment a bit.\\nLet\\'s take a closer look at some of the :code:`lora_finetune_distributed` config.\\n\\n.. code-block:: yaml\\n\\n  # Model Arguments\\n  model:\\n    _component_: lora_llama2_7b\\n    lora_attn_modules: [\\'q_proj\\', \\'v_proj\\']\\n    lora_rank: 8\\n    lora_alpha: 16\\n  ...\\n\\nWe see that the\\n'), TextContentItem(type='text', text='Result 5:\\nDocument_id:73794\\nContent: etune\\n:func:`torchtune.models.llama3.llama3_8b` with DoRA, you would use :func:`torchtune.models.llama3.lora_llama3_8b` with ``use_dora=True``:\\n\\n.. code-block:: bash\\n\\n  tune run lora_finetune_single_device --config llama3/8B_lora_single_device \\\\\\n  model.use_dora=True\\n\\n.. code-block:: yaml\\n\\n  model:\\n    _component_: torchtune.models.lora_llama3_8b\\n    use_dora: True\\n\\nSince DoRA extends LoRA, the parameters for :ref:`customizing LoRA <glossary_lora>` are identical. You can also quantize the base model weights like in :ref:`glossary_qlora` by using ``quantize=True`` to reap\\neven more memory savings!\\n\\n.. code-block:: bash\\n\\n  tune run lora_finetune_single_device --config llama3/8B_lora_single_device \\\\\\n  model.apply_lora_to_mlp=True \\\\\\n  model.lora_attn_modules=[\"q_proj\",\"k_proj\",\"v_proj\"] \\\\\\n  model.lora_rank=16 \\\\\\n  model.lora_alpha=32 \\\\\\n  model.use_dora=True \\\\\\n  model.quantize_base=True\\n\\n.. code-block:: yaml\\n\\n  model:\\n    _component_: torchtune.models.lora_llama3_8b\\n    apply_lora_to_mlp: True\\n    lora_attn_modules: [\"q_proj\", \"k_proj\", \"v_proj\"]\\n    lora_rank: 16\\n    lora_alpha: 32\\n    use_dora: True\\n    quantize_base: True\\n\\n\\n.. note::\\n\\n   Under the hood, we\\'ve enabled DoRA by adding the :class:`~torchtune.modules.peft.DoRALinear` module, which we swap\\n   out for :class:`~torchtune.modules.peft.LoRALinear` when ``use_dora=True``.\\n\\n.. _glossary_distrib:\\n\\n\\n.. TODO\\n\\n.. Distributed\\n.. -----------\\n\\n.. .. _glossary_fsdp:\\n\\n.. Fully Sharded Data Parallel (FSDP)\\n.. ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\\n.. All our ``_distributed`` recipes use `FSDP <https://pytorch.org/docs/stable/fsdp.html>`.\\n.. .. _glossary_fsdp2:\\n\\n'), TextContentItem(type='text', text='END of knowledge_search tool results.\\n')]), CompletionMessage(role='assistant', content='You can use the following function call to answer the user\\'s question:\\n\\n{\"type\": \"function\", \"name\": \"knowledge_search\", \"parameters\": {\"query\": \"How to fine-tune a Llama2 model with LoRA in torchtune\"}}', stop_reason=<StopReason.end_of_turn: 'end_of_turn'>, tool_calls=[]), UserMessage(role='user', content='Tell me how to use LoRA', context=None), CompletionMessage(role='assistant', content='', stop_reason=<StopReason.end_of_turn: 'end_of_turn'>, tool_calls=[ToolCall(call_id='<UUID>', tool_name='knowledge_search', arguments={'query': 'How to use LoRA'})]), ToolResponseMessage(role='tool', call_id='<UUID>', tool_name='knowledge_search', content=[TextContentItem(type='text', text='knowledge_search tool found 5 chunks:\\nBEGIN of knowledge_search tool results.\\n'), TextContentItem(type='text', text=\"Result 1:\\nDocument_id:65813\\nContent: .. _lora_finetune_label:\\n\\n============================\\nFine-Tuning Llama2 with LoRA\\n============================\\n\\nThis guide will teach you about `LoRA <https://arxiv.org/abs/2106.09685>`_, a parameter-efficient finetuning technique,\\nand show you how you can use torchtune to finetune a Llama2 model with LoRA.\\nIf you already know what LoRA is and want to get straight to running\\nyour own LoRA finetune in torchtune, you can jump to :ref:`LoRA finetuning recipe in torchtune<lora_recipe_label>`.\\n\\n.. grid:: 2\\n\\n    .. grid-item-card:: :octicon:`mortar-board;1em;` What you will learn\\n\\n      * What LoRA is and how it saves memory during finetuning\\n      * An overview of LoRA components in torchtune\\n      * How to run a LoRA finetune using torchtune\\n      * How to experiment with different LoRA configurations\\n\\n    .. grid-item-card:: :octicon:`list-unordered;1em;` Prerequisites\\n\\n      * Be familiar with :ref:`torchtune<overview_label>`\\n      * Make sure to :ref:`install torchtune<install_label>`\\n      * Make sure you have downloaded the :ref:`Llama2-7B model weights<download_llama_label>`\\n\\nWhat is LoRA?\\n-------------\\n\\n`LoRA <https://arxiv.org/abs/2106.09685>`_ is an adapter-based method for\\nparameter-efficient finetuning that adds trainable low-rank decomposition matrices to different layers of a neural network,\\nthen freezes the network's remaining parameters. LoRA is most commonly applied to\\ntransformer models, in which case it is common to add the low-rank matrices\\nto some of the linear projections in each transformer layer's self-attention.\\n\\n.. note::\\n\\n    If you're unfamiliar, check out these references for the `definition of rank <https://en.wikipedia.org/wiki/Rank_(linear_algebra)>`_\\n    and discussion of `low-rank approximations <https://en.wikipedia.org/wiki/Low-rank_approximation>`_.\\n\\nBy finetuning with LoRA (as opposed to finetuning all model parameters),\\nyou can expect to see memory savings due to a substantial reduction in the\\nnumber of parameters with gradients. When using an optimizer with momentum,\\nlike `AdamW <https://py\\n\"), TextContentItem(type='text', text='Result 2:\\nDocument_id:65813\\nContent: 06% of all params are trainable.\\n\\n.. note::\\n    If you are directly using the LoRA recipe (as detailed :ref:`here<lora_recipe_label>`), you need only pass the\\n    relevant checkpoint path. Loading model weights and setting trainable parameters will be taken care\\n    of in the recipe.\\n\\n\\n.. _lora_recipe_label:\\n\\nLoRA finetuning recipe in torchtune\\n-----------------------------------\\n\\nFinally, we can put it all together and finetune a model using torchtune\\'s `LoRA recipe <https://github.com/pytorch/torchtune/blob/48626d19d2108f92c749411fbd5f0ff140023a25/recipes/lora_finetune.py>`_.\\nMake sure that you have first downloaded the Llama2 weights and tokenizer by following :ref:`these instructions<download_llama_label>`.\\nYou can then run the following command to perform a LoRA finetune of Llama2-7B with two GPUs (each having VRAM of at least 16GB):\\n\\n.. code-block:: bash\\n\\n    tune run --nnodes 1 --nproc_per_node 2 lora_finetune_distributed --config llama2/7B_lora\\n\\n.. note::\\n    Make sure to point to the location of your Llama2 weights and tokenizer. This can be done\\n    either by adding :code:`checkpointer.checkpoint_files=[my_model_checkpoint_path] tokenizer_checkpoint=my_tokenizer_checkpoint_path`\\n    or by directly modifying the :code:`7B_lora.yaml` file. See our \"\":ref:`config_tutorial_label`\" recipe\\n    for more details on how you can easily clone and modify torchtune configs.\\n\\n.. note::\\n    You can modify the value of :code:`nproc_per_node` depending on (a) the number of GPUs you have available,\\n    and (b) the memory constraints of your hardware.\\n\\nThe preceding command will run a LoRA finetune with torchtune\\'s factory settings, but we may want to experiment a bit.\\nLet\\'s take a closer look at some of the :code:`lora_finetune_distributed` config.\\n\\n.. code-block:: yaml\\n\\n  # Model Arguments\\n  model:\\n    _component_: lora_llama2_7b\\n    lora_attn_modules: [\\'q_proj\\', \\'v_proj\\']\\n    lora_rank: 8\\n    lora_alpha: 16\\n  ...\\n\\nWe see that the\\n'), TextContentItem(type='text', text='Result 3:\\nDocument_id:73794\\nContent:  with training with LoRA quickly,\\njust specify any config with ``_lora`` in its name, e.g:\\n\\n.. code-block:: bash\\n\\n  tune run lora_finetune_single_device --config llama3/8B_lora_single_device\\n\\n\\nThere are two sets of parameters to customize LoRA to suit your needs. Firstly, the parameters which control\\nwhich linear layers LoRA should be applied to in the model:\\n\\n* ``lora_attn_modules: List[str]`` accepts a list of strings specifying which layers of the model to apply\\n  LoRA to:\\n\\n  * ``q_proj`` applies LoRA to the query projection layer.\\n  * ``k_proj`` applies LoRA to the key projection layer.\\n  * ``v_proj`` applies LoRA to the value projection layer.\\n  * ``output_proj`` applies LoRA to the attention output projection layer.\\n\\n  Whilst adding more layers to be fine-tuned may improve model accuracy,\\n  this will come at the cost of increased memory usage and reduced training speed.\\n\\n* ``apply_lora_to_mlp: Bool`` applies LoRA to the MLP in each transformer layer.\\n* ``apply_lora_to_output: Bool`` applies LoRA to the model\\'s final output projection.\\n  This is usually a projection to vocabulary space (e.g. in language models), but\\n  other modelling tasks may have different projections - classifier models will project\\n  to the number of classes, for example\\n\\n.. note::\\n\\n  Models which use tied embeddings (such as Gemma and Qwen2 1.5B and 0.5B) for the\\n  final output projection do not support ``apply_lora_to_output``.\\n\\nThese are all specified under the ``model`` flag or config entry, i.e:\\n\\n.. code-block:: bash\\n\\n  tune run lora_finetune_single_device --config llama3/8B_lora_single_device  \\\\\\n  model.apply_lora_to_mlp=True \\\\\\n  model.lora_attn_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"output_proj\"]\\n\\n.. code-block:: yaml\\n\\n  model:\\n    _component_: torchtune.models.llama3.lora_llama3_8b\\n    apply_lora_to_mlp: True\\n    model.lora_attn_modules: [\"q_proj\", \"k_proj\", \"v_proj\",\"output_proj\"]\\n\\nSecondly, parameters which control the scale of the impact of LoRA on the model:\\n\\n* ``lora_rank: int`` affects the scale of\\n'), TextContentItem(type='text', text='Result 4:\\nDocument_id:65813\\nContent:  LoRA to Llama2 models\\n------------------------------\\n\\nWith torchtune, we can easily apply LoRA to Llama2 with a variety of different configurations.\\nLet\\'s take a look at how to construct Llama2 models in torchtune with and without LoRA.\\n\\n.. code-block:: python\\n\\n  from torchtune.models.llama2 import llama2_7b, lora_llama2_7b\\n\\n  # Build Llama2 without any LoRA layers\\n  base_model = llama2_7b()\\n\\n  # The default settings for lora_llama2_7b will match those for llama2_7b\\n  # We just need to define which layers we want LoRA applied to.\\n  # Within each self-attention, we can choose from [\"q_proj\", \"k_proj\", \"v_proj\", and \"output_proj\"].\\n  # We can also set apply_lora_to_mlp=True or apply_lora_to_output=True to apply LoRA to other linear\\n  # layers outside of the self-attention.\\n  lora_model = lora_llama2_7b(lora_attn_modules=[\"q_proj\", \"v_proj\"])\\n\\n.. note::\\n\\n    Calling :func:`lora_llama_2_7b <torchtune.models.llama2.lora_llama2_7b>` alone will not handle the definition of which parameters are trainable.\\n    See :ref:`below<setting_trainable_params>` for how to do this.\\n\\nLet\\'s inspect each of these models a bit more closely.\\n\\n.. code-block:: bash\\n\\n  # Print the first layer\\'s self-attention in the usual Llama2 model\\n  >>> print(base_model.layers[0].attn)\\n  MultiHeadAttention(\\n    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\\n    (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\\n    (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\\n    (output_proj): Linear(in_features=4096, out_features=4096, bias=False)\\n    (pos_embeddings): RotaryPositionalEmbeddings()\\n  )\\n\\n  # Print the same for Llama2 with LoRA weights\\n  >>> print(lora_model.layers[0].attn)\\n  MultiHeadAttention(\\n    (q_proj): LoRALinear(\\n      (dropout): Dropout(p=0.0, inplace=False)\\n     \\n'), TextContentItem(type='text', text='Result 5:\\nDocument_id:a3a25\\nContent: ora_finetune_label>`.\\nFor more on QLoRA in torchtune, see our :ref:`QLoRA Tutorial <qlora_finetune_label>`.\\n\\nLet\\'s take a look at how we can fine-tune Llama3-8B-Instruct with LoRA on a single device using torchtune. In this example, we will fine-tune\\nfor one epoch on a common instruct dataset for illustrative purposes. The basic command for a single-device LoRA fine-tune is\\n\\n.. code-block:: bash\\n\\n    tune run lora_finetune_single_device --config llama3/8B_lora_single_device\\n\\n.. note::\\n    To see a full list of recipes and their corresponding configs, simply run ``tune ls`` from the command line.\\n\\nWe can also add :ref:`command-line overrides <cli_override>` as needed, e.g.\\n\\n.. code-block:: bash\\n\\n    tune run lora_finetune_single_device --config llama3/8B_lora_single_device \\\\\\n        checkpointer.checkpoint_dir=<checkpoint_dir> \\\\\\n        tokenizer.path=<checkpoint_dir>/tokenizer.model \\\\\\n        checkpointer.output_dir=<checkpoint_dir>\\n\\nThis will load the Llama3-8B-Instruct checkpoint and tokenizer from ``<checkpoint_dir>`` used in the :ref:`tune download <tune_download_label>` command above,\\nthen save a final checkpoint in the same directory following the original format. For more details on the\\ncheckpoint formats supported in torchtune, see our :ref:`checkpointing deep-dive <understand_checkpointer>`.\\n\\n.. note::\\n    To see the full set of configurable parameters for this (and other) configs we can use :ref:`tune cp <tune_cp_cli_label>` to copy (and modify)\\n    the default config. :ref:`tune cp <tune_cp_cli_label>` can be used with recipe scripts too, in case you want to make more custom changes\\n    that cannot be achieved by directly modifying existing configurable parameters. For more on :ref:`tune cp <tune_cp_cli_label>` see the section on\\n    :ref:`modifying configs <tune_cp_label>` in our \":ref:`finetune_llama_label`\" tutorial.\\n\\nOnce training is complete, the model checkpoints will be saved and their locations will be logged. For\\nLoRA fine-tuning, the final checkpoint will contain the merged weights, and a copy of just the (much smaller) LoRA weights\\nwill\\n'), TextContentItem(type='text', text='END of knowledge_search tool results.\\n')])])_[('response_format', None), ('sampling_params', SamplingParams(strategy=TopPSamplingStrategy(type='top_p', temperature=0.0001, top_p=0.9), max_tokens=0, repetition_penalty=1.0)), ('stream', True), ('tool_config', ToolConfig(tool_choice=<ToolChoice.auto: 'auto'>, tool_prompt_format=None, system_message_behavior=<SystemMessageBehavior.append: 'append'>)), ('tool_prompt_format', None), ('tools', [ToolDefinition(tool_name='knowledge_search', description='Search for information in a database.', parameters={'query': ToolParamDefinition(param_type='string', description='The query to search for. Can be a natural language sentence or keywords.', required=True, default=None)})])]": {
    "chunks": [
      {
        "event": {
          "delta": {
            "text": "",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "start"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "To",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " use LoRA, you can follow these steps:\n\n1. ",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " Install the necessary packages, including torchtune and the L",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "lama2 model.\n2.  Load the L",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "lama2 model and specify which layers to apply",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " LoRA to.\n3.  Define the LoRA parameters",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": ", such as the rank and alpha values.\n",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "4.  Train the model using the Lo",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "RA fine-tuning recipe in torcht",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "une.\n\nHere is an example of how to use LoRA",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " with the Llama2 model:\n\n```python\nfrom torcht",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "une.models.llama2 import llama2",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "_7b, lora",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "_llama2_7b\n\n# Build",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " Llama2 without any LoRA layers\nbase_model",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " = llama2_7b()\n\n# The default settings for",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " lora_llama2_7b will match those for llama",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "2_7b\n# We just need to define which",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " layers we want LoRA applied to.\n# Within each self",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "-attention, we can choose from [\"q_proj\", \"",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "k_proj\", \"v_proj\", and \"output_proj\"]",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": ".\n# We can also set apply_lora_to_mlp",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "=True or apply_lora_to_output=True to apply",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " LoRA to other linear\n# layers",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " outside of the self-attention.\nlora_model = l",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "ora_llama2_7b(lora_attn_modules",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "=[\"q_proj\", \"v_proj\"])\n\n# Print the first",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " layer's self-attention in",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " the usual Llama2 model\nprint(base",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "_model.layers[0].attn)\n# Print the same",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " for Llama2 with LoRA weights\nprint",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "(lora_model.layers[0].attn)\n``",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "`\n\nThis code will load the Llama",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "2 model and apply LoRA to the specified",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " layers. The `lora_at",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "tn_modules` parameter is used to specify which layers to apply",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " LoRA to, and the `apply_lora_to_m",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "lp` and `apply",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "_lora_to_output` parameters can be used",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " to apply LoRA to other linear layers outside of",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " the self-attention.\n\nYou can also use the `",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "tune run` command to fine-tune the model using",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " the LoRA fine-tuning recipe in torchtune.",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " For example:\n\n```bash\ntune run lora_f",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "inetune_single_device --config llama3/8B_lora",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "_single_device\n```\n\nThis will",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " run the LoRA fine-tuning recipe",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " on the Llama3-8B-Instruct model using",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " the default configuration. You can modify",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " the configuration by adding command-line overrides, such as:\n\n```bash\n",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "tune run",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "complete"
          },
          "logprobs": null,
          "stop_reason": {
            "__enum__": "StopReason",
            "value": "out_of_tokens"
          }
        },
        "metrics": null
      }
    ],
    "type": "generator"
  },
  "('meta-llama/Llama-3.1-8B-Instruct', [SystemMessage(role='system', content='You are a helpful assistant'), UserMessage(role='user', content='I am attaching some documentation for Torchtune. Help me answer questions I will ask next.', context=None), CompletionMessage(role='assistant', content='', stop_reason=<StopReason.end_of_turn: 'end_of_turn'>, tool_calls=[ToolCall(call_id='<UUID>', tool_name='knowledge_search', arguments={'query': 'Torchtune documentation'})]), ToolResponseMessage(role='tool', call_id='<UUID>', tool_name='knowledge_search', content=[TextContentItem(type='text', text='knowledge_search tool found 5 chunks:\\nBEGIN of knowledge_search tool results.\\n'), TextContentItem(type='text', text='Result 1:\\nDocument_id:ee8b0\\nContent:  conversational data, :func:`~torchtune.datasets.chat_dataset` seems to be a good fit. For any\\ncustom local dataset we always need to specify ``source``, ``data_files``, and ``split`` for any dataset\\nbuilder in torchtune. For :func:`~torchtune.datasets.chat_dataset`, we additionally need to specify\\n``conversation_column`` and ``conversation_style``. Our data follows the ``\"sharegpt\"`` format, so\\nwe can specify that here. Altogether, our :func:`~torchtune.datasets.chat_dataset` call should\\nlook like so:\\n\\n.. code-block:: python\\n\\n    from torchtune.datasets import chat_dataset\\n    from torchtune.models.llama3 import llama3_tokenizer\\n\\n    tokenizer = llama3_tokenizer(\"/tmp/Meta-Llama-3-8B-Instruct/original/tokenizer.model\")\\n    ds = chat_dataset(\\n        tokenizer=tokenizer,\\n        source=\"json\",\\n        data_files=\"data/my_data.json\",\\n        split=\"train\",\\n        conversation_column=\"dialogue\",\\n        conversation_style=\"sharegpt\",\\n    )\\n\\n.. code-block:: yaml\\n\\n    # In config\\n    tokenizer:\\n      _component_: torchtune.models.llama3.llama3_tokenizer\\n      path: /tmp/Meta-Llama-3-8B-Instruct/original/tokenizer.model\\n\\n    dataset:\\n      _component_: torchtune.datasets.chat_dataset\\n      source: json\\n      data_files: data/my_data.json\\n      split: train\\n      conversation_column: dialogue\\n      conversation_style: sharegpt\\n\\n.. note::\\n    You can pass in any keyword argument for `load_dataset <https://huggingface.co/docs/datasets/v2.20.0/en/package_reference/loading_methods#datasets.load_dataset>`_ into all our\\n    Dataset classes and they will honor them. This is useful for common parameters\\n    such as specifying the data split with :code:`split` or configuration with\\n    :code:`name`\\n\\nIf you needed to add a prompt template, you would simply pass it into the tokenizer.\\nSince we\\'re fine-tuning Llama3, the tokenizer will handle all formatting for\\nus and prompt templates are optional. Other models such as Mistral\\'s :class:`~torchtune.models.mistral._tokenizer.MistralTokenizer`,\\nuse a chat template by default (:class:`~torchtune.models.mistral.MistralChatTemplate`) to format\\nall messages according to their `recommendations <https://\\n'), TextContentItem(type='text', text=\"Result 2:\\nDocument_id:65813\\nContent: .. _lora_finetune_label:\\n\\n============================\\nFine-Tuning Llama2 with LoRA\\n============================\\n\\nThis guide will teach you about `LoRA <https://arxiv.org/abs/2106.09685>`_, a parameter-efficient finetuning technique,\\nand show you how you can use torchtune to finetune a Llama2 model with LoRA.\\nIf you already know what LoRA is and want to get straight to running\\nyour own LoRA finetune in torchtune, you can jump to :ref:`LoRA finetuning recipe in torchtune<lora_recipe_label>`.\\n\\n.. grid:: 2\\n\\n    .. grid-item-card:: :octicon:`mortar-board;1em;` What you will learn\\n\\n      * What LoRA is and how it saves memory during finetuning\\n      * An overview of LoRA components in torchtune\\n      * How to run a LoRA finetune using torchtune\\n      * How to experiment with different LoRA configurations\\n\\n    .. grid-item-card:: :octicon:`list-unordered;1em;` Prerequisites\\n\\n      * Be familiar with :ref:`torchtune<overview_label>`\\n      * Make sure to :ref:`install torchtune<install_label>`\\n      * Make sure you have downloaded the :ref:`Llama2-7B model weights<download_llama_label>`\\n\\nWhat is LoRA?\\n-------------\\n\\n`LoRA <https://arxiv.org/abs/2106.09685>`_ is an adapter-based method for\\nparameter-efficient finetuning that adds trainable low-rank decomposition matrices to different layers of a neural network,\\nthen freezes the network's remaining parameters. LoRA is most commonly applied to\\ntransformer models, in which case it is common to add the low-rank matrices\\nto some of the linear projections in each transformer layer's self-attention.\\n\\n.. note::\\n\\n    If you're unfamiliar, check out these references for the `definition of rank <https://en.wikipedia.org/wiki/Rank_(linear_algebra)>`_\\n    and discussion of `low-rank approximations <https://en.wikipedia.org/wiki/Low-rank_approximation>`_.\\n\\nBy finetuning with LoRA (as opposed to finetuning all model parameters),\\nyou can expect to see memory savings due to a substantial reduction in the\\nnumber of parameters with gradients. When using an optimizer with momentum,\\nlike `AdamW <https://py\\n\"), TextContentItem(type='text', text='Result 3:\\nDocument_id:73794\\nContent: ` module, which we swap\\n   out for :class:`~torchtune.modules.peft.LoRALinear` when ``use_dora=True``.\\n\\n.. _glossary_distrib:\\n\\n\\n.. TODO\\n\\n.. Distributed\\n.. -----------\\n\\n.. .. _glossary_fsdp:\\n\\n.. Fully Sharded Data Parallel (FSDP)\\n.. ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\\n.. All our ``_distributed`` recipes use `FSDP <https://pytorch.org/docs/stable/fsdp.html>`.\\n.. .. _glossary_fsdp2:\\n\\n'), TextContentItem(type='text', text='Result 4:\\nDocument_id:65813\\nContent: 06% of all params are trainable.\\n\\n.. note::\\n    If you are directly using the LoRA recipe (as detailed :ref:`here<lora_recipe_label>`), you need only pass the\\n    relevant checkpoint path. Loading model weights and setting trainable parameters will be taken care\\n    of in the recipe.\\n\\n\\n.. _lora_recipe_label:\\n\\nLoRA finetuning recipe in torchtune\\n-----------------------------------\\n\\nFinally, we can put it all together and finetune a model using torchtune\\'s `LoRA recipe <https://github.com/pytorch/torchtune/blob/48626d19d2108f92c749411fbd5f0ff140023a25/recipes/lora_finetune.py>`_.\\nMake sure that you have first downloaded the Llama2 weights and tokenizer by following :ref:`these instructions<download_llama_label>`.\\nYou can then run the following command to perform a LoRA finetune of Llama2-7B with two GPUs (each having VRAM of at least 16GB):\\n\\n.. code-block:: bash\\n\\n    tune run --nnodes 1 --nproc_per_node 2 lora_finetune_distributed --config llama2/7B_lora\\n\\n.. note::\\n    Make sure to point to the location of your Llama2 weights and tokenizer. This can be done\\n    either by adding :code:`checkpointer.checkpoint_files=[my_model_checkpoint_path] tokenizer_checkpoint=my_tokenizer_checkpoint_path`\\n    or by directly modifying the :code:`7B_lora.yaml` file. See our \"\":ref:`config_tutorial_label`\" recipe\\n    for more details on how you can easily clone and modify torchtune configs.\\n\\n.. note::\\n    You can modify the value of :code:`nproc_per_node` depending on (a) the number of GPUs you have available,\\n    and (b) the memory constraints of your hardware.\\n\\nThe preceding command will run a LoRA finetune with torchtune\\'s factory settings, but we may want to experiment a bit.\\nLet\\'s take a closer look at some of the :code:`lora_finetune_distributed` config.\\n\\n.. code-block:: yaml\\n\\n  # Model Arguments\\n  model:\\n    _component_: lora_llama2_7b\\n    lora_attn_modules: [\\'q_proj\\', \\'v_proj\\']\\n    lora_rank: 8\\n    lora_alpha: 16\\n  ...\\n\\nWe see that the\\n'), TextContentItem(type='text', text='Result 5:\\nDocument_id:73794\\nContent: etune\\n:func:`torchtune.models.llama3.llama3_8b` with DoRA, you would use :func:`torchtune.models.llama3.lora_llama3_8b` with ``use_dora=True``:\\n\\n.. code-block:: bash\\n\\n  tune run lora_finetune_single_device --config llama3/8B_lora_single_device \\\\\\n  model.use_dora=True\\n\\n.. code-block:: yaml\\n\\n  model:\\n    _component_: torchtune.models.lora_llama3_8b\\n    use_dora: True\\n\\nSince DoRA extends LoRA, the parameters for :ref:`customizing LoRA <glossary_lora>` are identical. You can also quantize the base model weights like in :ref:`glossary_qlora` by using ``quantize=True`` to reap\\neven more memory savings!\\n\\n.. code-block:: bash\\n\\n  tune run lora_finetune_single_device --config llama3/8B_lora_single_device \\\\\\n  model.apply_lora_to_mlp=True \\\\\\n  model.lora_attn_modules=[\"q_proj\",\"k_proj\",\"v_proj\"] \\\\\\n  model.lora_rank=16 \\\\\\n  model.lora_alpha=32 \\\\\\n  model.use_dora=True \\\\\\n  model.quantize_base=True\\n\\n.. code-block:: yaml\\n\\n  model:\\n    _component_: torchtune.models.lora_llama3_8b\\n    apply_lora_to_mlp: True\\n    lora_attn_modules: [\"q_proj\", \"k_proj\", \"v_proj\"]\\n    lora_rank: 16\\n    lora_alpha: 32\\n    use_dora: True\\n    quantize_base: True\\n\\n\\n.. note::\\n\\n   Under the hood, we\\'ve enabled DoRA by adding the :class:`~torchtune.modules.peft.DoRALinear` module, which we swap\\n   out for :class:`~torchtune.modules.peft.LoRALinear` when ``use_dora=True``.\\n\\n.. _glossary_distrib:\\n\\n\\n.. TODO\\n\\n.. Distributed\\n.. -----------\\n\\n.. .. _glossary_fsdp:\\n\\n.. Fully Sharded Data Parallel (FSDP)\\n.. ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\\n.. All our ``_distributed`` recipes use `FSDP <https://pytorch.org/docs/stable/fsdp.html>`.\\n.. .. _glossary_fsdp2:\\n\\n'), TextContentItem(type='text', text='END of knowledge_search tool results.\\n')]), CompletionMessage(role='assistant', content='You can use the following function call to answer the user\\'s question:\\n\\n{\"type\": \"function\", \"name\": \"knowledge_search\", \"parameters\": {\"query\": \"How to fine-tune a Llama2 model with LoRA in torchtune\"}}', stop_reason=<StopReason.end_of_turn: 'end_of_turn'>, tool_calls=[]), UserMessage(role='user', content='Tell me how to use LoRA', context=None)])_[('response_format', None), ('sampling_params', SamplingParams(strategy=TopPSamplingStrategy(type='top_p', temperature=0.0001, top_p=0.9), max_tokens=0, repetition_penalty=1.0)), ('stream', True), ('tool_config', ToolConfig(tool_choice=<ToolChoice.auto: 'auto'>, tool_prompt_format=None, system_message_behavior=<SystemMessageBehavior.append: 'append'>)), ('tool_prompt_format', None), ('tools', [ToolDefinition(tool_name='knowledge_search', description='Search for information in a database.', parameters={'query': ToolParamDefinition(param_type='string', description='The query to search for. Can be a natural language sentence or keywords.', required=True, default=None)})])]": {
    "chunks": [
      {
        "event": {
          "delta": {
            "text": "",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "start"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "{\"",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "type\": \"function\", \"name\": \"knowledge_search\", \"",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "parameters\": {\"query\": \"How to use LoRA\"}}",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "succeeded"
            },
            "tool_call": {
              "arguments": {
                "query": "How to use LoRA"
              },
              "call_id": "72fd6ac0-2a5c-46ce-87c3-8a8c470a6b4a",
              "tool_name": "knowledge_search"
            },
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": {
            "__enum__": "StopReason",
            "value": "end_of_turn"
          }
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "complete"
          },
          "logprobs": null,
          "stop_reason": {
            "__enum__": "StopReason",
            "value": "end_of_turn"
          }
        },
        "metrics": null
      }
    ],
    "type": "generator"
  },
  "('meta-llama/Llama-3.1-8B-Instruct', [SystemMessage(role='system', content='You are a helpful assistant'), UserMessage(role='user', content='I am attaching some documentation for Torchtune. Help me answer questions I will ask next.', context=None), CompletionMessage(role='assistant', content='', stop_reason=<StopReason.end_of_turn: 'end_of_turn'>, tool_calls=[ToolCall(call_id='<UUID>', tool_name='knowledge_search', arguments={'query': 'Torchtune documentation'})]), ToolResponseMessage(role='tool', call_id='<UUID>', tool_name='knowledge_search', content=[TextContentItem(type='text', text='knowledge_search tool found 5 chunks:\\nBEGIN of knowledge_search tool results.\\n'), TextContentItem(type='text', text='Result 1:\\nDocument_id:ee8b0\\nContent:  conversational data, :func:`~torchtune.datasets.chat_dataset` seems to be a good fit. For any\\ncustom local dataset we always need to specify ``source``, ``data_files``, and ``split`` for any dataset\\nbuilder in torchtune. For :func:`~torchtune.datasets.chat_dataset`, we additionally need to specify\\n``conversation_column`` and ``conversation_style``. Our data follows the ``\"sharegpt\"`` format, so\\nwe can specify that here. Altogether, our :func:`~torchtune.datasets.chat_dataset` call should\\nlook like so:\\n\\n.. code-block:: python\\n\\n    from torchtune.datasets import chat_dataset\\n    from torchtune.models.llama3 import llama3_tokenizer\\n\\n    tokenizer = llama3_tokenizer(\"/tmp/Meta-Llama-3-8B-Instruct/original/tokenizer.model\")\\n    ds = chat_dataset(\\n        tokenizer=tokenizer,\\n        source=\"json\",\\n        data_files=\"data/my_data.json\",\\n        split=\"train\",\\n        conversation_column=\"dialogue\",\\n        conversation_style=\"sharegpt\",\\n    )\\n\\n.. code-block:: yaml\\n\\n    # In config\\n    tokenizer:\\n      _component_: torchtune.models.llama3.llama3_tokenizer\\n      path: /tmp/Meta-Llama-3-8B-Instruct/original/tokenizer.model\\n\\n    dataset:\\n      _component_: torchtune.datasets.chat_dataset\\n      source: json\\n      data_files: data/my_data.json\\n      split: train\\n      conversation_column: dialogue\\n      conversation_style: sharegpt\\n\\n.. note::\\n    You can pass in any keyword argument for `load_dataset <https://huggingface.co/docs/datasets/v2.20.0/en/package_reference/loading_methods#datasets.load_dataset>`_ into all our\\n    Dataset classes and they will honor them. This is useful for common parameters\\n    such as specifying the data split with :code:`split` or configuration with\\n    :code:`name`\\n\\nIf you needed to add a prompt template, you would simply pass it into the tokenizer.\\nSince we\\'re fine-tuning Llama3, the tokenizer will handle all formatting for\\nus and prompt templates are optional. Other models such as Mistral\\'s :class:`~torchtune.models.mistral._tokenizer.MistralTokenizer`,\\nuse a chat template by default (:class:`~torchtune.models.mistral.MistralChatTemplate`) to format\\nall messages according to their `recommendations <https://\\n'), TextContentItem(type='text', text=\"Result 2:\\nDocument_id:65813\\nContent: .. _lora_finetune_label:\\n\\n============================\\nFine-Tuning Llama2 with LoRA\\n============================\\n\\nThis guide will teach you about `LoRA <https://arxiv.org/abs/2106.09685>`_, a parameter-efficient finetuning technique,\\nand show you how you can use torchtune to finetune a Llama2 model with LoRA.\\nIf you already know what LoRA is and want to get straight to running\\nyour own LoRA finetune in torchtune, you can jump to :ref:`LoRA finetuning recipe in torchtune<lora_recipe_label>`.\\n\\n.. grid:: 2\\n\\n    .. grid-item-card:: :octicon:`mortar-board;1em;` What you will learn\\n\\n      * What LoRA is and how it saves memory during finetuning\\n      * An overview of LoRA components in torchtune\\n      * How to run a LoRA finetune using torchtune\\n      * How to experiment with different LoRA configurations\\n\\n    .. grid-item-card:: :octicon:`list-unordered;1em;` Prerequisites\\n\\n      * Be familiar with :ref:`torchtune<overview_label>`\\n      * Make sure to :ref:`install torchtune<install_label>`\\n      * Make sure you have downloaded the :ref:`Llama2-7B model weights<download_llama_label>`\\n\\nWhat is LoRA?\\n-------------\\n\\n`LoRA <https://arxiv.org/abs/2106.09685>`_ is an adapter-based method for\\nparameter-efficient finetuning that adds trainable low-rank decomposition matrices to different layers of a neural network,\\nthen freezes the network's remaining parameters. LoRA is most commonly applied to\\ntransformer models, in which case it is common to add the low-rank matrices\\nto some of the linear projections in each transformer layer's self-attention.\\n\\n.. note::\\n\\n    If you're unfamiliar, check out these references for the `definition of rank <https://en.wikipedia.org/wiki/Rank_(linear_algebra)>`_\\n    and discussion of `low-rank approximations <https://en.wikipedia.org/wiki/Low-rank_approximation>`_.\\n\\nBy finetuning with LoRA (as opposed to finetuning all model parameters),\\nyou can expect to see memory savings due to a substantial reduction in the\\nnumber of parameters with gradients. When using an optimizer with momentum,\\nlike `AdamW <https://py\\n\"), TextContentItem(type='text', text='Result 3:\\nDocument_id:73794\\nContent: ` module, which we swap\\n   out for :class:`~torchtune.modules.peft.LoRALinear` when ``use_dora=True``.\\n\\n.. _glossary_distrib:\\n\\n\\n.. TODO\\n\\n.. Distributed\\n.. -----------\\n\\n.. .. _glossary_fsdp:\\n\\n.. Fully Sharded Data Parallel (FSDP)\\n.. ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\\n.. All our ``_distributed`` recipes use `FSDP <https://pytorch.org/docs/stable/fsdp.html>`.\\n.. .. _glossary_fsdp2:\\n\\n'), TextContentItem(type='text', text='Result 4:\\nDocument_id:65813\\nContent: 06% of all params are trainable.\\n\\n.. note::\\n    If you are directly using the LoRA recipe (as detailed :ref:`here<lora_recipe_label>`), you need only pass the\\n    relevant checkpoint path. Loading model weights and setting trainable parameters will be taken care\\n    of in the recipe.\\n\\n\\n.. _lora_recipe_label:\\n\\nLoRA finetuning recipe in torchtune\\n-----------------------------------\\n\\nFinally, we can put it all together and finetune a model using torchtune\\'s `LoRA recipe <https://github.com/pytorch/torchtune/blob/48626d19d2108f92c749411fbd5f0ff140023a25/recipes/lora_finetune.py>`_.\\nMake sure that you have first downloaded the Llama2 weights and tokenizer by following :ref:`these instructions<download_llama_label>`.\\nYou can then run the following command to perform a LoRA finetune of Llama2-7B with two GPUs (each having VRAM of at least 16GB):\\n\\n.. code-block:: bash\\n\\n    tune run --nnodes 1 --nproc_per_node 2 lora_finetune_distributed --config llama2/7B_lora\\n\\n.. note::\\n    Make sure to point to the location of your Llama2 weights and tokenizer. This can be done\\n    either by adding :code:`checkpointer.checkpoint_files=[my_model_checkpoint_path] tokenizer_checkpoint=my_tokenizer_checkpoint_path`\\n    or by directly modifying the :code:`7B_lora.yaml` file. See our \"\":ref:`config_tutorial_label`\" recipe\\n    for more details on how you can easily clone and modify torchtune configs.\\n\\n.. note::\\n    You can modify the value of :code:`nproc_per_node` depending on (a) the number of GPUs you have available,\\n    and (b) the memory constraints of your hardware.\\n\\nThe preceding command will run a LoRA finetune with torchtune\\'s factory settings, but we may want to experiment a bit.\\nLet\\'s take a closer look at some of the :code:`lora_finetune_distributed` config.\\n\\n.. code-block:: yaml\\n\\n  # Model Arguments\\n  model:\\n    _component_: lora_llama2_7b\\n    lora_attn_modules: [\\'q_proj\\', \\'v_proj\\']\\n    lora_rank: 8\\n    lora_alpha: 16\\n  ...\\n\\nWe see that the\\n'), TextContentItem(type='text', text='Result 5:\\nDocument_id:73794\\nContent: etune\\n:func:`torchtune.models.llama3.llama3_8b` with DoRA, you would use :func:`torchtune.models.llama3.lora_llama3_8b` with ``use_dora=True``:\\n\\n.. code-block:: bash\\n\\n  tune run lora_finetune_single_device --config llama3/8B_lora_single_device \\\\\\n  model.use_dora=True\\n\\n.. code-block:: yaml\\n\\n  model:\\n    _component_: torchtune.models.lora_llama3_8b\\n    use_dora: True\\n\\nSince DoRA extends LoRA, the parameters for :ref:`customizing LoRA <glossary_lora>` are identical. You can also quantize the base model weights like in :ref:`glossary_qlora` by using ``quantize=True`` to reap\\neven more memory savings!\\n\\n.. code-block:: bash\\n\\n  tune run lora_finetune_single_device --config llama3/8B_lora_single_device \\\\\\n  model.apply_lora_to_mlp=True \\\\\\n  model.lora_attn_modules=[\"q_proj\",\"k_proj\",\"v_proj\"] \\\\\\n  model.lora_rank=16 \\\\\\n  model.lora_alpha=32 \\\\\\n  model.use_dora=True \\\\\\n  model.quantize_base=True\\n\\n.. code-block:: yaml\\n\\n  model:\\n    _component_: torchtune.models.lora_llama3_8b\\n    apply_lora_to_mlp: True\\n    lora_attn_modules: [\"q_proj\", \"k_proj\", \"v_proj\"]\\n    lora_rank: 16\\n    lora_alpha: 32\\n    use_dora: True\\n    quantize_base: True\\n\\n\\n.. note::\\n\\n   Under the hood, we\\'ve enabled DoRA by adding the :class:`~torchtune.modules.peft.DoRALinear` module, which we swap\\n   out for :class:`~torchtune.modules.peft.LoRALinear` when ``use_dora=True``.\\n\\n.. _glossary_distrib:\\n\\n\\n.. TODO\\n\\n.. Distributed\\n.. -----------\\n\\n.. .. _glossary_fsdp:\\n\\n.. Fully Sharded Data Parallel (FSDP)\\n.. ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\\n.. All our ``_distributed`` recipes use `FSDP <https://pytorch.org/docs/stable/fsdp.html>`.\\n.. .. _glossary_fsdp2:\\n\\n'), TextContentItem(type='text', text='END of knowledge_search tool results.\\n')])])_[('response_format', None), ('sampling_params', SamplingParams(strategy=TopPSamplingStrategy(type='top_p', temperature=0.0001, top_p=0.9), max_tokens=0, repetition_penalty=1.0)), ('stream', True), ('tool_config', ToolConfig(tool_choice=<ToolChoice.auto: 'auto'>, tool_prompt_format=None, system_message_behavior=<SystemMessageBehavior.append: 'append'>)), ('tool_prompt_format', None), ('tools', [ToolDefinition(tool_name='knowledge_search', description='Search for information in a database.', parameters={'query': ToolParamDefinition(param_type='string', description='The query to search for. Can be a natural language sentence or keywords.', required=True, default=None)})])]": {
    "chunks": [
      {
        "event": {
          "delta": {
            "text": "",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "start"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "You",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " can use the following function call to answer",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " the user's question:\n\n{\"type\": \"function\", \"",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "name\": \"knowledge_search\", \"parameters\": {\"query\": \"",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "How to fine-tune a Llama",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "2 model with LoRA in torchtune",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "\"}}",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "complete"
          },
          "logprobs": null,
          "stop_reason": {
            "__enum__": "StopReason",
            "value": "end_of_turn"
          }
        },
        "metrics": null
      }
    ],
    "type": "generator"
  },
  "('meta-llama/Llama-3.1-8B-Instruct', [SystemMessage(role='system', content='You are a helpful assistant'), UserMessage(role='user', content='I am attaching some documentation for Torchtune. Help me answer questions I will ask next.', context=None), CompletionMessage(role='assistant', content='', stop_reason=<StopReason.end_of_turn: 'end_of_turn'>, tool_calls=[ToolCall(call_id='<UUID>', tool_name='knowledge_search', arguments={'query': 'Torchtune documentation'})]), ToolResponseMessage(role='tool', call_id='<UUID>', tool_name='knowledge_search', content=[TextContentItem(type='text', text='knowledge_search tool found 5 chunks:\\nBEGIN of knowledge_search tool results.\\n'), TextContentItem(type='text', text='Result 1:\\nDocument_id:f4fd3\\nContent:  conversational data, :func:`~torchtune.datasets.chat_dataset` seems to be a good fit. For any\\ncustom local dataset we always need to specify ``source``, ``data_files``, and ``split`` for any dataset\\nbuilder in torchtune. For :func:`~torchtune.datasets.chat_dataset`, we additionally need to specify\\n``conversation_column`` and ``conversation_style``. Our data follows the ``\"sharegpt\"`` format, so\\nwe can specify that here. Altogether, our :func:`~torchtune.datasets.chat_dataset` call should\\nlook like so:\\n\\n.. code-block:: python\\n\\n    from torchtune.datasets import chat_dataset\\n    from torchtune.models.llama3 import llama3_tokenizer\\n\\n    tokenizer = llama3_tokenizer(\"/tmp/Meta-Llama-3-8B-Instruct/original/tokenizer.model\")\\n    ds = chat_dataset(\\n        tokenizer=tokenizer,\\n        source=\"json\",\\n        data_files=\"data/my_data.json\",\\n        split=\"train\",\\n        conversation_column=\"dialogue\",\\n        conversation_style=\"sharegpt\",\\n    )\\n\\n.. code-block:: yaml\\n\\n    # In config\\n    tokenizer:\\n      _component_: torchtune.models.llama3.llama3_tokenizer\\n      path: /tmp/Meta-Llama-3-8B-Instruct/original/tokenizer.model\\n\\n    dataset:\\n      _component_: torchtune.datasets.chat_dataset\\n      source: json\\n      data_files: data/my_data.json\\n      split: train\\n      conversation_column: dialogue\\n      conversation_style: sharegpt\\n\\n.. note::\\n    You can pass in any keyword argument for `load_dataset <https://huggingface.co/docs/datasets/v2.20.0/en/package_reference/loading_methods#datasets.load_dataset>`_ into all our\\n    Dataset classes and they will honor them. This is useful for common parameters\\n    such as specifying the data split with :code:`split` or configuration with\\n    :code:`name`\\n\\nIf you needed to add a prompt template, you would simply pass it into the tokenizer.\\nSince we\\'re fine-tuning Llama3, the tokenizer will handle all formatting for\\nus and prompt templates are optional. Other models such as Mistral\\'s :class:`~torchtune.models.mistral._tokenizer.MistralTokenizer`,\\nuse a chat template by default (:class:`~torchtune.models.mistral.MistralChatTemplate`) to format\\nall messages according to their `recommendations <https://\\n'), TextContentItem(type='text', text=\"Result 2:\\nDocument_id:cbc88\\nContent: .. _lora_finetune_label:\\n\\n============================\\nFine-Tuning Llama2 with LoRA\\n============================\\n\\nThis guide will teach you about `LoRA <https://arxiv.org/abs/2106.09685>`_, a parameter-efficient finetuning technique,\\nand show you how you can use torchtune to finetune a Llama2 model with LoRA.\\nIf you already know what LoRA is and want to get straight to running\\nyour own LoRA finetune in torchtune, you can jump to :ref:`LoRA finetuning recipe in torchtune<lora_recipe_label>`.\\n\\n.. grid:: 2\\n\\n    .. grid-item-card:: :octicon:`mortar-board;1em;` What you will learn\\n\\n      * What LoRA is and how it saves memory during finetuning\\n      * An overview of LoRA components in torchtune\\n      * How to run a LoRA finetune using torchtune\\n      * How to experiment with different LoRA configurations\\n\\n    .. grid-item-card:: :octicon:`list-unordered;1em;` Prerequisites\\n\\n      * Be familiar with :ref:`torchtune<overview_label>`\\n      * Make sure to :ref:`install torchtune<install_label>`\\n      * Make sure you have downloaded the :ref:`Llama2-7B model weights<download_llama_label>`\\n\\nWhat is LoRA?\\n-------------\\n\\n`LoRA <https://arxiv.org/abs/2106.09685>`_ is an adapter-based method for\\nparameter-efficient finetuning that adds trainable low-rank decomposition matrices to different layers of a neural network,\\nthen freezes the network's remaining parameters. LoRA is most commonly applied to\\ntransformer models, in which case it is common to add the low-rank matrices\\nto some of the linear projections in each transformer layer's self-attention.\\n\\n.. note::\\n\\n    If you're unfamiliar, check out these references for the `definition of rank <https://en.wikipedia.org/wiki/Rank_(linear_algebra)>`_\\n    and discussion of `low-rank approximations <https://en.wikipedia.org/wiki/Low-rank_approximation>`_.\\n\\nBy finetuning with LoRA (as opposed to finetuning all model parameters),\\nyou can expect to see memory savings due to a substantial reduction in the\\nnumber of parameters with gradients. When using an optimizer with momentum,\\nlike `AdamW <https://py\\n\"), TextContentItem(type='text', text='Result 3:\\nDocument_id:8892b\\nContent: ` module, which we swap\\n   out for :class:`~torchtune.modules.peft.LoRALinear` when ``use_dora=True``.\\n\\n.. _glossary_distrib:\\n\\n\\n.. TODO\\n\\n.. Distributed\\n.. -----------\\n\\n.. .. _glossary_fsdp:\\n\\n.. Fully Sharded Data Parallel (FSDP)\\n.. ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\\n.. All our ``_distributed`` recipes use `FSDP <https://pytorch.org/docs/stable/fsdp.html>`.\\n.. .. _glossary_fsdp2:\\n\\n'), TextContentItem(type='text', text='Result 4:\\nDocument_id:cbc88\\nContent: 06% of all params are trainable.\\n\\n.. note::\\n    If you are directly using the LoRA recipe (as detailed :ref:`here<lora_recipe_label>`), you need only pass the\\n    relevant checkpoint path. Loading model weights and setting trainable parameters will be taken care\\n    of in the recipe.\\n\\n\\n.. _lora_recipe_label:\\n\\nLoRA finetuning recipe in torchtune\\n-----------------------------------\\n\\nFinally, we can put it all together and finetune a model using torchtune\\'s `LoRA recipe <https://github.com/pytorch/torchtune/blob/48626d19d2108f92c749411fbd5f0ff140023a25/recipes/lora_finetune.py>`_.\\nMake sure that you have first downloaded the Llama2 weights and tokenizer by following :ref:`these instructions<download_llama_label>`.\\nYou can then run the following command to perform a LoRA finetune of Llama2-7B with two GPUs (each having VRAM of at least 16GB):\\n\\n.. code-block:: bash\\n\\n    tune run --nnodes 1 --nproc_per_node 2 lora_finetune_distributed --config llama2/7B_lora\\n\\n.. note::\\n    Make sure to point to the location of your Llama2 weights and tokenizer. This can be done\\n    either by adding :code:`checkpointer.checkpoint_files=[my_model_checkpoint_path] tokenizer_checkpoint=my_tokenizer_checkpoint_path`\\n    or by directly modifying the :code:`7B_lora.yaml` file. See our \"\":ref:`config_tutorial_label`\" recipe\\n    for more details on how you can easily clone and modify torchtune configs.\\n\\n.. note::\\n    You can modify the value of :code:`nproc_per_node` depending on (a) the number of GPUs you have available,\\n    and (b) the memory constraints of your hardware.\\n\\nThe preceding command will run a LoRA finetune with torchtune\\'s factory settings, but we may want to experiment a bit.\\nLet\\'s take a closer look at some of the :code:`lora_finetune_distributed` config.\\n\\n.. code-block:: yaml\\n\\n  # Model Arguments\\n  model:\\n    _component_: lora_llama2_7b\\n    lora_attn_modules: [\\'q_proj\\', \\'v_proj\\']\\n    lora_rank: 8\\n    lora_alpha: 16\\n  ...\\n\\nWe see that the\\n'), TextContentItem(type='text', text='Result 5:\\nDocument_id:8892b\\nContent: etune\\n:func:`torchtune.models.llama3.llama3_8b` with DoRA, you would use :func:`torchtune.models.llama3.lora_llama3_8b` with ``use_dora=True``:\\n\\n.. code-block:: bash\\n\\n  tune run lora_finetune_single_device --config llama3/8B_lora_single_device \\\\\\n  model.use_dora=True\\n\\n.. code-block:: yaml\\n\\n  model:\\n    _component_: torchtune.models.lora_llama3_8b\\n    use_dora: True\\n\\nSince DoRA extends LoRA, the parameters for :ref:`customizing LoRA <glossary_lora>` are identical. You can also quantize the base model weights like in :ref:`glossary_qlora` by using ``quantize=True`` to reap\\neven more memory savings!\\n\\n.. code-block:: bash\\n\\n  tune run lora_finetune_single_device --config llama3/8B_lora_single_device \\\\\\n  model.apply_lora_to_mlp=True \\\\\\n  model.lora_attn_modules=[\"q_proj\",\"k_proj\",\"v_proj\"] \\\\\\n  model.lora_rank=16 \\\\\\n  model.lora_alpha=32 \\\\\\n  model.use_dora=True \\\\\\n  model.quantize_base=True\\n\\n.. code-block:: yaml\\n\\n  model:\\n    _component_: torchtune.models.lora_llama3_8b\\n    apply_lora_to_mlp: True\\n    lora_attn_modules: [\"q_proj\", \"k_proj\", \"v_proj\"]\\n    lora_rank: 16\\n    lora_alpha: 32\\n    use_dora: True\\n    quantize_base: True\\n\\n\\n.. note::\\n\\n   Under the hood, we\\'ve enabled DoRA by adding the :class:`~torchtune.modules.peft.DoRALinear` module, which we swap\\n   out for :class:`~torchtune.modules.peft.LoRALinear` when ``use_dora=True``.\\n\\n.. _glossary_distrib:\\n\\n\\n.. TODO\\n\\n.. Distributed\\n.. -----------\\n\\n.. .. _glossary_fsdp:\\n\\n.. Fully Sharded Data Parallel (FSDP)\\n.. ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\\n.. All our ``_distributed`` recipes use `FSDP <https://pytorch.org/docs/stable/fsdp.html>`.\\n.. .. _glossary_fsdp2:\\n\\n'), TextContentItem(type='text', text='END of knowledge_search tool results.\\n')]), CompletionMessage(role='assistant', content='You can use the following function call to answer the user\\'s question:\\n\\n{\"type\": \"function\", \"name\": \"knowledge_search\", \"parameters\": {\"query\": \"How to fine-tune a Llama2 model with LoRA in torchtune\"}}', stop_reason=<StopReason.end_of_turn: 'end_of_turn'>, tool_calls=[]), UserMessage(role='user', content='Tell me how to use LoRA', context=None), CompletionMessage(role='assistant', content='', stop_reason=<StopReason.end_of_turn: 'end_of_turn'>, tool_calls=[ToolCall(call_id='<UUID>', tool_name='knowledge_search', arguments={'query': 'How to use LoRA'})]), ToolResponseMessage(role='tool', call_id='<UUID>', tool_name='knowledge_search', content=[TextContentItem(type='text', text='knowledge_search tool found 5 chunks:\\nBEGIN of knowledge_search tool results.\\n'), TextContentItem(type='text', text=\"Result 1:\\nDocument_id:cbc88\\nContent: .. _lora_finetune_label:\\n\\n============================\\nFine-Tuning Llama2 with LoRA\\n============================\\n\\nThis guide will teach you about `LoRA <https://arxiv.org/abs/2106.09685>`_, a parameter-efficient finetuning technique,\\nand show you how you can use torchtune to finetune a Llama2 model with LoRA.\\nIf you already know what LoRA is and want to get straight to running\\nyour own LoRA finetune in torchtune, you can jump to :ref:`LoRA finetuning recipe in torchtune<lora_recipe_label>`.\\n\\n.. grid:: 2\\n\\n    .. grid-item-card:: :octicon:`mortar-board;1em;` What you will learn\\n\\n      * What LoRA is and how it saves memory during finetuning\\n      * An overview of LoRA components in torchtune\\n      * How to run a LoRA finetune using torchtune\\n      * How to experiment with different LoRA configurations\\n\\n    .. grid-item-card:: :octicon:`list-unordered;1em;` Prerequisites\\n\\n      * Be familiar with :ref:`torchtune<overview_label>`\\n      * Make sure to :ref:`install torchtune<install_label>`\\n      * Make sure you have downloaded the :ref:`Llama2-7B model weights<download_llama_label>`\\n\\nWhat is LoRA?\\n-------------\\n\\n`LoRA <https://arxiv.org/abs/2106.09685>`_ is an adapter-based method for\\nparameter-efficient finetuning that adds trainable low-rank decomposition matrices to different layers of a neural network,\\nthen freezes the network's remaining parameters. LoRA is most commonly applied to\\ntransformer models, in which case it is common to add the low-rank matrices\\nto some of the linear projections in each transformer layer's self-attention.\\n\\n.. note::\\n\\n    If you're unfamiliar, check out these references for the `definition of rank <https://en.wikipedia.org/wiki/Rank_(linear_algebra)>`_\\n    and discussion of `low-rank approximations <https://en.wikipedia.org/wiki/Low-rank_approximation>`_.\\n\\nBy finetuning with LoRA (as opposed to finetuning all model parameters),\\nyou can expect to see memory savings due to a substantial reduction in the\\nnumber of parameters with gradients. When using an optimizer with momentum,\\nlike `AdamW <https://py\\n\"), TextContentItem(type='text', text='Result 2:\\nDocument_id:cbc88\\nContent: 06% of all params are trainable.\\n\\n.. note::\\n    If you are directly using the LoRA recipe (as detailed :ref:`here<lora_recipe_label>`), you need only pass the\\n    relevant checkpoint path. Loading model weights and setting trainable parameters will be taken care\\n    of in the recipe.\\n\\n\\n.. _lora_recipe_label:\\n\\nLoRA finetuning recipe in torchtune\\n-----------------------------------\\n\\nFinally, we can put it all together and finetune a model using torchtune\\'s `LoRA recipe <https://github.com/pytorch/torchtune/blob/48626d19d2108f92c749411fbd5f0ff140023a25/recipes/lora_finetune.py>`_.\\nMake sure that you have first downloaded the Llama2 weights and tokenizer by following :ref:`these instructions<download_llama_label>`.\\nYou can then run the following command to perform a LoRA finetune of Llama2-7B with two GPUs (each having VRAM of at least 16GB):\\n\\n.. code-block:: bash\\n\\n    tune run --nnodes 1 --nproc_per_node 2 lora_finetune_distributed --config llama2/7B_lora\\n\\n.. note::\\n    Make sure to point to the location of your Llama2 weights and tokenizer. This can be done\\n    either by adding :code:`checkpointer.checkpoint_files=[my_model_checkpoint_path] tokenizer_checkpoint=my_tokenizer_checkpoint_path`\\n    or by directly modifying the :code:`7B_lora.yaml` file. See our \"\":ref:`config_tutorial_label`\" recipe\\n    for more details on how you can easily clone and modify torchtune configs.\\n\\n.. note::\\n    You can modify the value of :code:`nproc_per_node` depending on (a) the number of GPUs you have available,\\n    and (b) the memory constraints of your hardware.\\n\\nThe preceding command will run a LoRA finetune with torchtune\\'s factory settings, but we may want to experiment a bit.\\nLet\\'s take a closer look at some of the :code:`lora_finetune_distributed` config.\\n\\n.. code-block:: yaml\\n\\n  # Model Arguments\\n  model:\\n    _component_: lora_llama2_7b\\n    lora_attn_modules: [\\'q_proj\\', \\'v_proj\\']\\n    lora_rank: 8\\n    lora_alpha: 16\\n  ...\\n\\nWe see that the\\n'), TextContentItem(type='text', text='Result 3:\\nDocument_id:8892b\\nContent:  with training with LoRA quickly,\\njust specify any config with ``_lora`` in its name, e.g:\\n\\n.. code-block:: bash\\n\\n  tune run lora_finetune_single_device --config llama3/8B_lora_single_device\\n\\n\\nThere are two sets of parameters to customize LoRA to suit your needs. Firstly, the parameters which control\\nwhich linear layers LoRA should be applied to in the model:\\n\\n* ``lora_attn_modules: List[str]`` accepts a list of strings specifying which layers of the model to apply\\n  LoRA to:\\n\\n  * ``q_proj`` applies LoRA to the query projection layer.\\n  * ``k_proj`` applies LoRA to the key projection layer.\\n  * ``v_proj`` applies LoRA to the value projection layer.\\n  * ``output_proj`` applies LoRA to the attention output projection layer.\\n\\n  Whilst adding more layers to be fine-tuned may improve model accuracy,\\n  this will come at the cost of increased memory usage and reduced training speed.\\n\\n* ``apply_lora_to_mlp: Bool`` applies LoRA to the MLP in each transformer layer.\\n* ``apply_lora_to_output: Bool`` applies LoRA to the model\\'s final output projection.\\n  This is usually a projection to vocabulary space (e.g. in language models), but\\n  other modelling tasks may have different projections - classifier models will project\\n  to the number of classes, for example\\n\\n.. note::\\n\\n  Models which use tied embeddings (such as Gemma and Qwen2 1.5B and 0.5B) for the\\n  final output projection do not support ``apply_lora_to_output``.\\n\\nThese are all specified under the ``model`` flag or config entry, i.e:\\n\\n.. code-block:: bash\\n\\n  tune run lora_finetune_single_device --config llama3/8B_lora_single_device  \\\\\\n  model.apply_lora_to_mlp=True \\\\\\n  model.lora_attn_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"output_proj\"]\\n\\n.. code-block:: yaml\\n\\n  model:\\n    _component_: torchtune.models.llama3.lora_llama3_8b\\n    apply_lora_to_mlp: True\\n    model.lora_attn_modules: [\"q_proj\", \"k_proj\", \"v_proj\",\"output_proj\"]\\n\\nSecondly, parameters which control the scale of the impact of LoRA on the model:\\n\\n* ``lora_rank: int`` affects the scale of\\n'), TextContentItem(type='text', text='Result 4:\\nDocument_id:cbc88\\nContent:  LoRA to Llama2 models\\n------------------------------\\n\\nWith torchtune, we can easily apply LoRA to Llama2 with a variety of different configurations.\\nLet\\'s take a look at how to construct Llama2 models in torchtune with and without LoRA.\\n\\n.. code-block:: python\\n\\n  from torchtune.models.llama2 import llama2_7b, lora_llama2_7b\\n\\n  # Build Llama2 without any LoRA layers\\n  base_model = llama2_7b()\\n\\n  # The default settings for lora_llama2_7b will match those for llama2_7b\\n  # We just need to define which layers we want LoRA applied to.\\n  # Within each self-attention, we can choose from [\"q_proj\", \"k_proj\", \"v_proj\", and \"output_proj\"].\\n  # We can also set apply_lora_to_mlp=True or apply_lora_to_output=True to apply LoRA to other linear\\n  # layers outside of the self-attention.\\n  lora_model = lora_llama2_7b(lora_attn_modules=[\"q_proj\", \"v_proj\"])\\n\\n.. note::\\n\\n    Calling :func:`lora_llama_2_7b <torchtune.models.llama2.lora_llama2_7b>` alone will not handle the definition of which parameters are trainable.\\n    See :ref:`below<setting_trainable_params>` for how to do this.\\n\\nLet\\'s inspect each of these models a bit more closely.\\n\\n.. code-block:: bash\\n\\n  # Print the first layer\\'s self-attention in the usual Llama2 model\\n  >>> print(base_model.layers[0].attn)\\n  MultiHeadAttention(\\n    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\\n    (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\\n    (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\\n    (output_proj): Linear(in_features=4096, out_features=4096, bias=False)\\n    (pos_embeddings): RotaryPositionalEmbeddings()\\n  )\\n\\n  # Print the same for Llama2 with LoRA weights\\n  >>> print(lora_model.layers[0].attn)\\n  MultiHeadAttention(\\n    (q_proj): LoRALinear(\\n      (dropout): Dropout(p=0.0, inplace=False)\\n     \\n'), TextContentItem(type='text', text='Result 5:\\nDocument_id:9dcb7\\nContent: ora_finetune_label>`.\\nFor more on QLoRA in torchtune, see our :ref:`QLoRA Tutorial <qlora_finetune_label>`.\\n\\nLet\\'s take a look at how we can fine-tune Llama3-8B-Instruct with LoRA on a single device using torchtune. In this example, we will fine-tune\\nfor one epoch on a common instruct dataset for illustrative purposes. The basic command for a single-device LoRA fine-tune is\\n\\n.. code-block:: bash\\n\\n    tune run lora_finetune_single_device --config llama3/8B_lora_single_device\\n\\n.. note::\\n    To see a full list of recipes and their corresponding configs, simply run ``tune ls`` from the command line.\\n\\nWe can also add :ref:`command-line overrides <cli_override>` as needed, e.g.\\n\\n.. code-block:: bash\\n\\n    tune run lora_finetune_single_device --config llama3/8B_lora_single_device \\\\\\n        checkpointer.checkpoint_dir=<checkpoint_dir> \\\\\\n        tokenizer.path=<checkpoint_dir>/tokenizer.model \\\\\\n        checkpointer.output_dir=<checkpoint_dir>\\n\\nThis will load the Llama3-8B-Instruct checkpoint and tokenizer from ``<checkpoint_dir>`` used in the :ref:`tune download <tune_download_label>` command above,\\nthen save a final checkpoint in the same directory following the original format. For more details on the\\ncheckpoint formats supported in torchtune, see our :ref:`checkpointing deep-dive <understand_checkpointer>`.\\n\\n.. note::\\n    To see the full set of configurable parameters for this (and other) configs we can use :ref:`tune cp <tune_cp_cli_label>` to copy (and modify)\\n    the default config. :ref:`tune cp <tune_cp_cli_label>` can be used with recipe scripts too, in case you want to make more custom changes\\n    that cannot be achieved by directly modifying existing configurable parameters. For more on :ref:`tune cp <tune_cp_cli_label>` see the section on\\n    :ref:`modifying configs <tune_cp_label>` in our \":ref:`finetune_llama_label`\" tutorial.\\n\\nOnce training is complete, the model checkpoints will be saved and their locations will be logged. For\\nLoRA fine-tuning, the final checkpoint will contain the merged weights, and a copy of just the (much smaller) LoRA weights\\nwill\\n'), TextContentItem(type='text', text='END of knowledge_search tool results.\\n')])])_[('response_format', None), ('sampling_params', SamplingParams(strategy=TopPSamplingStrategy(type='top_p', temperature=0.0001, top_p=0.9), max_tokens=0, repetition_penalty=1.0)), ('stream', True), ('tool_config', ToolConfig(tool_choice=<ToolChoice.auto: 'auto'>, tool_prompt_format=None, system_message_behavior=<SystemMessageBehavior.append: 'append'>)), ('tool_prompt_format', None), ('tools', [ToolDefinition(tool_name='knowledge_search', description='Search for information in a database.', parameters={'query': ToolParamDefinition(param_type='string', description='The query to search for. Can be a natural language sentence or keywords.', required=True, default=None)})])]": {
    "chunks": [
      {
        "event": {
          "delta": {
            "text": "",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "start"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "To",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " use LoRA, you can follow these steps:\n\n1.",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "  Install the necessary packages, including torchtune and the L",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "lama2 model.\n2.  Load the Llama2 model",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " and specify which layers to apply LoRA to.\n3. ",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " Define the LoRA parameters, such as the rank and alpha values",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": ".\n4.  Train the model using the LoRA fine-t",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "uning recipe in torchtune.\n\nHere is",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " an example of how to use",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " LoRA with the Llama2 model",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": ":\n\n```python\nfrom torchtune.models.llama2 import",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " llama2_7b, lora_llama",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "2_7b\n\n# Build Llama",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "2 without any LoRA layers\nbase_model = llama2_",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "7b()\n\n# The default settings for lora_llama2",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "_7b will match those for llama2_7",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "b\n# We just need to define which layers we want",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " LoRA applied to.\n# Within each self",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "-attention, we can choose from [\"q_proj\", \"",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "k_proj\", \"v_proj\", and \"output_proj",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "\"].\n# We can also set apply_lora_to_m",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "lp=True or apply_lora_to_output=True",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " to apply LoRA to other",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " linear\n# layers outside of the self-attention.\nl",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "ora_model = lora_llama",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "2_7b(lora_at",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "tn_modules=[\"q_proj\", \"",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "v_proj\"])\n\n# Print the first",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " layer's self-attention in the",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " usual Llama2 model\nprint",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "(base_model.layers[0].attn)\n# Print the",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " same for Llama2 with LoRA",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " weights\nprint(lora_model.layers[0].attn",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": ")\n```\n\nThis code will load the Llama2 model",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " and apply LoRA to the specified layers. The `l",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "ora_attn_modules` parameter is used to specify which layers",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " to apply LoRA to, and the `apply_lora",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "_to_mlp` and `apply_lora_to_output`",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " parameters can be used to apply LoRA",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " to other linear layers outside of the self",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "-attention.\n\nYou can also use the",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " `tune run` command to fine-tune the model",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " using the LoRA fine-tuning recipe in torchtune.",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " For example:\n\n```bash\ntune",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " run lora_finetune_single_device --config llama3",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "/8B_lora_single_device\n``",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "`\n\nThis will run the LoRA fine-tuning recipe on the",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " Llama3-8B-Instruct model using the default configuration",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": ". You can modify the configuration by adding command-line overrides, such",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " as:\n\n```bash\ntune run",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "complete"
          },
          "logprobs": null,
          "stop_reason": {
            "__enum__": "StopReason",
            "value": "out_of_tokens"
          }
        },
        "metrics": null
      }
    ],
    "type": "generator"
  },
  "('meta-llama/Llama-3.1-8B-Instruct', [SystemMessage(role='system', content='You are a helpful assistant'), UserMessage(role='user', content='I am attaching some documentation for Torchtune. Help me answer questions I will ask next.', context=None), CompletionMessage(role='assistant', content='', stop_reason=<StopReason.end_of_turn: 'end_of_turn'>, tool_calls=[ToolCall(call_id='<UUID>', tool_name='knowledge_search', arguments={'query': 'Torchtune documentation'})]), ToolResponseMessage(role='tool', call_id='<UUID>', tool_name='knowledge_search', content=[TextContentItem(type='text', text='knowledge_search tool found 5 chunks:\\nBEGIN of knowledge_search tool results.\\n'), TextContentItem(type='text', text='Result 1:\\nDocument_id:f4fd3\\nContent:  conversational data, :func:`~torchtune.datasets.chat_dataset` seems to be a good fit. For any\\ncustom local dataset we always need to specify ``source``, ``data_files``, and ``split`` for any dataset\\nbuilder in torchtune. For :func:`~torchtune.datasets.chat_dataset`, we additionally need to specify\\n``conversation_column`` and ``conversation_style``. Our data follows the ``\"sharegpt\"`` format, so\\nwe can specify that here. Altogether, our :func:`~torchtune.datasets.chat_dataset` call should\\nlook like so:\\n\\n.. code-block:: python\\n\\n    from torchtune.datasets import chat_dataset\\n    from torchtune.models.llama3 import llama3_tokenizer\\n\\n    tokenizer = llama3_tokenizer(\"/tmp/Meta-Llama-3-8B-Instruct/original/tokenizer.model\")\\n    ds = chat_dataset(\\n        tokenizer=tokenizer,\\n        source=\"json\",\\n        data_files=\"data/my_data.json\",\\n        split=\"train\",\\n        conversation_column=\"dialogue\",\\n        conversation_style=\"sharegpt\",\\n    )\\n\\n.. code-block:: yaml\\n\\n    # In config\\n    tokenizer:\\n      _component_: torchtune.models.llama3.llama3_tokenizer\\n      path: /tmp/Meta-Llama-3-8B-Instruct/original/tokenizer.model\\n\\n    dataset:\\n      _component_: torchtune.datasets.chat_dataset\\n      source: json\\n      data_files: data/my_data.json\\n      split: train\\n      conversation_column: dialogue\\n      conversation_style: sharegpt\\n\\n.. note::\\n    You can pass in any keyword argument for `load_dataset <https://huggingface.co/docs/datasets/v2.20.0/en/package_reference/loading_methods#datasets.load_dataset>`_ into all our\\n    Dataset classes and they will honor them. This is useful for common parameters\\n    such as specifying the data split with :code:`split` or configuration with\\n    :code:`name`\\n\\nIf you needed to add a prompt template, you would simply pass it into the tokenizer.\\nSince we\\'re fine-tuning Llama3, the tokenizer will handle all formatting for\\nus and prompt templates are optional. Other models such as Mistral\\'s :class:`~torchtune.models.mistral._tokenizer.MistralTokenizer`,\\nuse a chat template by default (:class:`~torchtune.models.mistral.MistralChatTemplate`) to format\\nall messages according to their `recommendations <https://\\n'), TextContentItem(type='text', text=\"Result 2:\\nDocument_id:cbc88\\nContent: .. _lora_finetune_label:\\n\\n============================\\nFine-Tuning Llama2 with LoRA\\n============================\\n\\nThis guide will teach you about `LoRA <https://arxiv.org/abs/2106.09685>`_, a parameter-efficient finetuning technique,\\nand show you how you can use torchtune to finetune a Llama2 model with LoRA.\\nIf you already know what LoRA is and want to get straight to running\\nyour own LoRA finetune in torchtune, you can jump to :ref:`LoRA finetuning recipe in torchtune<lora_recipe_label>`.\\n\\n.. grid:: 2\\n\\n    .. grid-item-card:: :octicon:`mortar-board;1em;` What you will learn\\n\\n      * What LoRA is and how it saves memory during finetuning\\n      * An overview of LoRA components in torchtune\\n      * How to run a LoRA finetune using torchtune\\n      * How to experiment with different LoRA configurations\\n\\n    .. grid-item-card:: :octicon:`list-unordered;1em;` Prerequisites\\n\\n      * Be familiar with :ref:`torchtune<overview_label>`\\n      * Make sure to :ref:`install torchtune<install_label>`\\n      * Make sure you have downloaded the :ref:`Llama2-7B model weights<download_llama_label>`\\n\\nWhat is LoRA?\\n-------------\\n\\n`LoRA <https://arxiv.org/abs/2106.09685>`_ is an adapter-based method for\\nparameter-efficient finetuning that adds trainable low-rank decomposition matrices to different layers of a neural network,\\nthen freezes the network's remaining parameters. LoRA is most commonly applied to\\ntransformer models, in which case it is common to add the low-rank matrices\\nto some of the linear projections in each transformer layer's self-attention.\\n\\n.. note::\\n\\n    If you're unfamiliar, check out these references for the `definition of rank <https://en.wikipedia.org/wiki/Rank_(linear_algebra)>`_\\n    and discussion of `low-rank approximations <https://en.wikipedia.org/wiki/Low-rank_approximation>`_.\\n\\nBy finetuning with LoRA (as opposed to finetuning all model parameters),\\nyou can expect to see memory savings due to a substantial reduction in the\\nnumber of parameters with gradients. When using an optimizer with momentum,\\nlike `AdamW <https://py\\n\"), TextContentItem(type='text', text='Result 3:\\nDocument_id:8892b\\nContent: ` module, which we swap\\n   out for :class:`~torchtune.modules.peft.LoRALinear` when ``use_dora=True``.\\n\\n.. _glossary_distrib:\\n\\n\\n.. TODO\\n\\n.. Distributed\\n.. -----------\\n\\n.. .. _glossary_fsdp:\\n\\n.. Fully Sharded Data Parallel (FSDP)\\n.. ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\\n.. All our ``_distributed`` recipes use `FSDP <https://pytorch.org/docs/stable/fsdp.html>`.\\n.. .. _glossary_fsdp2:\\n\\n'), TextContentItem(type='text', text='Result 4:\\nDocument_id:cbc88\\nContent: 06% of all params are trainable.\\n\\n.. note::\\n    If you are directly using the LoRA recipe (as detailed :ref:`here<lora_recipe_label>`), you need only pass the\\n    relevant checkpoint path. Loading model weights and setting trainable parameters will be taken care\\n    of in the recipe.\\n\\n\\n.. _lora_recipe_label:\\n\\nLoRA finetuning recipe in torchtune\\n-----------------------------------\\n\\nFinally, we can put it all together and finetune a model using torchtune\\'s `LoRA recipe <https://github.com/pytorch/torchtune/blob/48626d19d2108f92c749411fbd5f0ff140023a25/recipes/lora_finetune.py>`_.\\nMake sure that you have first downloaded the Llama2 weights and tokenizer by following :ref:`these instructions<download_llama_label>`.\\nYou can then run the following command to perform a LoRA finetune of Llama2-7B with two GPUs (each having VRAM of at least 16GB):\\n\\n.. code-block:: bash\\n\\n    tune run --nnodes 1 --nproc_per_node 2 lora_finetune_distributed --config llama2/7B_lora\\n\\n.. note::\\n    Make sure to point to the location of your Llama2 weights and tokenizer. This can be done\\n    either by adding :code:`checkpointer.checkpoint_files=[my_model_checkpoint_path] tokenizer_checkpoint=my_tokenizer_checkpoint_path`\\n    or by directly modifying the :code:`7B_lora.yaml` file. See our \"\":ref:`config_tutorial_label`\" recipe\\n    for more details on how you can easily clone and modify torchtune configs.\\n\\n.. note::\\n    You can modify the value of :code:`nproc_per_node` depending on (a) the number of GPUs you have available,\\n    and (b) the memory constraints of your hardware.\\n\\nThe preceding command will run a LoRA finetune with torchtune\\'s factory settings, but we may want to experiment a bit.\\nLet\\'s take a closer look at some of the :code:`lora_finetune_distributed` config.\\n\\n.. code-block:: yaml\\n\\n  # Model Arguments\\n  model:\\n    _component_: lora_llama2_7b\\n    lora_attn_modules: [\\'q_proj\\', \\'v_proj\\']\\n    lora_rank: 8\\n    lora_alpha: 16\\n  ...\\n\\nWe see that the\\n'), TextContentItem(type='text', text='Result 5:\\nDocument_id:8892b\\nContent: etune\\n:func:`torchtune.models.llama3.llama3_8b` with DoRA, you would use :func:`torchtune.models.llama3.lora_llama3_8b` with ``use_dora=True``:\\n\\n.. code-block:: bash\\n\\n  tune run lora_finetune_single_device --config llama3/8B_lora_single_device \\\\\\n  model.use_dora=True\\n\\n.. code-block:: yaml\\n\\n  model:\\n    _component_: torchtune.models.lora_llama3_8b\\n    use_dora: True\\n\\nSince DoRA extends LoRA, the parameters for :ref:`customizing LoRA <glossary_lora>` are identical. You can also quantize the base model weights like in :ref:`glossary_qlora` by using ``quantize=True`` to reap\\neven more memory savings!\\n\\n.. code-block:: bash\\n\\n  tune run lora_finetune_single_device --config llama3/8B_lora_single_device \\\\\\n  model.apply_lora_to_mlp=True \\\\\\n  model.lora_attn_modules=[\"q_proj\",\"k_proj\",\"v_proj\"] \\\\\\n  model.lora_rank=16 \\\\\\n  model.lora_alpha=32 \\\\\\n  model.use_dora=True \\\\\\n  model.quantize_base=True\\n\\n.. code-block:: yaml\\n\\n  model:\\n    _component_: torchtune.models.lora_llama3_8b\\n    apply_lora_to_mlp: True\\n    lora_attn_modules: [\"q_proj\", \"k_proj\", \"v_proj\"]\\n    lora_rank: 16\\n    lora_alpha: 32\\n    use_dora: True\\n    quantize_base: True\\n\\n\\n.. note::\\n\\n   Under the hood, we\\'ve enabled DoRA by adding the :class:`~torchtune.modules.peft.DoRALinear` module, which we swap\\n   out for :class:`~torchtune.modules.peft.LoRALinear` when ``use_dora=True``.\\n\\n.. _glossary_distrib:\\n\\n\\n.. TODO\\n\\n.. Distributed\\n.. -----------\\n\\n.. .. _glossary_fsdp:\\n\\n.. Fully Sharded Data Parallel (FSDP)\\n.. ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\\n.. All our ``_distributed`` recipes use `FSDP <https://pytorch.org/docs/stable/fsdp.html>`.\\n.. .. _glossary_fsdp2:\\n\\n'), TextContentItem(type='text', text='END of knowledge_search tool results.\\n')]), CompletionMessage(role='assistant', content='You can use the following function call to answer the user\\'s question:\\n\\n{\"type\": \"function\", \"name\": \"knowledge_search\", \"parameters\": {\"query\": \"How to fine-tune a Llama2 model with LoRA in torchtune\"}}', stop_reason=<StopReason.end_of_turn: 'end_of_turn'>, tool_calls=[]), UserMessage(role='user', content='Tell me how to use LoRA', context=None)])_[('response_format', None), ('sampling_params', SamplingParams(strategy=TopPSamplingStrategy(type='top_p', temperature=0.0001, top_p=0.9), max_tokens=0, repetition_penalty=1.0)), ('stream', True), ('tool_config', ToolConfig(tool_choice=<ToolChoice.auto: 'auto'>, tool_prompt_format=None, system_message_behavior=<SystemMessageBehavior.append: 'append'>)), ('tool_prompt_format', None), ('tools', [ToolDefinition(tool_name='knowledge_search', description='Search for information in a database.', parameters={'query': ToolParamDefinition(param_type='string', description='The query to search for. Can be a natural language sentence or keywords.', required=True, default=None)})])]": {
    "chunks": [
      {
        "event": {
          "delta": {
            "text": "",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "start"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "{\"",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "type\": \"function\", \"",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "name\": \"knowledge_search\", \"parameters\":",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " {\"query\": \"How to use LoRA\"}}",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "succeeded"
            },
            "tool_call": {
              "arguments": {
                "query": "How to use LoRA"
              },
              "call_id": "64448cc3-c11a-4bae-bdcc-e5b8d13b888f",
              "tool_name": "knowledge_search"
            },
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": {
            "__enum__": "StopReason",
            "value": "end_of_turn"
          }
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "complete"
          },
          "logprobs": null,
          "stop_reason": {
            "__enum__": "StopReason",
            "value": "end_of_turn"
          }
        },
        "metrics": null
      }
    ],
    "type": "generator"
  },
  "('meta-llama/Llama-3.1-8B-Instruct', [SystemMessage(role='system', content='You are a helpful assistant'), UserMessage(role='user', content='I am attaching some documentation for Torchtune. Help me answer questions I will ask next.', context=None), CompletionMessage(role='assistant', content='', stop_reason=<StopReason.end_of_turn: 'end_of_turn'>, tool_calls=[ToolCall(call_id='<UUID>', tool_name='knowledge_search', arguments={'query': 'Torchtune documentation'})]), ToolResponseMessage(role='tool', call_id='<UUID>', tool_name='knowledge_search', content=[TextContentItem(type='text', text='knowledge_search tool found 5 chunks:\\nBEGIN of knowledge_search tool results.\\n'), TextContentItem(type='text', text='Result 1:\\nDocument_id:f4fd3\\nContent:  conversational data, :func:`~torchtune.datasets.chat_dataset` seems to be a good fit. For any\\ncustom local dataset we always need to specify ``source``, ``data_files``, and ``split`` for any dataset\\nbuilder in torchtune. For :func:`~torchtune.datasets.chat_dataset`, we additionally need to specify\\n``conversation_column`` and ``conversation_style``. Our data follows the ``\"sharegpt\"`` format, so\\nwe can specify that here. Altogether, our :func:`~torchtune.datasets.chat_dataset` call should\\nlook like so:\\n\\n.. code-block:: python\\n\\n    from torchtune.datasets import chat_dataset\\n    from torchtune.models.llama3 import llama3_tokenizer\\n\\n    tokenizer = llama3_tokenizer(\"/tmp/Meta-Llama-3-8B-Instruct/original/tokenizer.model\")\\n    ds = chat_dataset(\\n        tokenizer=tokenizer,\\n        source=\"json\",\\n        data_files=\"data/my_data.json\",\\n        split=\"train\",\\n        conversation_column=\"dialogue\",\\n        conversation_style=\"sharegpt\",\\n    )\\n\\n.. code-block:: yaml\\n\\n    # In config\\n    tokenizer:\\n      _component_: torchtune.models.llama3.llama3_tokenizer\\n      path: /tmp/Meta-Llama-3-8B-Instruct/original/tokenizer.model\\n\\n    dataset:\\n      _component_: torchtune.datasets.chat_dataset\\n      source: json\\n      data_files: data/my_data.json\\n      split: train\\n      conversation_column: dialogue\\n      conversation_style: sharegpt\\n\\n.. note::\\n    You can pass in any keyword argument for `load_dataset <https://huggingface.co/docs/datasets/v2.20.0/en/package_reference/loading_methods#datasets.load_dataset>`_ into all our\\n    Dataset classes and they will honor them. This is useful for common parameters\\n    such as specifying the data split with :code:`split` or configuration with\\n    :code:`name`\\n\\nIf you needed to add a prompt template, you would simply pass it into the tokenizer.\\nSince we\\'re fine-tuning Llama3, the tokenizer will handle all formatting for\\nus and prompt templates are optional. Other models such as Mistral\\'s :class:`~torchtune.models.mistral._tokenizer.MistralTokenizer`,\\nuse a chat template by default (:class:`~torchtune.models.mistral.MistralChatTemplate`) to format\\nall messages according to their `recommendations <https://\\n'), TextContentItem(type='text', text=\"Result 2:\\nDocument_id:cbc88\\nContent: .. _lora_finetune_label:\\n\\n============================\\nFine-Tuning Llama2 with LoRA\\n============================\\n\\nThis guide will teach you about `LoRA <https://arxiv.org/abs/2106.09685>`_, a parameter-efficient finetuning technique,\\nand show you how you can use torchtune to finetune a Llama2 model with LoRA.\\nIf you already know what LoRA is and want to get straight to running\\nyour own LoRA finetune in torchtune, you can jump to :ref:`LoRA finetuning recipe in torchtune<lora_recipe_label>`.\\n\\n.. grid:: 2\\n\\n    .. grid-item-card:: :octicon:`mortar-board;1em;` What you will learn\\n\\n      * What LoRA is and how it saves memory during finetuning\\n      * An overview of LoRA components in torchtune\\n      * How to run a LoRA finetune using torchtune\\n      * How to experiment with different LoRA configurations\\n\\n    .. grid-item-card:: :octicon:`list-unordered;1em;` Prerequisites\\n\\n      * Be familiar with :ref:`torchtune<overview_label>`\\n      * Make sure to :ref:`install torchtune<install_label>`\\n      * Make sure you have downloaded the :ref:`Llama2-7B model weights<download_llama_label>`\\n\\nWhat is LoRA?\\n-------------\\n\\n`LoRA <https://arxiv.org/abs/2106.09685>`_ is an adapter-based method for\\nparameter-efficient finetuning that adds trainable low-rank decomposition matrices to different layers of a neural network,\\nthen freezes the network's remaining parameters. LoRA is most commonly applied to\\ntransformer models, in which case it is common to add the low-rank matrices\\nto some of the linear projections in each transformer layer's self-attention.\\n\\n.. note::\\n\\n    If you're unfamiliar, check out these references for the `definition of rank <https://en.wikipedia.org/wiki/Rank_(linear_algebra)>`_\\n    and discussion of `low-rank approximations <https://en.wikipedia.org/wiki/Low-rank_approximation>`_.\\n\\nBy finetuning with LoRA (as opposed to finetuning all model parameters),\\nyou can expect to see memory savings due to a substantial reduction in the\\nnumber of parameters with gradients. When using an optimizer with momentum,\\nlike `AdamW <https://py\\n\"), TextContentItem(type='text', text='Result 3:\\nDocument_id:8892b\\nContent: ` module, which we swap\\n   out for :class:`~torchtune.modules.peft.LoRALinear` when ``use_dora=True``.\\n\\n.. _glossary_distrib:\\n\\n\\n.. TODO\\n\\n.. Distributed\\n.. -----------\\n\\n.. .. _glossary_fsdp:\\n\\n.. Fully Sharded Data Parallel (FSDP)\\n.. ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\\n.. All our ``_distributed`` recipes use `FSDP <https://pytorch.org/docs/stable/fsdp.html>`.\\n.. .. _glossary_fsdp2:\\n\\n'), TextContentItem(type='text', text='Result 4:\\nDocument_id:cbc88\\nContent: 06% of all params are trainable.\\n\\n.. note::\\n    If you are directly using the LoRA recipe (as detailed :ref:`here<lora_recipe_label>`), you need only pass the\\n    relevant checkpoint path. Loading model weights and setting trainable parameters will be taken care\\n    of in the recipe.\\n\\n\\n.. _lora_recipe_label:\\n\\nLoRA finetuning recipe in torchtune\\n-----------------------------------\\n\\nFinally, we can put it all together and finetune a model using torchtune\\'s `LoRA recipe <https://github.com/pytorch/torchtune/blob/48626d19d2108f92c749411fbd5f0ff140023a25/recipes/lora_finetune.py>`_.\\nMake sure that you have first downloaded the Llama2 weights and tokenizer by following :ref:`these instructions<download_llama_label>`.\\nYou can then run the following command to perform a LoRA finetune of Llama2-7B with two GPUs (each having VRAM of at least 16GB):\\n\\n.. code-block:: bash\\n\\n    tune run --nnodes 1 --nproc_per_node 2 lora_finetune_distributed --config llama2/7B_lora\\n\\n.. note::\\n    Make sure to point to the location of your Llama2 weights and tokenizer. This can be done\\n    either by adding :code:`checkpointer.checkpoint_files=[my_model_checkpoint_path] tokenizer_checkpoint=my_tokenizer_checkpoint_path`\\n    or by directly modifying the :code:`7B_lora.yaml` file. See our \"\":ref:`config_tutorial_label`\" recipe\\n    for more details on how you can easily clone and modify torchtune configs.\\n\\n.. note::\\n    You can modify the value of :code:`nproc_per_node` depending on (a) the number of GPUs you have available,\\n    and (b) the memory constraints of your hardware.\\n\\nThe preceding command will run a LoRA finetune with torchtune\\'s factory settings, but we may want to experiment a bit.\\nLet\\'s take a closer look at some of the :code:`lora_finetune_distributed` config.\\n\\n.. code-block:: yaml\\n\\n  # Model Arguments\\n  model:\\n    _component_: lora_llama2_7b\\n    lora_attn_modules: [\\'q_proj\\', \\'v_proj\\']\\n    lora_rank: 8\\n    lora_alpha: 16\\n  ...\\n\\nWe see that the\\n'), TextContentItem(type='text', text='Result 5:\\nDocument_id:8892b\\nContent: etune\\n:func:`torchtune.models.llama3.llama3_8b` with DoRA, you would use :func:`torchtune.models.llama3.lora_llama3_8b` with ``use_dora=True``:\\n\\n.. code-block:: bash\\n\\n  tune run lora_finetune_single_device --config llama3/8B_lora_single_device \\\\\\n  model.use_dora=True\\n\\n.. code-block:: yaml\\n\\n  model:\\n    _component_: torchtune.models.lora_llama3_8b\\n    use_dora: True\\n\\nSince DoRA extends LoRA, the parameters for :ref:`customizing LoRA <glossary_lora>` are identical. You can also quantize the base model weights like in :ref:`glossary_qlora` by using ``quantize=True`` to reap\\neven more memory savings!\\n\\n.. code-block:: bash\\n\\n  tune run lora_finetune_single_device --config llama3/8B_lora_single_device \\\\\\n  model.apply_lora_to_mlp=True \\\\\\n  model.lora_attn_modules=[\"q_proj\",\"k_proj\",\"v_proj\"] \\\\\\n  model.lora_rank=16 \\\\\\n  model.lora_alpha=32 \\\\\\n  model.use_dora=True \\\\\\n  model.quantize_base=True\\n\\n.. code-block:: yaml\\n\\n  model:\\n    _component_: torchtune.models.lora_llama3_8b\\n    apply_lora_to_mlp: True\\n    lora_attn_modules: [\"q_proj\", \"k_proj\", \"v_proj\"]\\n    lora_rank: 16\\n    lora_alpha: 32\\n    use_dora: True\\n    quantize_base: True\\n\\n\\n.. note::\\n\\n   Under the hood, we\\'ve enabled DoRA by adding the :class:`~torchtune.modules.peft.DoRALinear` module, which we swap\\n   out for :class:`~torchtune.modules.peft.LoRALinear` when ``use_dora=True``.\\n\\n.. _glossary_distrib:\\n\\n\\n.. TODO\\n\\n.. Distributed\\n.. -----------\\n\\n.. .. _glossary_fsdp:\\n\\n.. Fully Sharded Data Parallel (FSDP)\\n.. ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\\n.. All our ``_distributed`` recipes use `FSDP <https://pytorch.org/docs/stable/fsdp.html>`.\\n.. .. _glossary_fsdp2:\\n\\n'), TextContentItem(type='text', text='END of knowledge_search tool results.\\n')])])_[('response_format', None), ('sampling_params', SamplingParams(strategy=TopPSamplingStrategy(type='top_p', temperature=0.0001, top_p=0.9), max_tokens=0, repetition_penalty=1.0)), ('stream', True), ('tool_config', ToolConfig(tool_choice=<ToolChoice.auto: 'auto'>, tool_prompt_format=None, system_message_behavior=<SystemMessageBehavior.append: 'append'>)), ('tool_prompt_format', None), ('tools', [ToolDefinition(tool_name='knowledge_search', description='Search for information in a database.', parameters={'query': ToolParamDefinition(param_type='string', description='The query to search for. Can be a natural language sentence or keywords.', required=True, default=None)})])]": {
    "chunks": [
      {
        "event": {
          "delta": {
            "text": "",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "start"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "You",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " can use the following function call to answer",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " the user's question:\n\n{\"type\": \"function\", \"",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "name\": \"knowledge_search\", \"parameters\":",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " {\"query\": \"How to fine-tune a Llama2",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " model with LoRA in torchtune\"}}",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "complete"
          },
          "logprobs": null,
          "stop_reason": {
            "__enum__": "StopReason",
            "value": "end_of_turn"
          }
        },
        "metrics": null
      }
    ],
    "type": "generator"
  },
  "('meta-llama/Llama-3.1-8B-Instruct', [SystemMessage(role='system', content='You are a helpful assistant'), UserMessage(role='user', content='I am attaching some documentation for Torchtune. Help me answer questions I will ask next.', context=None)])_[('response_format', None), ('sampling_params', SamplingParams(strategy=TopPSamplingStrategy(type='top_p', temperature=0.0001, top_p=0.9), max_tokens=0, repetition_penalty=1.0)), ('stream', True), ('tool_config', ToolConfig(tool_choice=<ToolChoice.auto: 'auto'>, tool_prompt_format=None, system_message_behavior=<SystemMessageBehavior.append: 'append'>)), ('tool_prompt_format', None), ('tools', [ToolDefinition(tool_name='knowledge_search', description='Search for information in a database.', parameters={'query': ToolParamDefinition(param_type='string', description='The query to search for. Can be a natural language sentence or keywords.', required=True, default=None)})])]": {
    "chunks": [
      {
        "event": {
          "delta": {
            "text": "",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "start"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "started"
            },
            "tool_call": "",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "in_progress"
            },
            "tool_call": "{\"type\": \"function\", \"name\": \"knowledge_search\",",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "in_progress"
            },
            "tool_call": " \"parameters\": {\"query\": \"Torcht",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "in_progress"
            },
            "tool_call": "une documentation\"}}",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "succeeded"
            },
            "tool_call": {
              "arguments": {
                "query": "Torchtune documentation"
              },
              "call_id": "1d8ffcd7-3e9a-402d-a21f-56d2024a16e7",
              "tool_name": "knowledge_search"
            },
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": {
            "__enum__": "StopReason",
            "value": "end_of_turn"
          }
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "complete"
          },
          "logprobs": null,
          "stop_reason": {
            "__enum__": "StopReason",
            "value": "end_of_turn"
          }
        },
        "metrics": null
      }
    ],
    "type": "generator"
  },
  "('meta-llama/Llama-3.1-8B-Instruct', [SystemMessage(role='system', content='You are a helpful assistant'), UserMessage(role='user', content='Instead of the standard multi-head attention, what attention type does Llama3-8B use?', context=None), CompletionMessage(role='assistant', content='', stop_reason=<StopReason.end_of_turn: 'end_of_turn'>, tool_calls=[ToolCall(call_id='<UUID>', tool_name='knowledge_search', arguments={'query': 'Llama3-8B attention type'})]), ToolResponseMessage(role='tool', call_id='<UUID>', tool_name='knowledge_search', content=[TextContentItem(type='text', text='knowledge_search tool found 5 chunks:\\nBEGIN of knowledge_search tool results.\\n'), TextContentItem(type='text', text='Result 1:\\nDocument_id:num-1\\nContent:  3 <https://llama.meta.com/llama3>`_ is a new family of models released by Meta AI that improves upon the performance of the Llama2 family\\nof models across a `range of different benchmarks <https://huggingface.co/meta-llama/Meta-Llama-3-8B#base-pretrained-models>`_.\\nCurrently there are two different sizes of Meta Llama 3: 8B and 70B. In this tutorial we will focus on the 8B size model.\\nThere are a few main changes between Llama2-7B and Llama3-8B models:\\n\\n- Llama3-8B uses `grouped-query attention <https://arxiv.org/abs/2305.13245>`_ instead of the standard multi-head attention from Llama2-7B\\n- Llama3-8B has a larger vocab size (128,256 instead of 32,000 from Llama2 models)\\n- Llama3-8B uses a different tokenizer than Llama2 models (`tiktoken <https://github.com/openai/tiktoken>`_ instead of `sentencepiece <https://github.com/google/sentencepiece>`_)\\n- Llama3-\\n'), TextContentItem(type='text', text=\"Result 2:\\nDocument_id:num-1\\nContent:  instead of 32,000 from Llama2 models)\\n- Llama3-8B uses a different tokenizer than Llama2 models (`tiktoken <https://github.com/openai/tiktoken>`_ instead of `sentencepiece <https://github.com/google/sentencepiece>`_)\\n- Llama3-8B uses a larger intermediate dimension in its MLP layers than Llama2-7B\\n- Llama3-8B uses a higher base value to calculate theta in its `rotary positional embeddings <https://arxiv.org/abs/2104.09864>`_\\n\\n|\\n\\nGetting access to Llama3-8B-Instruct\\n------------------------------------\\n\\nFor this tutorial, we will be using the instruction-tuned version of Llama3-8B. First, let's download the model from Hugging Face. You will need to follow the instructions\\non the `official Meta page <https://github.com/meta-llama/llama3/blob/main/README.md>`_ to gain access to the model.\\nNext, make sure you grab your Hugging Face token from `here <https://huggingface.co/settings/tokens>`_.\\n\\n\\n.. code-block:: bash\\n\\n    tune download meta-llama/Meta-Llama-3\\n\"), TextContentItem(type='text', text=\"Result 3:\\nDocument_id:num-0\\nContent: :`download Llama3 Instruct weights <llama3_label>`\\n\\n\\nTemplate changes from Llama2 to Llama3\\n--------------------------------------\\n\\nThe Llama2 chat model requires a specific template when prompting the pre-trained\\nmodel. Since the chat model was pretrained with this prompt template, if you want to run\\ninference on the model, you'll need to use the same template for optimal performance\\non chat data. Otherwise, the model will just perform standard text completion, which\\nmay or may not align with your intended use case.\\n\\nFrom the `official Llama2 prompt\\ntemplate guide <https://llama.meta.com/docs/model-cards-and-prompt-formats/meta-llama-2>`_\\nfor the Llama2 chat model, we can see that special tags are added:\\n\\n.. code-block:: text\\n\\n    <s>[INST] <<SYS>>\\n    You are a helpful, respectful, and honest assistant.\\n    <</SYS>>\\n\\n    Hi! I am a human. [/INST] Hello there! Nice to meet you! I'm Meta AI, your friendly AI assistant </s>\\n\\nLlama3 Instruct `overhauled <https://llama.meta.com/docs/model-cards-and-prompt-formats/meta-llama-3>`\\n\"), TextContentItem(type='text', text='Result 4:\\nDocument_id:num-0\\nContent: \\'m Meta AI, your friendly AI assistant<|eot_id|>\\n\\nThe tags are entirely different, and they are actually encoded differently than in\\nLlama2. Let\\'s walk through tokenizing an example with the Llama2 template and the\\nLlama3 template to understand how.\\n\\n.. note::\\n    The Llama3 Base model uses a `different prompt template\\n    <https://llama.meta.com/docs/model-cards-and-prompt-formats/meta-llama-3>`_ than Llama3 Instruct\\n    because it has not yet been instruct tuned and the extra special tokens are untrained. If you\\n    are running inference on the Llama3 Base model without fine-tuning we recommend the base\\n    template for optimal performance. Generally, for instruct and chat data, we recommend using\\n    Llama3 Instruct with its prompt template. The rest of this tutorial assumes you are using\\n    Llama3 Instruct.\\n\\n.. _prompt_template_vs_special_tokens:\\n\\nTokenizing prompt templates & special tokens\\n--------------------------------------------\\n\\nLet\\'s say I have a sample of a single user-assistant turn accompanied with a system\\nprompt:\\n\\n.. code-block:: python\\n\\n    sample = [\\n        {\\n            \"role\": \"system\",\\n            \"\\n'), TextContentItem(type='text', text='Result 5:\\nDocument_id:num-3\\nContent:  LoRA to Llama2 models\\n------------------------------\\n\\nWith torchtune, we can easily apply LoRA to Llama2 with a variety of different configurations.\\nLet\\'s take a look at how to construct Llama2 models in torchtune with and without LoRA.\\n\\n.. code-block:: python\\n\\n  from torchtune.models.llama2 import llama2_7b, lora_llama2_7b\\n\\n  # Build Llama2 without any LoRA layers\\n  base_model = llama2_7b()\\n\\n  # The default settings for lora_llama2_7b will match those for llama2_7b\\n  # We just need to define which layers we want LoRA applied to.\\n  # Within each self-attention, we can choose from [\"q_proj\", \"k_proj\", \"v_proj\", and \"output_proj\"].\\n  # We can also set apply_lora_to_mlp=True or apply_lora_to_output=True to apply LoRA to other linear\\n  # layers outside of the self-attention.\\n  lora_model = lora_llama2_7b(lora_attn_modules=[\"q_proj\", \"v_proj\"])\\n\\n.. note::\\n\\n    Calling :func:`lora_llama_2\\n'), TextContentItem(type='text', text='END of knowledge_search tool results.\\n')])])_[('response_format', None), ('sampling_params', SamplingParams(strategy=TopPSamplingStrategy(type='top_p', temperature=0.0001, top_p=0.9), max_tokens=0, repetition_penalty=1.0)), ('stream', True), ('tool_config', ToolConfig(tool_choice=<ToolChoice.auto: 'auto'>, tool_prompt_format=None, system_message_behavior=<SystemMessageBehavior.append: 'append'>)), ('tool_prompt_format', None), ('tools', [ToolDefinition(tool_name='insert_into_memory', description='Insert documents into memory', parameters={}), ToolDefinition(tool_name='knowledge_search', description='Search for information in a database.', parameters={'query': ToolParamDefinition(param_type='string', description='The query to search for. Can be a natural language sentence or keywords.', required=True, default=None)})])]": {
    "chunks": [
      {
        "event": {
          "delta": {
            "text": "",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "start"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "L",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "lama3-8B uses grouped-query attention instead of",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " the standard multi-head attention from Llama",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "2-7B.",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "complete"
          },
          "logprobs": null,
          "stop_reason": {
            "__enum__": "StopReason",
            "value": "end_of_turn"
          }
        },
        "metrics": null
      }
    ],
    "type": "generator"
  },
  "('meta-llama/Llama-3.1-8B-Instruct', [SystemMessage(role='system', content='You are a helpful assistant'), UserMessage(role='user', content='Instead of the standard multi-head attention, what attention type does Llama3-8B use?', context=None), CompletionMessage(role='assistant', content='', stop_reason=<StopReason.end_of_turn: 'end_of_turn'>, tool_calls=[ToolCall(call_id='<UUID>', tool_name='knowledge_search', arguments={'query': 'Llama3-8B attention type'})]), ToolResponseMessage(role='tool', call_id='<UUID>', tool_name='knowledge_search', content=[TextContentItem(type='text', text='knowledge_search tool found 5 chunks:\\nBEGIN of knowledge_search tool results.\\n'), TextContentItem(type='text', text='Result 1:\\nDocument_id:num-1\\nContent:  3 <https://llama.meta.com/llama3>`_ is a new family of models released by Meta AI that improves upon the performance of the Llama2 family\\nof models across a `range of different benchmarks <https://huggingface.co/meta-llama/Meta-Llama-3-8B#base-pretrained-models>`_.\\nCurrently there are two different sizes of Meta Llama 3: 8B and 70B. In this tutorial we will focus on the 8B size model.\\nThere are a few main changes between Llama2-7B and Llama3-8B models:\\n\\n- Llama3-8B uses `grouped-query attention <https://arxiv.org/abs/2305.13245>`_ instead of the standard multi-head attention from Llama2-7B\\n- Llama3-8B has a larger vocab size (128,256 instead of 32,000 from Llama2 models)\\n- Llama3-8B uses a different tokenizer than Llama2 models (`tiktoken <https://github.com/openai/tiktoken>`_ instead of `sentencepiece <https://github.com/google/sentencepiece>`_)\\n- Llama3-\\n'), TextContentItem(type='text', text=\"Result 2:\\nDocument_id:num-1\\nContent:  instead of 32,000 from Llama2 models)\\n- Llama3-8B uses a different tokenizer than Llama2 models (`tiktoken <https://github.com/openai/tiktoken>`_ instead of `sentencepiece <https://github.com/google/sentencepiece>`_)\\n- Llama3-8B uses a larger intermediate dimension in its MLP layers than Llama2-7B\\n- Llama3-8B uses a higher base value to calculate theta in its `rotary positional embeddings <https://arxiv.org/abs/2104.09864>`_\\n\\n|\\n\\nGetting access to Llama3-8B-Instruct\\n------------------------------------\\n\\nFor this tutorial, we will be using the instruction-tuned version of Llama3-8B. First, let's download the model from Hugging Face. You will need to follow the instructions\\non the `official Meta page <https://github.com/meta-llama/llama3/blob/main/README.md>`_ to gain access to the model.\\nNext, make sure you grab your Hugging Face token from `here <https://huggingface.co/settings/tokens>`_.\\n\\n\\n.. code-block:: bash\\n\\n    tune download meta-llama/Meta-Llama-3\\n\"), TextContentItem(type='text', text=\"Result 3:\\nDocument_id:num-0\\nContent: :`download Llama3 Instruct weights <llama3_label>`\\n\\n\\nTemplate changes from Llama2 to Llama3\\n--------------------------------------\\n\\nThe Llama2 chat model requires a specific template when prompting the pre-trained\\nmodel. Since the chat model was pretrained with this prompt template, if you want to run\\ninference on the model, you'll need to use the same template for optimal performance\\non chat data. Otherwise, the model will just perform standard text completion, which\\nmay or may not align with your intended use case.\\n\\nFrom the `official Llama2 prompt\\ntemplate guide <https://llama.meta.com/docs/model-cards-and-prompt-formats/meta-llama-2>`_\\nfor the Llama2 chat model, we can see that special tags are added:\\n\\n.. code-block:: text\\n\\n    <s>[INST] <<SYS>>\\n    You are a helpful, respectful, and honest assistant.\\n    <</SYS>>\\n\\n    Hi! I am a human. [/INST] Hello there! Nice to meet you! I'm Meta AI, your friendly AI assistant </s>\\n\\nLlama3 Instruct `overhauled <https://llama.meta.com/docs/model-cards-and-prompt-formats/meta-llama-3>`\\n\"), TextContentItem(type='text', text='Result 4:\\nDocument_id:num-0\\nContent: \\'m Meta AI, your friendly AI assistant<|eot_id|>\\n\\nThe tags are entirely different, and they are actually encoded differently than in\\nLlama2. Let\\'s walk through tokenizing an example with the Llama2 template and the\\nLlama3 template to understand how.\\n\\n.. note::\\n    The Llama3 Base model uses a `different prompt template\\n    <https://llama.meta.com/docs/model-cards-and-prompt-formats/meta-llama-3>`_ than Llama3 Instruct\\n    because it has not yet been instruct tuned and the extra special tokens are untrained. If you\\n    are running inference on the Llama3 Base model without fine-tuning we recommend the base\\n    template for optimal performance. Generally, for instruct and chat data, we recommend using\\n    Llama3 Instruct with its prompt template. The rest of this tutorial assumes you are using\\n    Llama3 Instruct.\\n\\n.. _prompt_template_vs_special_tokens:\\n\\nTokenizing prompt templates & special tokens\\n--------------------------------------------\\n\\nLet\\'s say I have a sample of a single user-assistant turn accompanied with a system\\nprompt:\\n\\n.. code-block:: python\\n\\n    sample = [\\n        {\\n            \"role\": \"system\",\\n            \"\\n'), TextContentItem(type='text', text='Result 5:\\nDocument_id:num-3\\nContent:  LoRA to Llama2 models\\n------------------------------\\n\\nWith torchtune, we can easily apply LoRA to Llama2 with a variety of different configurations.\\nLet\\'s take a look at how to construct Llama2 models in torchtune with and without LoRA.\\n\\n.. code-block:: python\\n\\n  from torchtune.models.llama2 import llama2_7b, lora_llama2_7b\\n\\n  # Build Llama2 without any LoRA layers\\n  base_model = llama2_7b()\\n\\n  # The default settings for lora_llama2_7b will match those for llama2_7b\\n  # We just need to define which layers we want LoRA applied to.\\n  # Within each self-attention, we can choose from [\"q_proj\", \"k_proj\", \"v_proj\", and \"output_proj\"].\\n  # We can also set apply_lora_to_mlp=True or apply_lora_to_output=True to apply LoRA to other linear\\n  # layers outside of the self-attention.\\n  lora_model = lora_llama2_7b(lora_attn_modules=[\"q_proj\", \"v_proj\"])\\n\\n.. note::\\n\\n    Calling :func:`lora_llama_2\\n'), TextContentItem(type='text', text='END of knowledge_search tool results.\\n')])])_[('response_format', None), ('sampling_params', SamplingParams(strategy=TopPSamplingStrategy(type='top_p', temperature=0.0001, top_p=0.9), max_tokens=0, repetition_penalty=1.0)), ('stream', True), ('tool_config', ToolConfig(tool_choice=<ToolChoice.auto: 'auto'>, tool_prompt_format=None, system_message_behavior=<SystemMessageBehavior.append: 'append'>)), ('tool_prompt_format', None), ('tools', [ToolDefinition(tool_name='knowledge_search', description='Search for information in a database.', parameters={'query': ToolParamDefinition(param_type='string', description='The query to search for. Can be a natural language sentence or keywords.', required=True, default=None)})])]": {
    "chunks": [
      {
        "event": {
          "delta": {
            "text": "",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "start"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "The",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " attention type used by Llama3-8B is grouped-query attention.",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "complete"
          },
          "logprobs": null,
          "stop_reason": {
            "__enum__": "StopReason",
            "value": "end_of_turn"
          }
        },
        "metrics": null
      }
    ],
    "type": "generator"
  },
  "('meta-llama/Llama-3.1-8B-Instruct', [SystemMessage(role='system', content='You are a helpful assistant'), UserMessage(role='user', content='Instead of the standard multi-head attention, what attention type does Llama3-8B use?', context=None)])_[('response_format', None), ('sampling_params', SamplingParams(strategy=TopPSamplingStrategy(type='top_p', temperature=0.0001, top_p=0.9), max_tokens=0, repetition_penalty=1.0)), ('stream', True), ('tool_config', ToolConfig(tool_choice=<ToolChoice.auto: 'auto'>, tool_prompt_format=None, system_message_behavior=<SystemMessageBehavior.append: 'append'>)), ('tool_prompt_format', None), ('tools', [ToolDefinition(tool_name='insert_into_memory', description='Insert documents into memory', parameters={}), ToolDefinition(tool_name='knowledge_search', description='Search for information in a database.', parameters={'query': ToolParamDefinition(param_type='string', description='The query to search for. Can be a natural language sentence or keywords.', required=True, default=None)})])]": {
    "chunks": [
      {
        "event": {
          "delta": {
            "text": "",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "start"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "{\n",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "    \"type\": \"function\",\n    \"",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "name\": \"knowledge_search\",\n    \"parameters\": {\n        \"",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "query\": \"Llama3-8B attention type\"\n   ",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " }\n}",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "succeeded"
            },
            "tool_call": {
              "arguments": {
                "query": "Llama3-8B attention type"
              },
              "call_id": "7521ed33-5f5a-4cdc-a91b-35319b704c5f",
              "tool_name": "knowledge_search"
            },
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": {
            "__enum__": "StopReason",
            "value": "end_of_turn"
          }
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "complete"
          },
          "logprobs": null,
          "stop_reason": {
            "__enum__": "StopReason",
            "value": "end_of_turn"
          }
        },
        "metrics": null
      }
    ],
    "type": "generator"
  },
  "('meta-llama/Llama-3.1-8B-Instruct', [SystemMessage(role='system', content='You are a helpful assistant'), UserMessage(role='user', content='Instead of the standard multi-head attention, what attention type does Llama3-8B use?', context=None)])_[('response_format', None), ('sampling_params', SamplingParams(strategy=TopPSamplingStrategy(type='top_p', temperature=0.0001, top_p=0.9), max_tokens=0, repetition_penalty=1.0)), ('stream', True), ('tool_config', ToolConfig(tool_choice=<ToolChoice.auto: 'auto'>, tool_prompt_format=None, system_message_behavior=<SystemMessageBehavior.append: 'append'>)), ('tool_prompt_format', None), ('tools', [ToolDefinition(tool_name='knowledge_search', description='Search for information in a database.', parameters={'query': ToolParamDefinition(param_type='string', description='The query to search for. Can be a natural language sentence or keywords.', required=True, default=None)})])]": {
    "chunks": [
      {
        "event": {
          "delta": {
            "text": "",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "start"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "started"
            },
            "tool_call": "",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "in_progress"
            },
            "tool_call": "{\"type\": \"function\", \"name\": \"knowledge_search\",",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "in_progress"
            },
            "tool_call": " \"parameters\": {\"query\": \"Llama",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "in_progress"
            },
            "tool_call": "3-8B attention type\"}}",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "succeeded"
            },
            "tool_call": {
              "arguments": {
                "query": "Llama3-8B attention type"
              },
              "call_id": "cd1db096-f717-4088-8a5d-15cfdac1661c",
              "tool_name": "knowledge_search"
            },
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": {
            "__enum__": "StopReason",
            "value": "end_of_turn"
          }
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "complete"
          },
          "logprobs": null,
          "stop_reason": {
            "__enum__": "StopReason",
            "value": "end_of_turn"
          }
        },
        "metrics": null
      }
    ],
    "type": "generator"
  },
  "('meta-llama/Llama-3.1-8B-Instruct', [SystemMessage(role='system', content='You are a helpful assistant'), UserMessage(role='user', content='Search the web and tell me who the current CEO of Meta is.', context=None), CompletionMessage(role='assistant', content='', stop_reason=<StopReason.end_of_turn: 'end_of_turn'>, tool_calls=[ToolCall(call_id='<UUID>', tool_name=<BuiltinTool.brave_search: 'brave_search'>, arguments={'query': 'current CEO of Meta'})]), ToolResponseMessage(role='tool', call_id='<UUID>', tool_name=<BuiltinTool.brave_search: 'brave_search'>, content='{\"query\": \"current CEO of Meta\", \"top_k\": [{\"title\": \"Executives - Meta\", \"url\": \"https://about.meta.com/media-gallery/executives/\", \"content\": \"Mark Zuckerberg, Founder, Chairman and Chief Executive Officer Joel Kaplan, Chief Global Affairs Officer Susan Li, Chief Financial Officer Javier Olivan, Chief Operating Officer Chris Cox, Chief Product Officer Andrew \\\\u2018Boz\\\\u2019 Bosworth, Chief Technology Officer Jennifer Newstead, Chief Legal Officer Dave Wehner, Chief Strategy Officer Will Cathcart, Head of WhatsApp Naomi Gleit, Head of Product John Hegeman, Chief Revenue Officer Adam Mosseri, Head of Instagram Erin Egan, Chief Privacy Officer, Policy Michel Protti, Chief Privacy Officer, Product Alex Schultz, Chief Marketing Officer and VP of Analytics Tom Alison, Head of Facebook Nicola Mendelsohn, Head of Global Business Group Ahmad Al-Dahle, VP and Head of GenAI at Meta Joelle Pineau, Vice President of AI Research and Head of FAIR at Meta\", \"score\": 0.8190992, \"raw_content\": null}, {\"title\": \"Mark Zuckerberg, Founder, Chairman and Chief Executive Officer - Meta\", \"url\": \"https://about.meta.com/media-gallery/executives/mark-zuckerberg/\", \"content\": \"Mark Zuckerberg, Founder, Chairman and Chief Executive Officer | Meta Meta Quest Ray-Ban Meta Meta Horizon Meta AI Meta Verified Meta Pay Meta Horizon Workrooms Meta and you Learn about our community Shop Meta Meta Quest Meta Portal Meta Horizon Mark Zuckerberg is the founder, chairman and CEO of Meta, which he originally founded as Facebook in 2004. In October 2021, Facebook rebranded to Meta to reflect all of its products and services across its family of apps and a focus on developing social experiences for the metaverse \\\\u2014 moving beyond 2D screens toward immersive experiences like augmented and virtual reality to help build the next evolution in social technology. Shop Ray-Ban Meta glassesRay-Ban StoriesPrivacy informationSupported countries \\\\u00a9 2025 Meta\", \"score\": 0.79099923, \"raw_content\": null}, {\"title\": \"Meet the Executive CSuite Team of Meta (Facebook) [2025]\", \"url\": \"https://digitaldefynd.com/IQ/meet-the-executive-csuite-team-of-meta-facebook/\", \"content\": \"Harvard University Executive Programs Free Harvard University Courses As a chief financial officer of Meta, Susan Li oversees the firm\\\\u2019s finance and facilities team to keep track of the company\\\\u2019s overall financial health. The chief operating officer of Meta, Javier Olivan, oversees the firm\\\\u2019s business team, infrastructure, and other products. Andrew Bosworth, called Boz, serves as chief technology officer at Meta and is responsible for leading the firm\\\\u2019s AR/VR organization, Reality Labs. Andrew has also served as engineering director to oversee events, mobile monetization, and feed ads and as VP of ads and business platforms to lead engineering, design, analytics, and product teams. Meta\\\\u2019s c-suite team comprises experienced and diverse executives, having extensive experience in technology, finance, legal, and all major industries.\", \"score\": 0.7602419, \"raw_content\": null}, {\"title\": \"Meta to spend up to $65 billion this year to power AI goals, Zuckerberg ...\", \"url\": \"https://www.reuters.com/technology/meta-invest-up-65-bln-capital-expenditure-this-year-2025-01-24/\", \"content\": \"Meta Platforms plans to spend as much as $65 billion this year to expand its AI infrastructure, CEO Mark Zuckerberg said on Friday, aiming to bolster the company\\'s position against rivals OpenAI\", \"score\": 0.73914057, \"raw_content\": null}, {\"title\": \"Meta - Leadership & Governance\", \"url\": \"https://investor.atmeta.com/leadership-and-governance/\", \"content\": \"Mr. Andreessen was a co-founder of Netscape Communications Corporation, a software company, serving in various positions, including Chief Technology Officer and Executive Vice President of Products. Ms. Killefer also served as Assistant Secretary for Management, Chief Financial Officer, and Chief Operating Officer of the U.S. Department of the Treasury from 1997 to 2000 and as a member of the IRS Oversight Board from 2000 to 2005, including as Chair of the IRS Oversight Board from 2002 to 2004. Ms. Travis has served as Executive Vice President and Chief Financial Officer of The Estee Lauder Companies Inc., a global manufacturer and marketer of skin care, makeup, fragrance and hair care products, since August 2012.\", \"score\": 0.6175132, \"raw_content\": null}]}')])_[('response_format', None), ('sampling_params', SamplingParams(strategy=TopPSamplingStrategy(type='top_p', temperature=0.0001, top_p=0.9), max_tokens=0, repetition_penalty=1.0)), ('stream', True), ('tool_config', ToolConfig(tool_choice=<ToolChoice.auto: 'auto'>, tool_prompt_format=None, system_message_behavior=<SystemMessageBehavior.append: 'append'>)), ('tool_prompt_format', None), ('tools', [ToolDefinition(tool_name=<BuiltinTool.brave_search: 'brave_search'>, description='Search the web for information', parameters={'query': ToolParamDefinition(param_type='string', description='The query to search for', required=True, default=None)})])]": {
    "chunks": [
      {
        "event": {
          "delta": {
            "text": "",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "start"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "The",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " current CEO of Meta is Mark Zuckerberg.",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "complete"
          },
          "logprobs": null,
          "stop_reason": {
            "__enum__": "StopReason",
            "value": "end_of_turn"
          }
        },
        "metrics": null
      }
    ],
    "type": "generator"
  },
  "('meta-llama/Llama-3.1-8B-Instruct', [SystemMessage(role='system', content='You are a helpful assistant'), UserMessage(role='user', content='Search the web and tell me who the current CEO of Meta is.', context=None), CompletionMessage(role='assistant', content='', stop_reason=<StopReason.end_of_turn: 'end_of_turn'>, tool_calls=[ToolCall(call_id='<UUID>', tool_name=<BuiltinTool.brave_search: 'brave_search'>, arguments={'query': 'current CEO of Meta'})]), ToolResponseMessage(role='tool', call_id='<UUID>', tool_name=<BuiltinTool.brave_search: 'brave_search'>, content='{\"query\": \"current CEO of Meta\", \"top_k\": [{\"title\": \"Executives - Meta\", \"url\": \"https://about.meta.com/media-gallery/executives/\", \"content\": \"Mark Zuckerberg, Founder, Chairman and Chief Executive Officer Joel Kaplan, Chief Global Affairs Officer Susan Li, Chief Financial Officer Javier Olivan, Chief Operating Officer Chris Cox, Chief Product Officer Andrew \\\\u2018Boz\\\\u2019 Bosworth, Chief Technology Officer Jennifer Newstead, Chief Legal Officer Dave Wehner, Chief Strategy Officer Will Cathcart, Head of WhatsApp Naomi Gleit, Head of Product John Hegeman, Chief Revenue Officer Adam Mosseri, Head of Instagram Erin Egan, Chief Privacy Officer, Policy Michel Protti, Chief Privacy Officer, Product Alex Schultz, Chief Marketing Officer and VP of Analytics Tom Alison, Head of Facebook Nicola Mendelsohn, Head of Global Business Group Ahmad Al-Dahle, VP and Head of GenAI at Meta Joelle Pineau, Vice President of AI Research and Head of FAIR at Meta\", \"score\": 0.8190992, \"raw_content\": null}, {\"title\": \"Mark Zuckerberg, Founder, Chairman and Chief Executive Officer - Meta\", \"url\": \"https://about.meta.com/media-gallery/executives/mark-zuckerberg/\", \"content\": \"Mark Zuckerberg, Founder, Chairman and Chief Executive Officer | Meta Meta Quest Ray-Ban Meta Meta Horizon Meta AI Meta Verified Meta Pay Meta Horizon Workrooms Meta and you Learn about our community Shop Meta Meta Quest Meta Portal Meta Horizon Mark Zuckerberg is the founder, chairman and CEO of Meta, which he originally founded as Facebook in 2004. In October 2021, Facebook rebranded to Meta to reflect all of its products and services across its family of apps and a focus on developing social experiences for the metaverse \\\\u2014 moving beyond 2D screens toward immersive experiences like augmented and virtual reality to help build the next evolution in social technology. Shop Ray-Ban Meta glassesRay-Ban StoriesPrivacy informationSupported countries \\\\u00a9 2025 Meta\", \"score\": 0.79099923, \"raw_content\": null}, {\"title\": \"Zuckerberg\\'s political pivot targets Apple, puts Meta staffers on edge\", \"url\": \"https://www.cnbc.com/2025/02/14/zuckerbergs-rightward-policy-shift-hits-meta-staffers-targets-apple.html\", \"content\": \"Meta CEO Mark Zuckerberg\\'s actions to curry favor with the president have rattled employees, but people familiar with his efforts say there\\'s a clear strategy.\", \"score\": 0.77179235, \"raw_content\": null}, {\"title\": \"Meet the Executive CSuite Team of Meta (Facebook) [2025]\", \"url\": \"https://digitaldefynd.com/IQ/meet-the-executive-csuite-team-of-meta-facebook/\", \"content\": \"Harvard University Executive Programs Free Harvard University Courses As a chief financial officer of Meta, Susan Li oversees the firm\\\\u2019s finance and facilities team to keep track of the company\\\\u2019s overall financial health. The chief operating officer of Meta, Javier Olivan, oversees the firm\\\\u2019s business team, infrastructure, and other products. Andrew Bosworth, called Boz, serves as chief technology officer at Meta and is responsible for leading the firm\\\\u2019s AR/VR organization, Reality Labs. Andrew has also served as engineering director to oversee events, mobile monetization, and feed ads and as VP of ads and business platforms to lead engineering, design, analytics, and product teams. Meta\\\\u2019s c-suite team comprises experienced and diverse executives, having extensive experience in technology, finance, legal, and all major industries.\", \"score\": 0.7602419, \"raw_content\": null}, {\"title\": \"Meta to spend up to $65 billion this year to power AI goals, Zuckerberg ...\", \"url\": \"https://www.reuters.com/technology/meta-invest-up-65-bln-capital-expenditure-this-year-2025-01-24/\", \"content\": \"Meta Platforms plans to spend as much as $65 billion this year to expand its AI infrastructure, CEO Mark Zuckerberg said on Friday, aiming to bolster the company\\'s position against rivals OpenAI\", \"score\": 0.73914057, \"raw_content\": null}]}')])_[('response_format', None), ('sampling_params', SamplingParams(strategy=TopPSamplingStrategy(type='top_p', temperature=0.0001, top_p=0.9), max_tokens=0, repetition_penalty=1.0)), ('stream', True), ('tool_config', ToolConfig(tool_choice=<ToolChoice.auto: 'auto'>, tool_prompt_format=None, system_message_behavior=<SystemMessageBehavior.append: 'append'>)), ('tool_prompt_format', None), ('tools', [ToolDefinition(tool_name=<BuiltinTool.brave_search: 'brave_search'>, description='Search the web for information', parameters={'query': ToolParamDefinition(param_type='string', description='The query to search for', required=True, default=None)})])]": {
    "chunks": [
      {
        "event": {
          "delta": {
            "text": "",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "start"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "The",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " current CEO of Meta is Mark Zuckerberg.",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "complete"
          },
          "logprobs": null,
          "stop_reason": {
            "__enum__": "StopReason",
            "value": "end_of_turn"
          }
        },
        "metrics": null
      }
    ],
    "type": "generator"
  },
  "('meta-llama/Llama-3.1-8B-Instruct', [SystemMessage(role='system', content='You are a helpful assistant'), UserMessage(role='user', content='Search the web and tell me who the current CEO of Meta is.', context=None)])_[('response_format', None), ('sampling_params', SamplingParams(strategy=TopPSamplingStrategy(type='top_p', temperature=0.0001, top_p=0.9), max_tokens=0, repetition_penalty=1.0)), ('stream', True), ('tool_config', ToolConfig(tool_choice=<ToolChoice.auto: 'auto'>, tool_prompt_format=None, system_message_behavior=<SystemMessageBehavior.append: 'append'>)), ('tool_prompt_format', None), ('tools', [ToolDefinition(tool_name=<BuiltinTool.brave_search: 'brave_search'>, description='Search the web for information', parameters={'query': ToolParamDefinition(param_type='string', description='The query to search for', required=True, default=None)})])]": {
    "chunks": [
      {
        "event": {
          "delta": {
            "text": "",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "start"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "started"
            },
            "tool_call": "",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "in_progress"
            },
            "tool_call": "brave_search.call(query=\"current CEO of Meta\")",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "succeeded"
            },
            "tool_call": {
              "arguments": {
                "query": "current CEO of Meta"
              },
              "call_id": "8d0f56f8-9051-4c76-8d76-f46ccf2b108b",
              "tool_name": {
                "__enum__": "BuiltinTool",
                "value": "brave_search"
              }
            },
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": {
            "__enum__": "StopReason",
            "value": "end_of_turn"
          }
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "complete"
          },
          "logprobs": null,
          "stop_reason": {
            "__enum__": "StopReason",
            "value": "end_of_turn"
          }
        },
        "metrics": null
      }
    ],
    "type": "generator"
  },
  "('meta-llama/Llama-3.1-8B-Instruct', [SystemMessage(role='system', content='You are a helpful assistant'), UserMessage(role='user', content='What is the boiling point of polyjuice?', context=None), CompletionMessage(role='assistant', content='', stop_reason=<StopReason.end_of_turn: 'end_of_turn'>, tool_calls=[ToolCall(call_id='<UUID>', tool_name='get_boiling_point', arguments={'liquid_name': 'polyjuice'})]), ToolResponseMessage(role='tool', call_id='<UUID>', tool_name='get_boiling_point', content='-100')])_[('response_format', None), ('sampling_params', SamplingParams(strategy=TopPSamplingStrategy(type='top_p', temperature=0.0001, top_p=0.9), max_tokens=0, repetition_penalty=1.0)), ('stream', True), ('tool_config', ToolConfig(tool_choice='get_boiling_point', tool_prompt_format=None, system_message_behavior=<SystemMessageBehavior.append: 'append'>)), ('tool_prompt_format', None), ('tools', [ToolDefinition(tool_name='get_boiling_point', description='Returns the boiling point of a liquid in Celcius or Fahrenheit', parameters={'liquid_name': ToolParamDefinition(param_type='str', description='The name of the liquid', required=True, default=None), 'celcius': ToolParamDefinition(param_type='bool', description='Whether to return the boiling point in Celcius', required=False, default=True)})])]": {
    "chunks": [
      {
        "event": {
          "delta": {
            "text": "",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "start"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "The",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " function `get_boiling_point`",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " is not able to find the boiling point of polyjuice as",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " it is a fictional liquid from the Harry Potter series.",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "complete"
          },
          "logprobs": null,
          "stop_reason": {
            "__enum__": "StopReason",
            "value": "end_of_turn"
          }
        },
        "metrics": null
      }
    ],
    "type": "generator"
  },
  "('meta-llama/Llama-3.1-8B-Instruct', [SystemMessage(role='system', content='You are a helpful assistant'), UserMessage(role='user', content='What is the boiling point of polyjuice?', context=None), CompletionMessage(role='assistant', content='', stop_reason=<StopReason.end_of_turn: 'end_of_turn'>, tool_calls=[ToolCall(call_id='<UUID>', tool_name='get_boiling_point', arguments={'liquid_name': 'polyjuice'})]), ToolResponseMessage(role='tool', call_id='<UUID>', tool_name='get_boiling_point', content='-100')])_[('response_format', None), ('sampling_params', SamplingParams(strategy=TopPSamplingStrategy(type='top_p', temperature=0.0001, top_p=0.9), max_tokens=0, repetition_penalty=1.0)), ('stream', True), ('tool_config', ToolConfig(tool_choice='get_boiling_point', tool_prompt_format=None, system_message_behavior=<SystemMessageBehavior.append: 'append'>)), ('tool_prompt_format', None), ('tools', [ToolDefinition(tool_name='get_boiling_point', description='Returns the boiling point of a liquid in Celcius or Fahrenheit', parameters={'liquid_name': ToolParamDefinition(param_type='string', description='The name of the liquid', required=True, default=None), 'celcius': ToolParamDefinition(param_type='bool', description='Whether to return the boiling point in Celcius', required=False, default=True)})])]": {
    "chunks": [
      {
        "event": {
          "delta": {
            "text": "",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "start"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "The",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " function `get_boiling_point` is not able to find the boiling point",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " of polyjuice as it is a fictional liquid from the Harry Potter series",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": ".",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "complete"
          },
          "logprobs": null,
          "stop_reason": {
            "__enum__": "StopReason",
            "value": "end_of_turn"
          }
        },
        "metrics": null
      }
    ],
    "type": "generator"
  },
  "('meta-llama/Llama-3.1-8B-Instruct', [SystemMessage(role='system', content='You are a helpful assistant'), UserMessage(role='user', content='What is the boiling point of polyjuice?', context=None), CompletionMessage(role='assistant', content='', stop_reason=<StopReason.end_of_turn: 'end_of_turn'>, tool_calls=[ToolCall(call_id='<UUID>', tool_name='get_boiling_point', arguments={'liquid_name': 'polyjuice'})]), ToolResponseMessage(role='tool', call_id='<UUID>', tool_name='get_boiling_point', content='-100')])_[('response_format', None), ('sampling_params', SamplingParams(strategy=TopPSamplingStrategy(type='top_p', temperature=0.0001, top_p=0.9), max_tokens=0, repetition_penalty=1.0)), ('stream', True), ('tool_config', ToolConfig(tool_choice=<ToolChoice.auto: 'auto'>, tool_prompt_format=None, system_message_behavior=<SystemMessageBehavior.append: 'append'>)), ('tool_prompt_format', None), ('tools', [ToolDefinition(tool_name='get_boiling_point', description='Returns the boiling point of a liquid in Celcius or Fahrenheit', parameters={'liquid_name': ToolParamDefinition(param_type='str', description='The name of the liquid', required=True, default=None), 'celcius': ToolParamDefinition(param_type='bool', description='Whether to return the boiling point in Celcius', required=False, default=True)}), ToolDefinition(tool_name=<BuiltinTool.brave_search: 'brave_search'>, description='Search the web for information', parameters={'query': ToolParamDefinition(param_type='string', description='The query to search for', required=True, default=None)})])]": {
    "chunks": [
      {
        "event": {
          "delta": {
            "text": "",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "start"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "The",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " function `get_boiling_point` is not",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " able to find the boiling point of poly",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "juice as it is not a real liquid. Polyjuice",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " is a magical potion from the Harry Potter",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " series.",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "complete"
          },
          "logprobs": null,
          "stop_reason": {
            "__enum__": "StopReason",
            "value": "end_of_turn"
          }
        },
        "metrics": null
      }
    ],
    "type": "generator"
  },
  "('meta-llama/Llama-3.1-8B-Instruct', [SystemMessage(role='system', content='You are a helpful assistant'), UserMessage(role='user', content='What is the boiling point of polyjuice?', context=None), CompletionMessage(role='assistant', content='', stop_reason=<StopReason.end_of_turn: 'end_of_turn'>, tool_calls=[ToolCall(call_id='<UUID>', tool_name='get_boiling_point', arguments={'liquid_name': 'polyjuice'})]), ToolResponseMessage(role='tool', call_id='<UUID>', tool_name='get_boiling_point', content='-100')])_[('response_format', None), ('sampling_params', SamplingParams(strategy=TopPSamplingStrategy(type='top_p', temperature=0.0001, top_p=0.9), max_tokens=0, repetition_penalty=1.0)), ('stream', True), ('tool_config', ToolConfig(tool_choice=<ToolChoice.auto: 'auto'>, tool_prompt_format=None, system_message_behavior=<SystemMessageBehavior.append: 'append'>)), ('tool_prompt_format', None), ('tools', [ToolDefinition(tool_name='get_boiling_point', description='Returns the boiling point of a liquid in Celcius or Fahrenheit', parameters={'liquid_name': ToolParamDefinition(param_type='string', description='The name of the liquid', required=True, default=None), 'celcius': ToolParamDefinition(param_type='bool', description='Whether to return the boiling point in Celcius', required=False, default=True)}), ToolDefinition(tool_name=<BuiltinTool.brave_search: 'brave_search'>, description='Search the web for information', parameters={'query': ToolParamDefinition(param_type='string', description='The query to search for', required=True, default=None)})])]": {
    "chunks": [
      {
        "event": {
          "delta": {
            "text": "",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "start"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "The",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " function `get_boiling_point` is not able to find the",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " boiling point of polyjuice as it is not a real liquid",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": ". Polyjuice is a magical potion",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " from the Harry Potter series.",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "complete"
          },
          "logprobs": null,
          "stop_reason": {
            "__enum__": "StopReason",
            "value": "end_of_turn"
          }
        },
        "metrics": null
      }
    ],
    "type": "generator"
  },
  "('meta-llama/Llama-3.1-8B-Instruct', [SystemMessage(role='system', content='You are a helpful assistant'), UserMessage(role='user', content='What is the boiling point of polyjuice?', context=None), CompletionMessage(role='assistant', content='', stop_reason=<StopReason.end_of_turn: 'end_of_turn'>, tool_calls=[ToolCall(call_id='<UUID>', tool_name='get_boiling_point', arguments={'liquid_name': 'polyjuice'})]), ToolResponseMessage(role='tool', call_id='<UUID>', tool_name='get_boiling_point', content='-100')])_[('response_format', None), ('sampling_params', SamplingParams(strategy=TopPSamplingStrategy(type='top_p', temperature=0.0001, top_p=0.9), max_tokens=0, repetition_penalty=1.0)), ('stream', True), ('tool_config', ToolConfig(tool_choice=<ToolChoice.required: 'required'>, tool_prompt_format=None, system_message_behavior=<SystemMessageBehavior.append: 'append'>)), ('tool_prompt_format', None), ('tools', [ToolDefinition(tool_name='get_boiling_point', description='Returns the boiling point of a liquid in Celcius or Fahrenheit', parameters={'liquid_name': ToolParamDefinition(param_type='str', description='The name of the liquid', required=True, default=None), 'celcius': ToolParamDefinition(param_type='bool', description='Whether to return the boiling point in Celcius', required=False, default=True)})])]": {
    "chunks": [
      {
        "event": {
          "delta": {
            "text": "",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "start"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "The",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " function `get_boiling_point` is not",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " able to find the boiling point of polyjuice as it is",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " not a real liquid. Polyjuice is",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " a magical potion from the Harry Potter series.",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "complete"
          },
          "logprobs": null,
          "stop_reason": {
            "__enum__": "StopReason",
            "value": "end_of_turn"
          }
        },
        "metrics": null
      }
    ],
    "type": "generator"
  },
  "('meta-llama/Llama-3.1-8B-Instruct', [SystemMessage(role='system', content='You are a helpful assistant'), UserMessage(role='user', content='What is the boiling point of polyjuice?', context=None), CompletionMessage(role='assistant', content='', stop_reason=<StopReason.end_of_turn: 'end_of_turn'>, tool_calls=[ToolCall(call_id='<UUID>', tool_name='get_boiling_point', arguments={'liquid_name': 'polyjuice'})]), ToolResponseMessage(role='tool', call_id='<UUID>', tool_name='get_boiling_point', content='-100')])_[('response_format', None), ('sampling_params', SamplingParams(strategy=TopPSamplingStrategy(type='top_p', temperature=0.0001, top_p=0.9), max_tokens=0, repetition_penalty=1.0)), ('stream', True), ('tool_config', ToolConfig(tool_choice=<ToolChoice.required: 'required'>, tool_prompt_format=None, system_message_behavior=<SystemMessageBehavior.append: 'append'>)), ('tool_prompt_format', None), ('tools', [ToolDefinition(tool_name='get_boiling_point', description='Returns the boiling point of a liquid in Celcius or Fahrenheit', parameters={'liquid_name': ToolParamDefinition(param_type='string', description='The name of the liquid', required=True, default=None), 'celcius': ToolParamDefinition(param_type='bool', description='Whether to return the boiling point in Celcius', required=False, default=True)})])]": {
    "chunks": [
      {
        "event": {
          "delta": {
            "text": "",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "start"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "The",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " function `get_boiling_point` is not able to find the",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " boiling point of polyjuice as it is",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " not a real liquid. Polyjuice is a magical potion from",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " the Harry Potter series.",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "complete"
          },
          "logprobs": null,
          "stop_reason": {
            "__enum__": "StopReason",
            "value": "end_of_turn"
          }
        },
        "metrics": null
      }
    ],
    "type": "generator"
  },
  "('meta-llama/Llama-3.1-8B-Instruct', [SystemMessage(role='system', content='You are a helpful assistant'), UserMessage(role='user', content='What is the boiling point of polyjuice?', context=None)])_[('response_format', None), ('sampling_params', SamplingParams(strategy=TopPSamplingStrategy(type='top_p', temperature=0.0001, top_p=0.9), max_tokens=0, repetition_penalty=1.0)), ('stream', True), ('tool_config', ToolConfig(tool_choice='get_boiling_point', tool_prompt_format=None, system_message_behavior=<SystemMessageBehavior.append: 'append'>)), ('tool_prompt_format', None), ('tools', [ToolDefinition(tool_name='get_boiling_point', description='Returns the boiling point of a liquid in Celcius or Fahrenheit', parameters={'liquid_name': ToolParamDefinition(param_type='str', description='The name of the liquid', required=True, default=None), 'celcius': ToolParamDefinition(param_type='bool', description='Whether to return the boiling point in Celcius', required=False, default=True)})])]": {
    "chunks": [
      {
        "event": {
          "delta": {
            "text": "",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "start"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "started"
            },
            "tool_call": "",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "in_progress"
            },
            "tool_call": "{\"type\": \"function\", \"name\": \"get_bo",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "in_progress"
            },
            "tool_call": "iling_point\", \"parameters\": {\"liquid",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "in_progress"
            },
            "tool_call": "_name\": \"polyjuice\"}}",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "succeeded"
            },
            "tool_call": {
              "arguments": {
                "liquid_name": "polyjuice"
              },
              "call_id": "490c45b2-2a13-4ee1-9e37-711fabdbcc88",
              "tool_name": "get_boiling_point"
            },
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": {
            "__enum__": "StopReason",
            "value": "end_of_turn"
          }
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "complete"
          },
          "logprobs": null,
          "stop_reason": {
            "__enum__": "StopReason",
            "value": "end_of_turn"
          }
        },
        "metrics": null
      }
    ],
    "type": "generator"
  },
  "('meta-llama/Llama-3.1-8B-Instruct', [SystemMessage(role='system', content='You are a helpful assistant'), UserMessage(role='user', content='What is the boiling point of polyjuice?', context=None)])_[('response_format', None), ('sampling_params', SamplingParams(strategy=TopPSamplingStrategy(type='top_p', temperature=0.0001, top_p=0.9), max_tokens=0, repetition_penalty=1.0)), ('stream', True), ('tool_config', ToolConfig(tool_choice='get_boiling_point', tool_prompt_format=None, system_message_behavior=<SystemMessageBehavior.append: 'append'>)), ('tool_prompt_format', None), ('tools', [ToolDefinition(tool_name='get_boiling_point', description='Returns the boiling point of a liquid in Celcius or Fahrenheit', parameters={'liquid_name': ToolParamDefinition(param_type='string', description='The name of the liquid', required=True, default=None), 'celcius': ToolParamDefinition(param_type='bool', description='Whether to return the boiling point in Celcius', required=False, default=True)})])]": {
    "chunks": [
      {
        "event": {
          "delta": {
            "text": "",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "start"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "started"
            },
            "tool_call": "",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "in_progress"
            },
            "tool_call": "{\"type\": \"function\", \"name\":",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "in_progress"
            },
            "tool_call": " \"get_boiling_point\", \"parameters\": {\"liquid",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "in_progress"
            },
            "tool_call": "_name\": \"polyjuice\"}}",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "succeeded"
            },
            "tool_call": {
              "arguments": {
                "liquid_name": "polyjuice"
              },
              "call_id": "7820ea73-1e96-460d-a2ed-a3db03cc9b6d",
              "tool_name": "get_boiling_point"
            },
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": {
            "__enum__": "StopReason",
            "value": "end_of_turn"
          }
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "complete"
          },
          "logprobs": null,
          "stop_reason": {
            "__enum__": "StopReason",
            "value": "end_of_turn"
          }
        },
        "metrics": null
      }
    ],
    "type": "generator"
  },
  "('meta-llama/Llama-3.1-8B-Instruct', [SystemMessage(role='system', content='You are a helpful assistant'), UserMessage(role='user', content='What is the boiling point of polyjuice?', context=None)])_[('response_format', None), ('sampling_params', SamplingParams(strategy=TopPSamplingStrategy(type='top_p', temperature=0.0001, top_p=0.9), max_tokens=0, repetition_penalty=1.0)), ('stream', True), ('tool_config', ToolConfig(tool_choice=<ToolChoice.auto: 'auto'>, tool_prompt_format=None, system_message_behavior=<SystemMessageBehavior.append: 'append'>)), ('tool_prompt_format', None), ('tools', [ToolDefinition(tool_name='get_boiling_point', description='Returns the boiling point of a liquid in Celcius or Fahrenheit', parameters={'liquid_name': ToolParamDefinition(param_type='str', description='The name of the liquid', required=True, default=None), 'celcius': ToolParamDefinition(param_type='bool', description='Whether to return the boiling point in Celcius', required=False, default=True)}), ToolDefinition(tool_name=<BuiltinTool.brave_search: 'brave_search'>, description='Search the web for information', parameters={'query': ToolParamDefinition(param_type='string', description='The query to search for', required=True, default=None)})])]": {
    "chunks": [
      {
        "event": {
          "delta": {
            "text": "",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "start"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "started"
            },
            "tool_call": "",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "in_progress"
            },
            "tool_call": "{\"type\": \"function\", \"name\": \"get",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "in_progress"
            },
            "tool_call": "_boiling_point\", \"parameters\": {\"liquid_name\": \"polyjuice",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "in_progress"
            },
            "tool_call": "\"}}",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "succeeded"
            },
            "tool_call": {
              "arguments": {
                "liquid_name": "polyjuice"
              },
              "call_id": "b5f6f475-f1ed-4916-9959-405e72ca0c1d",
              "tool_name": "get_boiling_point"
            },
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": {
            "__enum__": "StopReason",
            "value": "end_of_turn"
          }
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "complete"
          },
          "logprobs": null,
          "stop_reason": {
            "__enum__": "StopReason",
            "value": "end_of_turn"
          }
        },
        "metrics": null
      }
    ],
    "type": "generator"
  },
  "('meta-llama/Llama-3.1-8B-Instruct', [SystemMessage(role='system', content='You are a helpful assistant'), UserMessage(role='user', content='What is the boiling point of polyjuice?', context=None)])_[('response_format', None), ('sampling_params', SamplingParams(strategy=TopPSamplingStrategy(type='top_p', temperature=0.0001, top_p=0.9), max_tokens=0, repetition_penalty=1.0)), ('stream', True), ('tool_config', ToolConfig(tool_choice=<ToolChoice.auto: 'auto'>, tool_prompt_format=None, system_message_behavior=<SystemMessageBehavior.append: 'append'>)), ('tool_prompt_format', None), ('tools', [ToolDefinition(tool_name='get_boiling_point', description='Returns the boiling point of a liquid in Celcius or Fahrenheit', parameters={'liquid_name': ToolParamDefinition(param_type='string', description='The name of the liquid', required=True, default=None), 'celcius': ToolParamDefinition(param_type='bool', description='Whether to return the boiling point in Celcius', required=False, default=True)}), ToolDefinition(tool_name=<BuiltinTool.brave_search: 'brave_search'>, description='Search the web for information', parameters={'query': ToolParamDefinition(param_type='string', description='The query to search for', required=True, default=None)})])]": {
    "chunks": [
      {
        "event": {
          "delta": {
            "text": "",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "start"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "started"
            },
            "tool_call": "",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "in_progress"
            },
            "tool_call": "{\"type\": \"function\", \"name\": \"get_boiling",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "in_progress"
            },
            "tool_call": "_point\", \"parameters\": {\"liquid_name\": \"polyju",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "in_progress"
            },
            "tool_call": "ice\"}}",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "succeeded"
            },
            "tool_call": {
              "arguments": {
                "liquid_name": "polyjuice"
              },
              "call_id": "95b2fc7c-de68-4824-95bf-860d72eee69a",
              "tool_name": "get_boiling_point"
            },
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": {
            "__enum__": "StopReason",
            "value": "end_of_turn"
          }
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "complete"
          },
          "logprobs": null,
          "stop_reason": {
            "__enum__": "StopReason",
            "value": "end_of_turn"
          }
        },
        "metrics": null
      }
    ],
    "type": "generator"
  },
  "('meta-llama/Llama-3.1-8B-Instruct', [SystemMessage(role='system', content='You are a helpful assistant'), UserMessage(role='user', content='What is the boiling point of polyjuice?', context=None)])_[('response_format', None), ('sampling_params', SamplingParams(strategy=TopPSamplingStrategy(type='top_p', temperature=0.0001, top_p=0.9), max_tokens=0, repetition_penalty=1.0)), ('stream', True), ('tool_config', ToolConfig(tool_choice=<ToolChoice.none: 'none'>, tool_prompt_format=None, system_message_behavior=<SystemMessageBehavior.append: 'append'>)), ('tool_prompt_format', None), ('tools', [ToolDefinition(tool_name='get_boiling_point', description='Returns the boiling point of a liquid in Celcius or Fahrenheit', parameters={'liquid_name': ToolParamDefinition(param_type='str', description='The name of the liquid', required=True, default=None), 'celcius': ToolParamDefinition(param_type='bool', description='Whether to return the boiling point in Celcius', required=False, default=True)})])]": {
    "chunks": [
      {
        "event": {
          "delta": {
            "text": "",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "start"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "I",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " couldn't find any information on the boiling point of Polyjuice",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": ". Polyjuice is a magical potion in",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " the Harry Potter series that allows the drinker to transform into",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " someone else. It's not a physical substance with a boiling point",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": ". If you have any other questions, I'd be happy to",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " help.",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "complete"
          },
          "logprobs": null,
          "stop_reason": {
            "__enum__": "StopReason",
            "value": "end_of_turn"
          }
        },
        "metrics": null
      }
    ],
    "type": "generator"
  },
  "('meta-llama/Llama-3.1-8B-Instruct', [SystemMessage(role='system', content='You are a helpful assistant'), UserMessage(role='user', content='What is the boiling point of polyjuice?', context=None)])_[('response_format', None), ('sampling_params', SamplingParams(strategy=TopPSamplingStrategy(type='top_p', temperature=0.0001, top_p=0.9), max_tokens=0, repetition_penalty=1.0)), ('stream', True), ('tool_config', ToolConfig(tool_choice=<ToolChoice.none: 'none'>, tool_prompt_format=None, system_message_behavior=<SystemMessageBehavior.append: 'append'>)), ('tool_prompt_format', None), ('tools', [ToolDefinition(tool_name='get_boiling_point', description='Returns the boiling point of a liquid in Celcius or Fahrenheit', parameters={'liquid_name': ToolParamDefinition(param_type='string', description='The name of the liquid', required=True, default=None), 'celcius': ToolParamDefinition(param_type='bool', description='Whether to return the boiling point in Celcius', required=False, default=True)})])]": {
    "chunks": [
      {
        "event": {
          "delta": {
            "text": "",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "start"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "I",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " couldn't find any information on the boiling point",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " of Polyjuice. Polyjuice is a magical potion in the",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " Harry Potter series that allows the drinker to transform into someone else.",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " It's not a physical substance with a boiling point. If you have",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " any other questions, I'd be happy to help.",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "complete"
          },
          "logprobs": null,
          "stop_reason": {
            "__enum__": "StopReason",
            "value": "end_of_turn"
          }
        },
        "metrics": null
      }
    ],
    "type": "generator"
  },
  "('meta-llama/Llama-3.1-8B-Instruct', [SystemMessage(role='system', content='You are a helpful assistant'), UserMessage(role='user', content='What is the boiling point of polyjuice?', context=None)])_[('response_format', None), ('sampling_params', SamplingParams(strategy=TopPSamplingStrategy(type='top_p', temperature=0.0001, top_p=0.9), max_tokens=0, repetition_penalty=1.0)), ('stream', True), ('tool_config', ToolConfig(tool_choice=<ToolChoice.required: 'required'>, tool_prompt_format=None, system_message_behavior=<SystemMessageBehavior.append: 'append'>)), ('tool_prompt_format', None), ('tools', [ToolDefinition(tool_name='get_boiling_point', description='Returns the boiling point of a liquid in Celcius or Fahrenheit', parameters={'liquid_name': ToolParamDefinition(param_type='str', description='The name of the liquid', required=True, default=None), 'celcius': ToolParamDefinition(param_type='bool', description='Whether to return the boiling point in Celcius', required=False, default=True)})])]": {
    "chunks": [
      {
        "event": {
          "delta": {
            "text": "",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "start"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "started"
            },
            "tool_call": "",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "in_progress"
            },
            "tool_call": "{\"type\": \"function\", \"name\": \"",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "in_progress"
            },
            "tool_call": "get_boiling_point\", \"parameters\": {\"liquid_name\": \"",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "in_progress"
            },
            "tool_call": "polyjuice\"}}",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "succeeded"
            },
            "tool_call": {
              "arguments": {
                "liquid_name": "polyjuice"
              },
              "call_id": "3e1a2cdc-46c3-4f2f-9fca-874fdea1700c",
              "tool_name": "get_boiling_point"
            },
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": {
            "__enum__": "StopReason",
            "value": "end_of_turn"
          }
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "complete"
          },
          "logprobs": null,
          "stop_reason": {
            "__enum__": "StopReason",
            "value": "end_of_turn"
          }
        },
        "metrics": null
      }
    ],
    "type": "generator"
  },
  "('meta-llama/Llama-3.1-8B-Instruct', [SystemMessage(role='system', content='You are a helpful assistant'), UserMessage(role='user', content='What is the boiling point of polyjuice?', context=None)])_[('response_format', None), ('sampling_params', SamplingParams(strategy=TopPSamplingStrategy(type='top_p', temperature=0.0001, top_p=0.9), max_tokens=0, repetition_penalty=1.0)), ('stream', True), ('tool_config', ToolConfig(tool_choice=<ToolChoice.required: 'required'>, tool_prompt_format=None, system_message_behavior=<SystemMessageBehavior.append: 'append'>)), ('tool_prompt_format', None), ('tools', [ToolDefinition(tool_name='get_boiling_point', description='Returns the boiling point of a liquid in Celcius or Fahrenheit', parameters={'liquid_name': ToolParamDefinition(param_type='string', description='The name of the liquid', required=True, default=None), 'celcius': ToolParamDefinition(param_type='bool', description='Whether to return the boiling point in Celcius', required=False, default=True)})])]": {
    "chunks": [
      {
        "event": {
          "delta": {
            "text": "",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "start"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "started"
            },
            "tool_call": "",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "in_progress"
            },
            "tool_call": "{\"type\": \"function\", \"name\":",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "in_progress"
            },
            "tool_call": " \"get_boiling_point\", \"parameters\":",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "in_progress"
            },
            "tool_call": " {\"liquid_name\": \"polyjuice\"}}",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "succeeded"
            },
            "tool_call": {
              "arguments": {
                "liquid_name": "polyjuice"
              },
              "call_id": "9cdde944-ef35-4d66-a109-7bc4a1f3b40c",
              "tool_name": "get_boiling_point"
            },
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": {
            "__enum__": "StopReason",
            "value": "end_of_turn"
          }
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "complete"
          },
          "logprobs": null,
          "stop_reason": {
            "__enum__": "StopReason",
            "value": "end_of_turn"
          }
        },
        "metrics": null
      }
    ],
    "type": "generator"
  },
  "('meta-llama/Llama-3.1-8B-Instruct', [SystemMessage(role='system', content='You are a helpful assistant'), UserMessage(role='user', content='Write code and execute it to find the answer for: What is the 100th prime number?', context=None), CompletionMessage(role='assistant', content='', stop_reason=<StopReason.end_of_turn: 'end_of_turn'>, tool_calls=[ToolCall(call_id='<UUID>', tool_name=<BuiltinTool.code_interpreter: 'code_interpreter'>, arguments={'code': 'def is_prime(n):\\n    if n <= 1:\\n        return False\\n    if n <= 3:\\n        return True\\n    if n % 2 == 0 or n % 3 == 0:\\n        return False\\n    i = 5\\n    while i * i <= n:\\n        if n % i == 0 or n % (i + 2) == 0:\\n            return False\\n        i += 6\\n    return True\\n\\ndef get_nth_prime(n):\\n    count = 0\\n    num = 2\\n    while True:\\n        if is_prime(num):\\n            count += 1\\n            if count == n:\\n                return num\\n        num += 1\\n\\nprint(get_nth_prime(100))'})]), ToolResponseMessage(role='tool', call_id='<UUID>', tool_name=<BuiltinTool.code_interpreter: 'code_interpreter'>, content=\"completed\\n[stderr]\\nTraceback (most recent call last):\\n  line 5, in <module>\\n    from bwrap.core import main\\nModuleNotFoundError: No module named 'bwrap.core'\\n[/stderr]\")])_[('response_format', None), ('sampling_params', SamplingParams(strategy=TopPSamplingStrategy(type='top_p', temperature=0.0001, top_p=0.9), max_tokens=0, repetition_penalty=1.0)), ('stream', True), ('tool_config', ToolConfig(tool_choice=<ToolChoice.auto: 'auto'>, tool_prompt_format=None, system_message_behavior=<SystemMessageBehavior.append: 'append'>)), ('tool_prompt_format', None), ('tools', [ToolDefinition(tool_name=<BuiltinTool.code_interpreter: 'code_interpreter'>, description='Execute code', parameters={'code': ToolParamDefinition(param_type='string', description='The code to execute', required=True, default=None)})])]": {
    "chunks": [
      {
        "event": {
          "delta": {
            "text": "",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "start"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "The",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " 100th prime number is ",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "541.",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "complete"
          },
          "logprobs": null,
          "stop_reason": {
            "__enum__": "StopReason",
            "value": "end_of_turn"
          }
        },
        "metrics": null
      }
    ],
    "type": "generator"
  },
  "('meta-llama/Llama-3.1-8B-Instruct', [SystemMessage(role='system', content='You are a helpful assistant'), UserMessage(role='user', content='Write code and execute it to find the answer for: What is the 100th prime number?', context=None)])_[('response_format', None), ('sampling_params', SamplingParams(strategy=TopPSamplingStrategy(type='top_p', temperature=0.0001, top_p=0.9), max_tokens=0, repetition_penalty=1.0)), ('stream', True), ('tool_config', ToolConfig(tool_choice=<ToolChoice.auto: 'auto'>, tool_prompt_format=None, system_message_behavior=<SystemMessageBehavior.append: 'append'>)), ('tool_prompt_format', None), ('tools', [ToolDefinition(tool_name=<BuiltinTool.code_interpreter: 'code_interpreter'>, description='Execute code', parameters={'code': ToolParamDefinition(param_type='string', description='The code to execute', required=True, default=None)})])]": {
    "chunks": [
      {
        "event": {
          "delta": {
            "text": "",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "start"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "started"
            },
            "tool_call": "",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "in_progress"
            },
            "tool_call": "def is_prime(n):\n    if n",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "in_progress"
            },
            "tool_call": " <= 1:\n        return False\n    if",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "in_progress"
            },
            "tool_call": " n <= 3:\n        return True\n    if n",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "in_progress"
            },
            "tool_call": " % 2 == 0 or n % 3 == ",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "in_progress"
            },
            "tool_call": "0:\n        return False\n    i =",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "in_progress"
            },
            "tool_call": " 5\n    while i * i <= n:\n        if",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "in_progress"
            },
            "tool_call": " n % i == 0 or n % (i + ",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "in_progress"
            },
            "tool_call": "2) == 0:\n            return False\n        i +=",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "in_progress"
            },
            "tool_call": " 6\n    return True\n\ndef get_nth_prime(n):\n",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "in_progress"
            },
            "tool_call": "    count = 0\n    num = 2\n   ",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "in_progress"
            },
            "tool_call": " while True:\n        if is_prime(num):\n            count += ",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "in_progress"
            },
            "tool_call": "1\n            if count == n:\n                return num\n       ",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "in_progress"
            },
            "tool_call": " num += 1\n\nprint(get_nth_prime(100))",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "succeeded"
            },
            "tool_call": {
              "arguments": {
                "code": "def is_prime(n):\n    if n <= 1:\n        return False\n    if n <= 3:\n        return True\n    if n % 2 == 0 or n % 3 == 0:\n        return False\n    i = 5\n    while i * i <= n:\n        if n % i == 0 or n % (i + 2) == 0:\n            return False\n        i += 6\n    return True\n\ndef get_nth_prime(n):\n    count = 0\n    num = 2\n    while True:\n        if is_prime(num):\n            count += 1\n            if count == n:\n                return num\n        num += 1\n\nprint(get_nth_prime(100))"
              },
              "call_id": "2bc94f5e-77a2-4db6-ad05-44597eee1dc3",
              "tool_name": {
                "__enum__": "BuiltinTool",
                "value": "code_interpreter"
              }
            },
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": {
            "__enum__": "StopReason",
            "value": "end_of_turn"
          }
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "complete"
          },
          "logprobs": null,
          "stop_reason": {
            "__enum__": "StopReason",
            "value": "end_of_turn"
          }
        },
        "metrics": null
      }
    ],
    "type": "generator"
  },
  "('meta-llama/Llama-3.1-8B-Instruct', [SystemMessage(role='system', content='You are a helpful assistant'), UserMessage(role='user', content='when was Perplexity the company founded?', context=None), CompletionMessage(role='assistant', content='', stop_reason=<StopReason.end_of_turn: 'end_of_turn'>, tool_calls=[ToolCall(call_id='<UUID>', tool_name='knowledge_search', arguments={'query': 'Perplexity company founding date'})]), ToolResponseMessage(role='tool', call_id='<UUID>', tool_name='knowledge_search', content=[TextContentItem(type='text', text='knowledge_search tool found 3 chunks:\\nBEGIN of knowledge_search tool results.\\n'), TextContentItem(type='text', text='Result 1:\\nDocument_id:perpl\\nContent: Perplexity the company was founded in 2022 by Aravind Srinivas, Andy Konwinski, Denis Yarats and Johnny Ho, engineers with backgrounds in back-end systems, artificial intelligence (AI) and machine learning:\\n\\n    Srinivas, the CEO, worked at OpenAI as an AI researcher.\\n    Konwinski was among the founding team at Databricks.\\n    Yarats, the CTO, was an AI research scientist at Meta.\\n    Ho, the CSO, worked as an engineer at Quora, then as a quantitative trader on Wall Street.[5]\\n'), TextContentItem(type='text', text='Result 2:\\nDocument_id:perpl\\nContent:  Ho, the CSO, worked as an engineer at Quora, then as a quantitative trader on Wall Street.[5]\\n'), TextContentItem(type='text', text='Result 3:\\nDocument_id:nba_w\\nContent: The NBA was created on August 3, 1949, with the merger of the Basketball Association of America (BAA) and the National Basketball League (NBL).\\n'), TextContentItem(type='text', text='END of knowledge_search tool results.\\n')]), CompletionMessage(role='assistant', content='', stop_reason=<StopReason.end_of_turn: 'end_of_turn'>, tool_calls=[ToolCall(call_id='<UUID>', tool_name='knowledge_search', arguments={'query': 'Perplexity company founding date'})]), ToolResponseMessage(role='tool', call_id='<UUID>', tool_name='knowledge_search', content=[TextContentItem(type='text', text='knowledge_search tool found 3 chunks:\\nBEGIN of knowledge_search tool results.\\n'), TextContentItem(type='text', text='Result 1:\\nDocument_id:perpl\\nContent: Perplexity the company was founded in 2022 by Aravind Srinivas, Andy Konwinski, Denis Yarats and Johnny Ho, engineers with backgrounds in back-end systems, artificial intelligence (AI) and machine learning:\\n\\n    Srinivas, the CEO, worked at OpenAI as an AI researcher.\\n    Konwinski was among the founding team at Databricks.\\n    Yarats, the CTO, was an AI research scientist at Meta.\\n    Ho, the CSO, worked as an engineer at Quora, then as a quantitative trader on Wall Street.[5]\\n'), TextContentItem(type='text', text='Result 2:\\nDocument_id:perpl\\nContent:  Ho, the CSO, worked as an engineer at Quora, then as a quantitative trader on Wall Street.[5]\\n'), TextContentItem(type='text', text='Result 3:\\nDocument_id:nba_w\\nContent: The NBA was created on August 3, 1949, with the merger of the Basketball Association of America (BAA) and the National Basketball League (NBL).\\n'), TextContentItem(type='text', text='END of knowledge_search tool results.\\n')])])_[('response_format', None), ('sampling_params', SamplingParams(strategy=TopPSamplingStrategy(type='top_p', temperature=0.0001, top_p=0.9), max_tokens=0, repetition_penalty=1.0)), ('stream', True), ('tool_config', ToolConfig(tool_choice=<ToolChoice.auto: 'auto'>, tool_prompt_format=None, system_message_behavior=<SystemMessageBehavior.append: 'append'>)), ('tool_prompt_format', None), ('tools', [ToolDefinition(tool_name='knowledge_search', description='Search for information in a database.', parameters={'query': ToolParamDefinition(param_type='string', description='The query to search for. Can be a natural language sentence or keywords.', required=True, default=None)}), ToolDefinition(tool_name=<BuiltinTool.code_interpreter: 'code_interpreter'>, description='Execute code', parameters={'code': ToolParamDefinition(param_type='string', description='The code to execute', required=True, default=None)})])]": {
    "chunks": [
      {
        "event": {
          "delta": {
            "text": "",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "start"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "Per",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "plexity the company was founded in 2022.",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "complete"
          },
          "logprobs": null,
          "stop_reason": {
            "__enum__": "StopReason",
            "value": "end_of_turn"
          }
        },
        "metrics": null
      }
    ],
    "type": "generator"
  },
  "('meta-llama/Llama-3.1-8B-Instruct', [SystemMessage(role='system', content='You are a helpful assistant'), UserMessage(role='user', content='when was Perplexity the company founded?', context=None), CompletionMessage(role='assistant', content='', stop_reason=<StopReason.end_of_turn: 'end_of_turn'>, tool_calls=[ToolCall(call_id='<UUID>', tool_name='knowledge_search', arguments={'query': 'Perplexity company founding date'})]), ToolResponseMessage(role='tool', call_id='<UUID>', tool_name='knowledge_search', content=[TextContentItem(type='text', text='knowledge_search tool found 3 chunks:\\nBEGIN of knowledge_search tool results.\\n'), TextContentItem(type='text', text='Result 1:\\nDocument_id:perpl\\nContent: Perplexity the company was founded in 2022 by Aravind Srinivas, Andy Konwinski, Denis Yarats and Johnny Ho, engineers with backgrounds in back-end systems, artificial intelligence (AI) and machine learning:\\n\\n    Srinivas, the CEO, worked at OpenAI as an AI researcher.\\n    Konwinski was among the founding team at Databricks.\\n    Yarats, the CTO, was an AI research scientist at Meta.\\n    Ho, the CSO, worked as an engineer at Quora, then as a quantitative trader on Wall Street.[5]\\n'), TextContentItem(type='text', text='Result 2:\\nDocument_id:perpl\\nContent:  Ho, the CSO, worked as an engineer at Quora, then as a quantitative trader on Wall Street.[5]\\n'), TextContentItem(type='text', text='Result 3:\\nDocument_id:nba_w\\nContent: The NBA was created on August 3, 1949, with the merger of the Basketball Association of America (BAA) and the National Basketball League (NBL).\\n'), TextContentItem(type='text', text='END of knowledge_search tool results.\\n')]), CompletionMessage(role='assistant', content='', stop_reason=<StopReason.end_of_turn: 'end_of_turn'>, tool_calls=[ToolCall(call_id='<UUID>', tool_name='knowledge_search', arguments={'query': 'Perplexity company founding date'})]), ToolResponseMessage(role='tool', call_id='<UUID>', tool_name='knowledge_search', content=[TextContentItem(type='text', text='knowledge_search tool found 3 chunks:\\nBEGIN of knowledge_search tool results.\\n'), TextContentItem(type='text', text='Result 1:\\nDocument_id:perpl\\nContent: Perplexity the company was founded in 2022 by Aravind Srinivas, Andy Konwinski, Denis Yarats and Johnny Ho, engineers with backgrounds in back-end systems, artificial intelligence (AI) and machine learning:\\n\\n    Srinivas, the CEO, worked at OpenAI as an AI researcher.\\n    Konwinski was among the founding team at Databricks.\\n    Yarats, the CTO, was an AI research scientist at Meta.\\n    Ho, the CSO, worked as an engineer at Quora, then as a quantitative trader on Wall Street.[5]\\n'), TextContentItem(type='text', text='Result 2:\\nDocument_id:perpl\\nContent:  Ho, the CSO, worked as an engineer at Quora, then as a quantitative trader on Wall Street.[5]\\n'), TextContentItem(type='text', text='Result 3:\\nDocument_id:nba_w\\nContent: The NBA was created on August 3, 1949, with the merger of the Basketball Association of America (BAA) and the National Basketball League (NBL).\\n'), TextContentItem(type='text', text='END of knowledge_search tool results.\\n')])])_[('response_format', None), ('sampling_params', SamplingParams(strategy=TopPSamplingStrategy(type='top_p', temperature=0.0001, top_p=0.9), max_tokens=0, repetition_penalty=1.0)), ('stream', True), ('tool_config', ToolConfig(tool_choice=<ToolChoice.auto: 'auto'>, tool_prompt_format=None, system_message_behavior=<SystemMessageBehavior.append: 'append'>)), ('tool_prompt_format', None), ('tools', [ToolDefinition(tool_name=<BuiltinTool.code_interpreter: 'code_interpreter'>, description='Execute code', parameters={'code': ToolParamDefinition(param_type='string', description='The code to execute', required=True, default=None)}), ToolDefinition(tool_name='knowledge_search', description='Search for information in a database.', parameters={'query': ToolParamDefinition(param_type='string', description='The query to search for. Can be a natural language sentence or keywords.', required=True, default=None)})])]": {
    "chunks": [
      {
        "event": {
          "delta": {
            "text": "",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "start"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "Per",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "plexity the company was founded in 2022.",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "complete"
          },
          "logprobs": null,
          "stop_reason": {
            "__enum__": "StopReason",
            "value": "end_of_turn"
          }
        },
        "metrics": null
      }
    ],
    "type": "generator"
  },
  "('meta-llama/Llama-3.1-8B-Instruct', [SystemMessage(role='system', content='You are a helpful assistant'), UserMessage(role='user', content='when was Perplexity the company founded?', context=None), CompletionMessage(role='assistant', content='', stop_reason=<StopReason.end_of_turn: 'end_of_turn'>, tool_calls=[ToolCall(call_id='<UUID>', tool_name='knowledge_search', arguments={'query': 'Perplexity company founding date'})]), ToolResponseMessage(role='tool', call_id='<UUID>', tool_name='knowledge_search', content=[TextContentItem(type='text', text='knowledge_search tool found 3 chunks:\\nBEGIN of knowledge_search tool results.\\n'), TextContentItem(type='text', text='Result 1:\\nDocument_id:perpl\\nContent: Perplexity the company was founded in 2022 by Aravind Srinivas, Andy Konwinski, Denis Yarats and Johnny Ho, engineers with backgrounds in back-end systems, artificial intelligence (AI) and machine learning:\\n\\n    Srinivas, the CEO, worked at OpenAI as an AI researcher.\\n    Konwinski was among the founding team at Databricks.\\n    Yarats, the CTO, was an AI research scientist at Meta.\\n    Ho, the CSO, worked as an engineer at Quora, then as a quantitative trader on Wall Street.[5]\\n'), TextContentItem(type='text', text='Result 2:\\nDocument_id:perpl\\nContent:  Ho, the CSO, worked as an engineer at Quora, then as a quantitative trader on Wall Street.[5]\\n'), TextContentItem(type='text', text='Result 3:\\nDocument_id:nba_w\\nContent: The NBA was created on August 3, 1949, with the merger of the Basketball Association of America (BAA) and the National Basketball League (NBL).\\n'), TextContentItem(type='text', text='END of knowledge_search tool results.\\n')])])_[('response_format', None), ('sampling_params', SamplingParams(strategy=TopPSamplingStrategy(type='top_p', temperature=0.0001, top_p=0.9), max_tokens=0, repetition_penalty=1.0)), ('stream', True), ('tool_config', ToolConfig(tool_choice=<ToolChoice.auto: 'auto'>, tool_prompt_format=None, system_message_behavior=<SystemMessageBehavior.append: 'append'>)), ('tool_prompt_format', None), ('tools', [ToolDefinition(tool_name='knowledge_search', description='Search for information in a database.', parameters={'query': ToolParamDefinition(param_type='string', description='The query to search for. Can be a natural language sentence or keywords.', required=True, default=None)}), ToolDefinition(tool_name=<BuiltinTool.code_interpreter: 'code_interpreter'>, description='Execute code', parameters={'code': ToolParamDefinition(param_type='string', description='The code to execute', required=True, default=None)})])]": {
    "chunks": [
      {
        "event": {
          "delta": {
            "text": "",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "start"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "{\"",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "type\": \"function\", \"name",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "\": \"knowledge_search\", \"parameters\": {\"query",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "\": \"Perplexity company founding date\"}}",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "succeeded"
            },
            "tool_call": {
              "arguments": {
                "query": "Perplexity company founding date"
              },
              "call_id": "01fde956-fea7-4462-89e7-1fa42735d393",
              "tool_name": "knowledge_search"
            },
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": {
            "__enum__": "StopReason",
            "value": "end_of_turn"
          }
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "complete"
          },
          "logprobs": null,
          "stop_reason": {
            "__enum__": "StopReason",
            "value": "end_of_turn"
          }
        },
        "metrics": null
      }
    ],
    "type": "generator"
  },
  "('meta-llama/Llama-3.1-8B-Instruct', [SystemMessage(role='system', content='You are a helpful assistant'), UserMessage(role='user', content='when was Perplexity the company founded?', context=None), CompletionMessage(role='assistant', content='', stop_reason=<StopReason.end_of_turn: 'end_of_turn'>, tool_calls=[ToolCall(call_id='<UUID>', tool_name='knowledge_search', arguments={'query': 'Perplexity company founding date'})]), ToolResponseMessage(role='tool', call_id='<UUID>', tool_name='knowledge_search', content=[TextContentItem(type='text', text='knowledge_search tool found 3 chunks:\\nBEGIN of knowledge_search tool results.\\n'), TextContentItem(type='text', text='Result 1:\\nDocument_id:perpl\\nContent: Perplexity the company was founded in 2022 by Aravind Srinivas, Andy Konwinski, Denis Yarats and Johnny Ho, engineers with backgrounds in back-end systems, artificial intelligence (AI) and machine learning:\\n\\n    Srinivas, the CEO, worked at OpenAI as an AI researcher.\\n    Konwinski was among the founding team at Databricks.\\n    Yarats, the CTO, was an AI research scientist at Meta.\\n    Ho, the CSO, worked as an engineer at Quora, then as a quantitative trader on Wall Street.[5]\\n'), TextContentItem(type='text', text='Result 2:\\nDocument_id:perpl\\nContent:  Ho, the CSO, worked as an engineer at Quora, then as a quantitative trader on Wall Street.[5]\\n'), TextContentItem(type='text', text='Result 3:\\nDocument_id:nba_w\\nContent: The NBA was created on August 3, 1949, with the merger of the Basketball Association of America (BAA) and the National Basketball League (NBL).\\n'), TextContentItem(type='text', text='END of knowledge_search tool results.\\n')])])_[('response_format', None), ('sampling_params', SamplingParams(strategy=TopPSamplingStrategy(type='top_p', temperature=0.0001, top_p=0.9), max_tokens=0, repetition_penalty=1.0)), ('stream', True), ('tool_config', ToolConfig(tool_choice=<ToolChoice.auto: 'auto'>, tool_prompt_format=None, system_message_behavior=<SystemMessageBehavior.append: 'append'>)), ('tool_prompt_format', None), ('tools', [ToolDefinition(tool_name=<BuiltinTool.code_interpreter: 'code_interpreter'>, description='Execute code', parameters={'code': ToolParamDefinition(param_type='string', description='The code to execute', required=True, default=None)}), ToolDefinition(tool_name='knowledge_search', description='Search for information in a database.', parameters={'query': ToolParamDefinition(param_type='string', description='The query to search for. Can be a natural language sentence or keywords.', required=True, default=None)})])]": {
    "chunks": [
      {
        "event": {
          "delta": {
            "text": "",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "start"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "{\"",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "type\": \"function\", \"name\": \"knowledge_search\",",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " \"parameters\": {\"query\":",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " \"Perplexity company founding date\"}}",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "succeeded"
            },
            "tool_call": {
              "arguments": {
                "query": "Perplexity company founding date"
              },
              "call_id": "e4a5ff1d-ac00-4e0a-b93b-17e19fa3bc55",
              "tool_name": "knowledge_search"
            },
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": {
            "__enum__": "StopReason",
            "value": "end_of_turn"
          }
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "complete"
          },
          "logprobs": null,
          "stop_reason": {
            "__enum__": "StopReason",
            "value": "end_of_turn"
          }
        },
        "metrics": null
      }
    ],
    "type": "generator"
  },
  "('meta-llama/Llama-3.1-8B-Instruct', [SystemMessage(role='system', content='You are a helpful assistant'), UserMessage(role='user', content='when was Perplexity the company founded?', context=None)])_[('response_format', None), ('sampling_params', SamplingParams(strategy=TopPSamplingStrategy(type='top_p', temperature=0.0001, top_p=0.9), max_tokens=0, repetition_penalty=1.0)), ('stream', True), ('tool_config', ToolConfig(tool_choice=<ToolChoice.auto: 'auto'>, tool_prompt_format=None, system_message_behavior=<SystemMessageBehavior.append: 'append'>)), ('tool_prompt_format', None), ('tools', [ToolDefinition(tool_name='knowledge_search', description='Search for information in a database.', parameters={'query': ToolParamDefinition(param_type='string', description='The query to search for. Can be a natural language sentence or keywords.', required=True, default=None)}), ToolDefinition(tool_name=<BuiltinTool.code_interpreter: 'code_interpreter'>, description='Execute code', parameters={'code': ToolParamDefinition(param_type='string', description='The code to execute', required=True, default=None)})])]": {
    "chunks": [
      {
        "event": {
          "delta": {
            "text": "",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "start"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "started"
            },
            "tool_call": "",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "in_progress"
            },
            "tool_call": "{\"type\": \"function\", \"name\": \"knowledge_search\",",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "in_progress"
            },
            "tool_call": " \"parameters\": {\"query\": \"Perplexity company",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "in_progress"
            },
            "tool_call": " founding date\"}}",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "succeeded"
            },
            "tool_call": {
              "arguments": {
                "query": "Perplexity company founding date"
              },
              "call_id": "bb3d8ff1-e17d-4bd5-9863-d809112093eb",
              "tool_name": "knowledge_search"
            },
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": {
            "__enum__": "StopReason",
            "value": "end_of_turn"
          }
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "complete"
          },
          "logprobs": null,
          "stop_reason": {
            "__enum__": "StopReason",
            "value": "end_of_turn"
          }
        },
        "metrics": null
      }
    ],
    "type": "generator"
  },
  "('meta-llama/Llama-3.1-8B-Instruct', [SystemMessage(role='system', content='You are a helpful assistant'), UserMessage(role='user', content='when was Perplexity the company founded?', context=None)])_[('response_format', None), ('sampling_params', SamplingParams(strategy=TopPSamplingStrategy(type='top_p', temperature=0.0001, top_p=0.9), max_tokens=0, repetition_penalty=1.0)), ('stream', True), ('tool_config', ToolConfig(tool_choice=<ToolChoice.auto: 'auto'>, tool_prompt_format=None, system_message_behavior=<SystemMessageBehavior.append: 'append'>)), ('tool_prompt_format', None), ('tools', [ToolDefinition(tool_name=<BuiltinTool.code_interpreter: 'code_interpreter'>, description='Execute code', parameters={'code': ToolParamDefinition(param_type='string', description='The code to execute', required=True, default=None)}), ToolDefinition(tool_name='knowledge_search', description='Search for information in a database.', parameters={'query': ToolParamDefinition(param_type='string', description='The query to search for. Can be a natural language sentence or keywords.', required=True, default=None)})])]": {
    "chunks": [
      {
        "event": {
          "delta": {
            "text": "",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "start"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "started"
            },
            "tool_call": "",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "in_progress"
            },
            "tool_call": "{\"type\": \"function\", \"name\": \"knowledge_search\",",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "in_progress"
            },
            "tool_call": " \"parameters\": {\"query\": \"Perplexity company founding",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "in_progress"
            },
            "tool_call": " date\"}}",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "succeeded"
            },
            "tool_call": {
              "arguments": {
                "query": "Perplexity company founding date"
              },
              "call_id": "6add8292-f388-4ec5-8ec5-5071c9397492",
              "tool_name": "knowledge_search"
            },
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": {
            "__enum__": "StopReason",
            "value": "end_of_turn"
          }
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "complete"
          },
          "logprobs": null,
          "stop_reason": {
            "__enum__": "StopReason",
            "value": "end_of_turn"
          }
        },
        "metrics": null
      }
    ],
    "type": "generator"
  },
  "('meta-llama/Llama-3.1-8B-Instruct', [SystemMessage(role='system', content='You are a helpful assistant'), UserMessage(role='user', content='when was the nba created?', context=None), CompletionMessage(role='assistant', content='', stop_reason=<StopReason.end_of_turn: 'end_of_turn'>, tool_calls=[ToolCall(call_id='<UUID>', tool_name='knowledge_search', arguments={'query': 'NBA creation date'})]), ToolResponseMessage(role='tool', call_id='<UUID>', tool_name='knowledge_search', content=[TextContentItem(type='text', text='knowledge_search tool found 3 chunks:\\nBEGIN of knowledge_search tool results.\\n'), TextContentItem(type='text', text='Result 1:\\nDocument_id:nba_w\\nContent: The NBA was created on August 3, 1949, with the merger of the Basketball Association of America (BAA) and the National Basketball League (NBL).\\n'), TextContentItem(type='text', text='Result 2:\\nDocument_id:perpl\\nContent: Perplexity the company was founded in 2022 by Aravind Srinivas, Andy Konwinski, Denis Yarats and Johnny Ho, engineers with backgrounds in back-end systems, artificial intelligence (AI) and machine learning:\\n\\n    Srinivas, the CEO, worked at OpenAI as an AI researcher.\\n    Konwinski was among the founding team at Databricks.\\n    Yarats, the CTO, was an AI research scientist at Meta.\\n    Ho, the CSO, worked as an engineer at Quora, then as a quantitative trader on Wall Street.[5]\\n'), TextContentItem(type='text', text='Result 3:\\nDocument_id:perpl\\nContent:  Ho, the CSO, worked as an engineer at Quora, then as a quantitative trader on Wall Street.[5]\\n'), TextContentItem(type='text', text='END of knowledge_search tool results.\\n')])])_[('response_format', None), ('sampling_params', SamplingParams(strategy=TopPSamplingStrategy(type='top_p', temperature=0.0001, top_p=0.9), max_tokens=0, repetition_penalty=1.0)), ('stream', True), ('tool_config', ToolConfig(tool_choice=<ToolChoice.auto: 'auto'>, tool_prompt_format=None, system_message_behavior=<SystemMessageBehavior.append: 'append'>)), ('tool_prompt_format', None), ('tools', [ToolDefinition(tool_name='knowledge_search', description='Search for information in a database.', parameters={'query': ToolParamDefinition(param_type='string', description='The query to search for. Can be a natural language sentence or keywords.', required=True, default=None)}), ToolDefinition(tool_name=<BuiltinTool.code_interpreter: 'code_interpreter'>, description='Execute code', parameters={'code': ToolParamDefinition(param_type='string', description='The code to execute', required=True, default=None)})])]": {
    "chunks": [
      {
        "event": {
          "delta": {
            "text": "",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "start"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "The",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " NBA was created on August 3, 1949, with",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " the merger of the Basketball Association of America (BAA) and",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " the National Basketball League (NBL).",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "complete"
          },
          "logprobs": null,
          "stop_reason": {
            "__enum__": "StopReason",
            "value": "end_of_turn"
          }
        },
        "metrics": null
      }
    ],
    "type": "generator"
  },
  "('meta-llama/Llama-3.1-8B-Instruct', [SystemMessage(role='system', content='You are a helpful assistant'), UserMessage(role='user', content='when was the nba created?', context=None), CompletionMessage(role='assistant', content='', stop_reason=<StopReason.end_of_turn: 'end_of_turn'>, tool_calls=[ToolCall(call_id='<UUID>', tool_name='knowledge_search', arguments={'query': 'NBA creation date'})]), ToolResponseMessage(role='tool', call_id='<UUID>', tool_name='knowledge_search', content=[TextContentItem(type='text', text='knowledge_search tool found 3 chunks:\\nBEGIN of knowledge_search tool results.\\n'), TextContentItem(type='text', text='Result 1:\\nDocument_id:nba_w\\nContent: The NBA was created on August 3, 1949, with the merger of the Basketball Association of America (BAA) and the National Basketball League (NBL).\\n'), TextContentItem(type='text', text='Result 2:\\nDocument_id:perpl\\nContent: Perplexity the company was founded in 2022 by Aravind Srinivas, Andy Konwinski, Denis Yarats and Johnny Ho, engineers with backgrounds in back-end systems, artificial intelligence (AI) and machine learning:\\n\\n    Srinivas, the CEO, worked at OpenAI as an AI researcher.\\n    Konwinski was among the founding team at Databricks.\\n    Yarats, the CTO, was an AI research scientist at Meta.\\n    Ho, the CSO, worked as an engineer at Quora, then as a quantitative trader on Wall Street.[5]\\n'), TextContentItem(type='text', text='Result 3:\\nDocument_id:perpl\\nContent:  Ho, the CSO, worked as an engineer at Quora, then as a quantitative trader on Wall Street.[5]\\n'), TextContentItem(type='text', text='END of knowledge_search tool results.\\n')])])_[('response_format', None), ('sampling_params', SamplingParams(strategy=TopPSamplingStrategy(type='top_p', temperature=0.0001, top_p=0.9), max_tokens=0, repetition_penalty=1.0)), ('stream', True), ('tool_config', ToolConfig(tool_choice=<ToolChoice.auto: 'auto'>, tool_prompt_format=None, system_message_behavior=<SystemMessageBehavior.append: 'append'>)), ('tool_prompt_format', None), ('tools', [ToolDefinition(tool_name=<BuiltinTool.code_interpreter: 'code_interpreter'>, description='Execute code', parameters={'code': ToolParamDefinition(param_type='string', description='The code to execute', required=True, default=None)}), ToolDefinition(tool_name='knowledge_search', description='Search for information in a database.', parameters={'query': ToolParamDefinition(param_type='string', description='The query to search for. Can be a natural language sentence or keywords.', required=True, default=None)})])]": {
    "chunks": [
      {
        "event": {
          "delta": {
            "text": "",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "start"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "The",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " NBA was created on August 3, ",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "1949, with the merger of the",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " Basketball Association of America (BAA) and the National Basketball",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": " League (NBL).",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "complete"
          },
          "logprobs": null,
          "stop_reason": {
            "__enum__": "StopReason",
            "value": "end_of_turn"
          }
        },
        "metrics": null
      }
    ],
    "type": "generator"
  },
  "('meta-llama/Llama-3.1-8B-Instruct', [SystemMessage(role='system', content='You are a helpful assistant'), UserMessage(role='user', content='when was the nba created?', context=None)])_[('response_format', None), ('sampling_params', SamplingParams(strategy=TopPSamplingStrategy(type='top_p', temperature=0.0001, top_p=0.9), max_tokens=0, repetition_penalty=1.0)), ('stream', True), ('tool_config', ToolConfig(tool_choice=<ToolChoice.auto: 'auto'>, tool_prompt_format=None, system_message_behavior=<SystemMessageBehavior.append: 'append'>)), ('tool_prompt_format', None), ('tools', [ToolDefinition(tool_name='knowledge_search', description='Search for information in a database.', parameters={'query': ToolParamDefinition(param_type='string', description='The query to search for. Can be a natural language sentence or keywords.', required=True, default=None)}), ToolDefinition(tool_name=<BuiltinTool.code_interpreter: 'code_interpreter'>, description='Execute code', parameters={'code': ToolParamDefinition(param_type='string', description='The code to execute', required=True, default=None)})])]": {
    "chunks": [
      {
        "event": {
          "delta": {
            "text": "",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "start"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "started"
            },
            "tool_call": "",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "in_progress"
            },
            "tool_call": "{\"type\": \"function\", \"name\": \"knowledge_search\",",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "in_progress"
            },
            "tool_call": " \"parameters\": {\"query\": \"NBA creation date\"}}",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "succeeded"
            },
            "tool_call": {
              "arguments": {
                "query": "NBA creation date"
              },
              "call_id": "b48fae99-3d49-4d12-a7b2-f1efeef35329",
              "tool_name": "knowledge_search"
            },
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": {
            "__enum__": "StopReason",
            "value": "end_of_turn"
          }
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "complete"
          },
          "logprobs": null,
          "stop_reason": {
            "__enum__": "StopReason",
            "value": "end_of_turn"
          }
        },
        "metrics": null
      }
    ],
    "type": "generator"
  },
  "('meta-llama/Llama-3.1-8B-Instruct', [SystemMessage(role='system', content='You are a helpful assistant'), UserMessage(role='user', content='when was the nba created?', context=None)])_[('response_format', None), ('sampling_params', SamplingParams(strategy=TopPSamplingStrategy(type='top_p', temperature=0.0001, top_p=0.9), max_tokens=0, repetition_penalty=1.0)), ('stream', True), ('tool_config', ToolConfig(tool_choice=<ToolChoice.auto: 'auto'>, tool_prompt_format=None, system_message_behavior=<SystemMessageBehavior.append: 'append'>)), ('tool_prompt_format', None), ('tools', [ToolDefinition(tool_name=<BuiltinTool.code_interpreter: 'code_interpreter'>, description='Execute code', parameters={'code': ToolParamDefinition(param_type='string', description='The code to execute', required=True, default=None)}), ToolDefinition(tool_name='knowledge_search', description='Search for information in a database.', parameters={'query': ToolParamDefinition(param_type='string', description='The query to search for. Can be a natural language sentence or keywords.', required=True, default=None)})])]": {
    "chunks": [
      {
        "event": {
          "delta": {
            "text": "",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "start"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "started"
            },
            "tool_call": "",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "in_progress"
            },
            "tool_call": "{\"type\": \"function\", \"name\": \"knowledge_search",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "in_progress"
            },
            "tool_call": "\", \"parameters\": {\"query\": \"NBA creation date\"}}",
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": null
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "parse_status": {
              "__enum__": "ToolCallParseStatus",
              "value": "succeeded"
            },
            "tool_call": {
              "arguments": {
                "query": "NBA creation date"
              },
              "call_id": "f50656dd-201d-44b0-8f9f-ca88b970b3fd",
              "tool_name": "knowledge_search"
            },
            "type": "tool_call"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "progress"
          },
          "logprobs": null,
          "stop_reason": {
            "__enum__": "StopReason",
            "value": "end_of_turn"
          }
        },
        "metrics": null
      },
      {
        "event": {
          "delta": {
            "text": "",
            "type": "text"
          },
          "event_type": {
            "__enum__": "ChatCompletionResponseEventType",
            "value": "complete"
          },
          "logprobs": null,
          "stop_reason": {
            "__enum__": "StopReason",
            "value": "end_of_turn"
          }
        },
        "metrics": null
      }
    ],
    "type": "generator"
  }
}